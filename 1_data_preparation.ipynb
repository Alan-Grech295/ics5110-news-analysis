{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6343c350fa0753b7",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "This script contains all the code to process the original CSV files into a structured dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972a986a59419019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EYONGEBOB\\AppData\\Local\\Temp\\ipykernel_1592\\3323452323.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import logging\n",
    "import os\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Downloads spaCy model\n",
    "subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "# Loads spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d7bc4a3fd10f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original datasets\n",
    "news_articles_df = pd.read_csv(\"data/original/local_news_articles.csv\")\n",
    "press_releases_df = pd.read_csv(\"data/original/police_press_releases.csv\")\n",
    "\n",
    "# Rename news article `article_id` column to `id`\n",
    "news_articles_df.rename(columns={\"article_id\": \"id\"}, inplace=True)\n",
    "\n",
    "# Add `id` column to police press releases, continuing from the news articles ids\n",
    "start = news_articles_df[\"id\"].max() + 1\n",
    "press_releases_df.insert(0, \"id\", range(start, start + len(press_releases_df)))\n",
    "\n",
    "# We can save the police press releases as is; they are all valid accidents\n",
    "press_releases_df.to_csv(\"data/police_press_releases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c818ab5dfbcec",
   "metadata": {},
   "source": [
    "# 1. Replace special characters\n",
    "## Why?\n",
    "1. Special characters are not always supported by NLP libraries.\n",
    "2. Special characters are not always converted to lowercase successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab8d8f36780746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map special characters to ASCII\n",
    "mapping = str.maketrans({\n",
    "    \"ċ\": \"c\",\n",
    "    \"Ċ\": \"C\",\n",
    "    \"ġ\": \"g\",\n",
    "    \"Ġ\": \"G\",\n",
    "    \"ħ\": \"h\",\n",
    "    \"Ħ\": \"H\",\n",
    "    \"ż\": \"z\",\n",
    "    \"Ż\": \"Z\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"‘\": \"'\",\n",
    "    \"’\": \"'\",\n",
    "    \"—\": \"-\",\n",
    "    \"–\": \"-\",\n",
    "    \"…\": \"...\",\n",
    "})\n",
    "\n",
    "def clean_articles(df: pd.DataFrame, columns):\n",
    "    \"\"\"Replaces special characters in the given dataframe columns with their ASCII counterparts\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).str.translate(mapping)\n",
    "    return df\n",
    "\n",
    "# Clean the two datasets\n",
    "news_articles_df = clean_articles(news_articles_df, [\"title\", \"subtitle\", \"content\"])\n",
    "press_releases_df = clean_articles(press_releases_df, [\"title\", \"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f54bcfbeff479",
   "metadata": {},
   "source": [
    "# 2.a. Remove non-related articles\n",
    "Some articles in the dataset do not refer to vehicle accidents (e.g. refers to work accidents or new accident prevention policies). We need to remove these.\n",
    "\n",
    "This is done in two ways:\n",
    "1. Matching accident phrases (e.g. car crash)\n",
    "2. Finding reference to a person, vehicle and accident or injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "507144bfe0768b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_subj = {\"man\", \"woman\", \"child\", \"driver\", \"motorist\", \"motorcyclist\", \"pedestrian\"}\n",
    "vehicles = {\"car\", \"motorcycle\", \"truck\", \"van\", \"bus\", \"bicycle\",\n",
    "            \"Audi\", \"BMW\", \"Chevrolet\", \"Citroen\", \"Dodge\", \"Fiat\", \"Ford\", \"Honda\", \"Hyundai\", \"Isuzu\",\n",
    "            \"Jaguar\", \"Jeep\", \"Kia\", \"Kymco\", \"Mercedes\", \"Mercedes-Benz\", \"Mini\", \"Mitsubishi\", \"Nissan\",\n",
    "            \"Peugeot\", \"Renault\", \"Skoda\", \"Subaru\", \"Suzuki\", \"Toyota\", \"Volkswagen\", \"VW\", \"Volvo\"}\n",
    "accident = {\"accident\", \"crash\", \"collision\"}\n",
    "injuries = {\"injure\", \"die\"}\n",
    "\n",
    "accident_phrases = [\n",
    "    \"car crash\", \"traffic accident\", \"road accident\", \"collision\",\n",
    "    \"crashed\", \"crash\", \"hit by a car\", \"motorcycle accident\",\n",
    "    \"injured in a crash\", \"overturned\", \"run over\", \"lost control\"\n",
    "]\n",
    "\n",
    "accident_matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = [nlp(text) for text in accident_phrases]\n",
    "accident_matcher.add(\"ACCIDENT_PATTERNS\", patterns)\n",
    "\n",
    "def refers_to_accident(text: str) -> bool:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    matches = accident_matcher(doc)\n",
    "\n",
    "    # If any accident phrases are found, assume it is a valid article\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "\n",
    "    has_people = False\n",
    "    has_vehicles = False\n",
    "    has_accident = False\n",
    "    has_injury = False\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ in people_subj:\n",
    "            has_people = True\n",
    "\n",
    "        if token.lemma_ in vehicles:\n",
    "            has_vehicles = True\n",
    "\n",
    "        if token.lemma_ in accident:\n",
    "            has_accident = True\n",
    "\n",
    "        if token.lemma_ in injuries:\n",
    "            has_injury = True\n",
    "\n",
    "        # If people, vehicles and accident or injury is mentioned, assume it is a valid article\n",
    "        if has_people and has_vehicles and (has_accident or has_injury):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# IDs of articles not referring to vehicle accidents\n",
    "non_related_news_article_ids = []\n",
    "\n",
    "for id, text in news_articles_df[[\"id\", \"content\"]].values:\n",
    "    is_accident = refers_to_accident(text)\n",
    "    if not is_accident:\n",
    "        non_related_news_article_ids.append(id)\n",
    "\n",
    "os.makedirs(\"data/intermediate\", exist_ok=True)\n",
    "\n",
    "non_related_news_article_df = news_articles_df[news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "related_news_article_df = news_articles_df[~news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "\n",
    "# Save dataframes as CSVs to view results\n",
    "non_related_news_article_df.to_csv(\"data/intermediate/local_news_articles_exclusions.csv\", index=False)\n",
    "related_news_article_df.to_csv(\"data/intermediate/local_news_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e56a312ffe763b",
   "metadata": {},
   "source": [
    "# 2.b. Using LLMs to flag non-related articles\n",
    "While the previous method works quite well, some articles still get through.\n",
    "To catch these, we pass the remaining articles through 3 LLMs (GPT 5 Mini, Grok 4 Fast, Deepseek R1).\n",
    "\n",
    "The LLMs were set up through [Microsoft Foundry](https://ai.azure.com/) to have a unified API to communicate with different LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb25c1ae9683630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DefaultAzureCredential failed to retrieve a token from the included credentials.\n",
      "Attempted credentials:\n",
      "\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
      "Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n",
      "\tWorkloadIdentityCredential: WorkloadIdentityCredential authentication unavailable. The workload options are not fully configured. See the troubleshooting guide for more information: https://aka.ms/azsdk/python/identity/workloadidentitycredential/troubleshoot. Missing required arguments: 'tenant_id', 'client_id', 'token_file_path'.\n",
      "\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n",
      "\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n",
      "\tVisualStudioCodeCredential: VisualStudioCodeCredential requires the 'azure-identity-broker' package to be installed. You must also ensure you have the Azure Resources extension installed and have signed in to Azure via Visual Studio Code.\n",
      "\tAzureCliCredential: Azure CLI not found on path\n",
      "\tAzurePowerShellCredential: Az.Account module >= 2.2.0 is not installed\n",
      "\tAzureDeveloperCliCredential: Azure Developer CLI could not be found. Please visit https://aka.ms/azure-dev for installation instructions and then,once installed, authenticate to your Azure account using 'azd auth login'.\n",
      "\tBrokerCredential: InteractiveBrowserBrokerCredential unavailable. The 'azure-identity-broker' package is required to use brokered authentication.\n",
      "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initialising API\"\"\"\n",
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "endpoint = \"https://news-analysis-resource.openai.azure.com/openai/v1/\"\n",
    "\n",
    "# Need to login using `az login --use-device-code`\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token_provider,\n",
    ")\n",
    "\n",
    "try:\n",
    "    token_provider()\n",
    "    run_cell = True\n",
    "except:\n",
    "    run_cell = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00e9ea8a57544a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not run_cell\n",
    "\n",
    "models = [(\"gpt-5-mini\", 50), (\"grok-4-fast-non-reasoning\", 50), (\"DeepSeek-R1-0528\", 20)]\n",
    "\n",
    "class NonAccidentIDs(BaseModel):\n",
    "    ids: List[int] = Field(description=\"A list of ids of news articles that are not accidents\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a senior data scientist reviewing a semi-structured dataset of vehicle accidents news articles. The articles were obtained by simple web scraping (ex: on the tag of the article) which means that some articles do not refer to actual accidents (for example, they refer to new accident/traffic measures).\n",
    "\n",
    "Your job is to review the given accident CSV and return a list of news article IDs that do not refer to accidents.\n",
    "Be very critical! Any article which mentions a specific accident and provides details on it should not be removed.\n",
    "\n",
    "IMPORTANT: You MUST return the data by calling the `set_non_accident_ids` function.\n",
    "\n",
    "Do not return anything other than a function call.\n",
    "\"\"\"\n",
    "\n",
    "csv_prompt = lambda dataset_csv, start_rows, end_rows, total_rows: f\"\"\"\n",
    "MAKE SURE THAT THE RETURNED IDS EXIST IN THIS CSV!\n",
    "\n",
    "Accident CSV Data ({start_rows}-{end_rows}/{total_rows} rows):\n",
    "\n",
    "{dataset_csv}\n",
    "\"\"\"\n",
    "\n",
    "# LLM function definition\n",
    "result_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"set_non_accident_ids\",\n",
    "        \"description\": \"Set the IDs of the news articles which do not refer to an accident\",\n",
    "        \"parameters\": NonAccidentIDs.model_json_schema()\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [result_function]\n",
    "\n",
    "tqdm.set_lock(threading.RLock())\n",
    "\n",
    "def process_model(model, num_rows_per_request, dataset_df: pd.DataFrame, system_prompt) -> set[int]:\n",
    "    total_ids = set()\n",
    "\n",
    "    with tqdm(range(0, len(dataset_df.index), num_rows_per_request), desc=f\"Querying {model}\", total=len(dataset_df.index), unit=\" rows\") as pbar:\n",
    "        i = 0\n",
    "        while i < len(dataset_df.index):\n",
    "            try:\n",
    "                # Get row range as the LLMs cannot process the entire file at once\n",
    "                start = i\n",
    "                end = min(i + num_rows_per_request, len(dataset_df.index))\n",
    "                df_section: pd.DataFrame = dataset_df.iloc[start:end]\n",
    "                df_section_csv = df_section.to_csv(index=False)\n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": csv_prompt(df_section_csv, start + 1, end + 1, len(dataset_df.index)),\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                result: NonAccidentIDs = NonAccidentIDs.model_validate_json(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "\n",
    "                for id in result.ids:\n",
    "                    # Throw an error if a returned ID is not in the dataset\n",
    "                    if id not in dataset_df[\"id\"].values:\n",
    "                        raise ValueError(f\"ID {id} not in dataset\")\n",
    "\n",
    "                total_ids.update(result.ids)\n",
    "\n",
    "                # Update tqdm progress bar\n",
    "                pbar.update(end - start)\n",
    "                i += num_rows_per_request\n",
    "            except Exception as e:\n",
    "                # If we get an error, retry the model (i.e. do not increment i)\n",
    "                logging.warning(f\"Failed to query {model}: {e}\")\n",
    "\n",
    "    return total_ids\n",
    "\n",
    "# Run LLMs in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_model,\n",
    "            model,\n",
    "            num_rows_per_request,\n",
    "            related_news_article_df,\n",
    "            system_prompt\n",
    "        ): model\n",
    "        for model, num_rows_per_request in models\n",
    "    }\n",
    "\n",
    "    model_ids = {}\n",
    "\n",
    "    for f in futures.keys():\n",
    "        result = f.result()\n",
    "        model_ids[futures[f]] = result\n",
    "\n",
    "all_ids = list(model_ids.values())\n",
    "# Combine IDs from all LLMs\n",
    "ids_union = all_ids[0].union(*all_ids[1:])\n",
    "\n",
    "# Save the LLM excluded articles as a CSV to review\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_exclusions.csv\", index=False)\n",
    "# Save the same LLM excluded articles as a CSV. This CSV will be manually checked and modified\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555b459dc1ee45f",
   "metadata": {},
   "source": [
    "# 2.c. Manually review LLM exclusions\n",
    "The LLMs tend to flag valid articles as invalid. Since this is a short list (because we already removed the bulk of invalid articles in step 1), we can go through the list manually and remove the valid articles.\n",
    "\n",
    "## Removed IDs\n",
    "- 3699\n",
    "- 1352\n",
    "- 370\n",
    "- 287\n",
    "- 489424\n",
    "- 491118\n",
    "- 491371\n",
    "- 494102\n",
    "- 495320\n",
    "- 495942\n",
    "- 496362\n",
    "\n",
    "*Note: `local_news_articles_llm_exclusions.csv` contains the excluded rows as given by the LLMs. `local_news_articles_llm_manual_exclusions.csv` contains the same excluded rows given by the LLMs, except that any rows referring to valid accidents were manually removed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53eeb00bf883dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual removal of valid rows\n",
    "manual_ids = {3699, 1352, 370, 287, 489424, 491118, 491371, 494102, 495320, 495942, 496362}\n",
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "llm_manual_exclusions = llm_manual_exclusions[~llm_manual_exclusions[\"id\"].isin(manual_ids)]\n",
    "llm_manual_exclusions.to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f5a91b49de2f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "\n",
    "# Filtering the original news articles\n",
    "news_articles_df = related_news_article_df[~related_news_article_df[\"id\"].isin(llm_manual_exclusions[\"id\"])]\n",
    "news_articles_df.to_csv(\"data/local_news_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab9079a6b2e4e2",
   "metadata": {},
   "source": [
    "# 3. Data extraction\n",
    "Use `news_articles_df` and `press_releases_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00aa23a6-8b31-4e00-b1d2-6f0c29b81f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'url', 'source_name', 'source_url', 'title', 'subtitle',\n",
      "       'author_name', 'publish_date', 'content', 'top_image_url',\n",
      "       'top_image_caption', 'created_at', 'tags', 'categories'],\n",
      "      dtype='object')\n",
      "Index(['id', 'title', 'date_published', 'date_modified', 'content'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(news_articles_df.columns)\n",
    "print(press_releases_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4223aa0b-cca8-40db-a949-6d6aeae9b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE FOR DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eebfd82a-f0f1-429f-9fcd-261d1c4353cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:25<00:00,  8.51it/s]\n",
      "100%|██████████| 111/111 [00:07<00:00, 14.98it/s]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'accident1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m accidents_df \u001b[38;5;241m=\u001b[39m process_datasets(news_articles_df, press_releases_df)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# ===================== SAVE OUTPUT =====================\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m accidents_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccident1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Extraction complete. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(accidents_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accidents saved to accident1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3989\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3978\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3980\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3981\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3982\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3986\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3987\u001b[0m )\n\u001b[1;32m-> 3989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3990\u001b[0m     path_or_buf,\n\u001b[0;32m   3991\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3992\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3993\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3994\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3995\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3996\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3997\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3998\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3999\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   4000\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   4001\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   4002\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   4003\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   4004\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   4005\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   4006\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'accident1.csv'"
     ]
    }
   ],
   "source": [
    "# ===================== INSTALL DEPENDENCIES =====================\n",
    "\n",
    "# ===================== IMPORTS =====================\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import dateparser\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "# ===================== LOAD NLP MODEL =====================\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ===================== HELPER FUNCTIONS =====================\n",
    "\n",
    "def extract_date_time(text: str):\n",
    "    \"\"\"Extract date and time objects from text safely\"\"\"\n",
    "    try:\n",
    "        parsed = search_dates(text)\n",
    "        if parsed:\n",
    "            dt = parsed[0][1]\n",
    "            return dt.date(), dt.time()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def determine_accident_type(sentence: str):\n",
    "    \"\"\"Determine accident type using keyword heuristics\"\"\"\n",
    "    types = {\n",
    "        \"collision\": [\"collision\", \"crash\", \"hit\"],\n",
    "        \"rollover\": [\"rollover\", \"overturned\"],\n",
    "        \"pedestrian\": [\"pedestrian\", \"walked into\"],\n",
    "        \"rear-end\": [\"rear-ended\"],\n",
    "        \"multi-vehicle\": [\"multiple vehicles\", \"pile-up\"],\n",
    "    }\n",
    "    for k, words in types.items():\n",
    "        if any(w in sentence.lower() for w in words):\n",
    "            return k\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_accidents_from_text(article_id: str, text: str, fallback_date=None) -> list:\n",
    "    \"\"\"\n",
    "    Extracts multiple accidents from an article.\n",
    "    Each accident includes:\n",
    "    - Accident type\n",
    "    - Date and time (objects)\n",
    "    - Location (NER)\n",
    "    - Car type\n",
    "    - Sentence character positions\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    accidents = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        if any(word in sent.text.lower() for word in [\"accident\", \"crash\", \"collision\"]):\n",
    "            date_obj, time_obj = extract_date_time(sent.text)\n",
    "\n",
    "            # fallback: use article publish date if sentence date is None\n",
    "            if date_obj is None and fallback_date:\n",
    "                date_obj = fallback_date.date() if hasattr(fallback_date, 'date') else None\n",
    "                time_obj = fallback_date.time() if hasattr(fallback_date, 'time') else None\n",
    "\n",
    "            accident_type = determine_accident_type(sent.text)\n",
    "            car_type = extract_car_type(sent.text)\n",
    "\n",
    "            location = None\n",
    "            for ent in sent.ents:\n",
    "                if ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "                    location = ent.text\n",
    "                    break\n",
    "\n",
    "            accidents.append({\n",
    "                \"id\": article_id,\n",
    "                \"accident_type\": accident_type,\n",
    "                \"publish_date\": date_obj,\n",
    "                \"publish_time\": time_obj,\n",
    "                \"location\": location,\n",
    "                \"car_type\": car_type,\n",
    "                \"text_start_char\": sent.start_char,\n",
    "                \"text_end_char\": sent.end_char,\n",
    "                \"sentence_text\": sent.text\n",
    "            })\n",
    "    return accidents\n",
    "\n",
    "# ===================== DATASET PROCESSING =====================\n",
    "\n",
    "def process_datasets(news_df, police_df):\n",
    "    all_accidents = []\n",
    "\n",
    "    for df, text_col in [\n",
    "        (news_df, \"content\"),\n",
    "        (police_df, \"content\")\n",
    "    ]:\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            article_id = row[\"id\"]\n",
    "            text = row[text_col]\n",
    "\n",
    "            if isinstance(text, str) and len(text) > 50:\n",
    "                accidents = extract_accidents_from_text(article_id, text)\n",
    "                all_accidents.extend(accidents)\n",
    "\n",
    "    return pd.DataFrame(all_accidents)\n",
    "\n",
    "\n",
    "# ===================== RUN PIPELINE =====================\n",
    "\n",
    "# Example:\n",
    "# news_articles_df = pd.read_csv(\"local_news_articles.csv\")\n",
    "# press_releases_df = pd.read_csv(\"police_press_releases.csv\")\n",
    "\n",
    "accidents_df = process_datasets(news_articles_df, press_releases_df)\n",
    "\n",
    "# ===================== SAVE OUTPUT =====================\n",
    "accidents_df.to_csv(\"accident1.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Extraction complete. {len(accidents_df)} accidents saved to accident1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9bc23-4a34-4fd2-935c-2a96a2112894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f70389-cb72-4d0e-b3cc-271dfc5c62e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
