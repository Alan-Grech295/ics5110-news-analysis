{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6343c350fa0753b7",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "This script contains all the code to process the original CSV files into a structured dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "972a986a59419019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EYONGEBOB\\AppData\\Local\\Temp\\ipykernel_14484\\3323452323.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import logging\n",
    "import os\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Downloads spaCy model\n",
    "subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "# Loads spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96d7bc4a3fd10f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original datasets\n",
    "news_articles_df = pd.read_csv(\"data/original/local_news_articles.csv\")\n",
    "press_releases_df = pd.read_csv(\"data/original/police_press_releases.csv\")\n",
    "\n",
    "# Rename news article `article_id` column to `id`\n",
    "news_articles_df.rename(columns={\"article_id\": \"id\"}, inplace=True)\n",
    "\n",
    "# Add `id` column to police press releases, continuing from the news articles ids\n",
    "start = news_articles_df[\"id\"].max() + 1\n",
    "press_releases_df.insert(0, \"id\", range(start, start + len(press_releases_df)))\n",
    "\n",
    "# We can save the police press releases as is; they are all valid accidents\n",
    "press_releases_df.to_csv(\"data/police_press_releases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c818ab5dfbcec",
   "metadata": {},
   "source": [
    "# 1. Replace special characters\n",
    "## Why?\n",
    "1. Special characters are not always supported by NLP libraries.\n",
    "2. Special characters are not always converted to lowercase successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab8d8f36780746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map special characters to ASCII\n",
    "mapping = str.maketrans({\n",
    "    \"ċ\": \"c\",\n",
    "    \"Ċ\": \"C\",\n",
    "    \"ġ\": \"g\",\n",
    "    \"Ġ\": \"G\",\n",
    "    \"ħ\": \"h\",\n",
    "    \"Ħ\": \"H\",\n",
    "    \"ż\": \"z\",\n",
    "    \"Ż\": \"Z\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"‘\": \"'\",\n",
    "    \"’\": \"'\",\n",
    "    \"—\": \"-\",\n",
    "    \"–\": \"-\",\n",
    "    \"…\": \"...\",\n",
    "})\n",
    "\n",
    "def clean_articles(df: pd.DataFrame, columns):\n",
    "    \"\"\"Replaces special characters in the given dataframe columns with their ASCII counterparts\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).str.translate(mapping)\n",
    "    return df\n",
    "\n",
    "# Clean the two datasets\n",
    "news_articles_df = clean_articles(news_articles_df, [\"title\", \"subtitle\", \"content\"])\n",
    "press_releases_df = clean_articles(press_releases_df, [\"title\", \"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f54bcfbeff479",
   "metadata": {},
   "source": [
    "# 2.a. Remove non-related articles\n",
    "Some articles in the dataset do not refer to vehicle accidents (e.g. refers to work accidents or new accident prevention policies). We need to remove these.\n",
    "\n",
    "This is done in two ways:\n",
    "1. Matching accident phrases (e.g. car crash)\n",
    "2. Finding reference to a person, vehicle and accident or injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507144bfe0768b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_subj = {\"man\", \"woman\", \"child\", \"driver\", \"motorist\", \"motorcyclist\", \"pedestrian\"}\n",
    "vehicles = {\"car\", \"motorcycle\", \"truck\", \"van\", \"bus\", \"bicycle\",\n",
    "            \"Audi\", \"BMW\", \"Chevrolet\", \"Citroen\", \"Dodge\", \"Fiat\", \"Ford\", \"Honda\", \"Hyundai\", \"Isuzu\",\n",
    "            \"Jaguar\", \"Jeep\", \"Kia\", \"Kymco\", \"Mercedes\", \"Mercedes-Benz\", \"Mini\", \"Mitsubishi\", \"Nissan\",\n",
    "            \"Peugeot\", \"Renault\", \"Skoda\", \"Subaru\", \"Suzuki\", \"Toyota\", \"Volkswagen\", \"VW\", \"Volvo\"}\n",
    "accident = {\"accident\", \"crash\", \"collision\"}\n",
    "injuries = {\"injure\", \"die\"}\n",
    "\n",
    "accident_phrases = [\n",
    "    \"car crash\", \"traffic accident\", \"road accident\", \"collision\",\n",
    "    \"crashed\", \"crash\", \"hit by a car\", \"motorcycle accident\",\n",
    "    \"injured in a crash\", \"overturned\", \"run over\", \"lost control\"\n",
    "]\n",
    "\n",
    "accident_matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = [nlp(text) for text in accident_phrases]\n",
    "accident_matcher.add(\"ACCIDENT_PATTERNS\", patterns)\n",
    "\n",
    "def refers_to_accident(text: str) -> bool:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    matches = accident_matcher(doc)\n",
    "\n",
    "    # If any accident phrases are found, assume it is a valid article\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "\n",
    "    has_people = False\n",
    "    has_vehicles = False\n",
    "    has_accident = False\n",
    "    has_injury = False\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ in people_subj:\n",
    "            has_people = True\n",
    "\n",
    "        if token.lemma_ in vehicles:\n",
    "            has_vehicles = True\n",
    "\n",
    "        if token.lemma_ in accident:\n",
    "            has_accident = True\n",
    "\n",
    "        if token.lemma_ in injuries:\n",
    "            has_injury = True\n",
    "\n",
    "        # If people, vehicles and accident or injury is mentioned, assume it is a valid article\n",
    "        if has_people and has_vehicles and (has_accident or has_injury):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# IDs of articles not referring to vehicle accidents\n",
    "non_related_news_article_ids = []\n",
    "\n",
    "for id, text in news_articles_df[[\"id\", \"content\"]].values:\n",
    "    is_accident = refers_to_accident(text)\n",
    "    if not is_accident:\n",
    "        non_related_news_article_ids.append(id)\n",
    "\n",
    "os.makedirs(\"data/intermediate\", exist_ok=True)\n",
    "\n",
    "non_related_news_article_df = news_articles_df[news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "related_news_article_df = news_articles_df[~news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "\n",
    "# Save dataframes as CSVs to view results\n",
    "non_related_news_article_df.to_csv(\"data/intermediate/local_news_articles_exclusions.csv\", index=False)\n",
    "related_news_article_df.to_csv(\"data/intermediate/local_news_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e56a312ffe763b",
   "metadata": {},
   "source": [
    "# 2.b. Using LLMs to flag non-related articles\n",
    "While the previous method works quite well, some articles still get through.\n",
    "To catch these, we pass the remaining articles through 3 LLMs (GPT 5 Mini, Grok 4 Fast, Deepseek R1).\n",
    "\n",
    "The LLMs were set up through [Microsoft Foundry](https://ai.azure.com/) to have a unified API to communicate with different LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb25c1ae9683630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DefaultAzureCredential failed to retrieve a token from the included credentials.\n",
      "Attempted credentials:\n",
      "\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
      "Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n",
      "\tWorkloadIdentityCredential: WorkloadIdentityCredential authentication unavailable. The workload options are not fully configured. See the troubleshooting guide for more information: https://aka.ms/azsdk/python/identity/workloadidentitycredential/troubleshoot. Missing required arguments: 'tenant_id', 'client_id', 'token_file_path'.\n",
      "\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n",
      "\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n",
      "\tVisualStudioCodeCredential: VisualStudioCodeCredential requires the 'azure-identity-broker' package to be installed. You must also ensure you have the Azure Resources extension installed and have signed in to Azure via Visual Studio Code.\n",
      "\tAzureCliCredential: Azure CLI not found on path\n",
      "\tAzurePowerShellCredential: Az.Account module >= 2.2.0 is not installed\n",
      "\tAzureDeveloperCliCredential: Azure Developer CLI could not be found. Please visit https://aka.ms/azure-dev for installation instructions and then,once installed, authenticate to your Azure account using 'azd auth login'.\n",
      "\tBrokerCredential: InteractiveBrowserBrokerCredential unavailable. The 'azure-identity-broker' package is required to use brokered authentication.\n",
      "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initialising API\"\"\"\n",
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "endpoint = \"https://news-analysis-resource.openai.azure.com/openai/v1/\"\n",
    "\n",
    "# Need to login using `az login --use-device-code`\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token_provider,\n",
    ")\n",
    "\n",
    "try:\n",
    "    token_provider()\n",
    "    run_cell = True\n",
    "except:\n",
    "    run_cell = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c00e9ea8a57544a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not run_cell\n",
    "\n",
    "models = [(\"gpt-5-mini\", 50), (\"grok-4-fast-non-reasoning\", 50), (\"DeepSeek-R1-0528\", 20)]\n",
    "\n",
    "class NonAccidentIDs(BaseModel):\n",
    "    ids: List[int] = Field(description=\"A list of ids of news articles that are not accidents\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a senior data scientist reviewing a semi-structured dataset of vehicle accidents news articles. The articles were obtained by simple web scraping (ex: on the tag of the article) which means that some articles do not refer to actual accidents (for example, they refer to new accident/traffic measures).\n",
    "\n",
    "Your job is to review the given accident CSV and return a list of news article IDs that do not refer to accidents.\n",
    "Be very critical! Any article which mentions a specific accident and provides details on it should not be removed.\n",
    "\n",
    "IMPORTANT: You MUST return the data by calling the `set_non_accident_ids` function.\n",
    "\n",
    "Do not return anything other than a function call.\n",
    "\"\"\"\n",
    "\n",
    "csv_prompt = lambda dataset_csv, start_rows, end_rows, total_rows: f\"\"\"\n",
    "MAKE SURE THAT THE RETURNED IDS EXIST IN THIS CSV!\n",
    "\n",
    "Accident CSV Data ({start_rows}-{end_rows}/{total_rows} rows):\n",
    "\n",
    "{dataset_csv}\n",
    "\"\"\"\n",
    "\n",
    "# LLM function definition\n",
    "result_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"set_non_accident_ids\",\n",
    "        \"description\": \"Set the IDs of the news articles which do not refer to an accident\",\n",
    "        \"parameters\": NonAccidentIDs.model_json_schema()\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [result_function]\n",
    "\n",
    "tqdm.set_lock(threading.RLock())\n",
    "\n",
    "def process_model(model, num_rows_per_request, dataset_df: pd.DataFrame, system_prompt) -> set[int]:\n",
    "    total_ids = set()\n",
    "\n",
    "    with tqdm(range(0, len(dataset_df.index), num_rows_per_request), desc=f\"Querying {model}\", total=len(dataset_df.index), unit=\" rows\") as pbar:\n",
    "        i = 0\n",
    "        while i < len(dataset_df.index):\n",
    "            try:\n",
    "                # Get row range as the LLMs cannot process the entire file at once\n",
    "                start = i\n",
    "                end = min(i + num_rows_per_request, len(dataset_df.index))\n",
    "                df_section: pd.DataFrame = dataset_df.iloc[start:end]\n",
    "                df_section_csv = df_section.to_csv(index=False)\n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": csv_prompt(df_section_csv, start + 1, end + 1, len(dataset_df.index)),\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                result: NonAccidentIDs = NonAccidentIDs.model_validate_json(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "\n",
    "                for id in result.ids:\n",
    "                    # Throw an error if a returned ID is not in the dataset\n",
    "                    if id not in dataset_df[\"id\"].values:\n",
    "                        raise ValueError(f\"ID {id} not in dataset\")\n",
    "\n",
    "                total_ids.update(result.ids)\n",
    "\n",
    "                # Update tqdm progress bar\n",
    "                pbar.update(end - start)\n",
    "                i += num_rows_per_request\n",
    "            except Exception as e:\n",
    "                # If we get an error, retry the model (i.e. do not increment i)\n",
    "                logging.warning(f\"Failed to query {model}: {e}\")\n",
    "\n",
    "    return total_ids\n",
    "\n",
    "# Run LLMs in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_model,\n",
    "            model,\n",
    "            num_rows_per_request,\n",
    "            related_news_article_df,\n",
    "            system_prompt\n",
    "        ): model\n",
    "        for model, num_rows_per_request in models\n",
    "    }\n",
    "\n",
    "    model_ids = {}\n",
    "\n",
    "    for f in futures.keys():\n",
    "        result = f.result()\n",
    "        model_ids[futures[f]] = result\n",
    "\n",
    "all_ids = list(model_ids.values())\n",
    "# Combine IDs from all LLMs\n",
    "ids_union = all_ids[0].union(*all_ids[1:])\n",
    "\n",
    "# Save the LLM excluded articles as a CSV to review\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_exclusions.csv\", index=False)\n",
    "# Save the same LLM excluded articles as a CSV. This CSV will be manually checked and modified\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555b459dc1ee45f",
   "metadata": {},
   "source": [
    "# 2.c. Manually review LLM exclusions\n",
    "The LLMs tend to flag valid articles as invalid. Since this is a short list (because we already removed the bulk of invalid articles in step 1), we can go through the list manually and remove the valid articles.\n",
    "\n",
    "## Removed IDs\n",
    "- 3699\n",
    "- 1352\n",
    "- 370\n",
    "- 287\n",
    "- 489424\n",
    "- 491118\n",
    "- 491371\n",
    "- 494102\n",
    "- 495320\n",
    "- 495942\n",
    "- 496362\n",
    "\n",
    "*Note: `local_news_articles_llm_exclusions.csv` contains the excluded rows as given by the LLMs. `local_news_articles_llm_manual_exclusions.csv` contains the same excluded rows given by the LLMs, except that any rows referring to valid accidents were manually removed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53eeb00bf883dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual removal of valid rows\n",
    "manual_ids = {3699, 1352, 370, 287, 489424, 491118, 491371, 494102, 495320, 495942, 496362}\n",
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "llm_manual_exclusions = llm_manual_exclusions[~llm_manual_exclusions[\"id\"].isin(manual_ids)]\n",
    "llm_manual_exclusions.to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f5a91b49de2f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "\n",
    "# Filtering the original news articles\n",
    "news_articles_df = related_news_article_df[~related_news_article_df[\"id\"].isin(llm_manual_exclusions[\"id\"])]\n",
    "news_articles_df.to_csv(\"data/local_news_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab9079a6b2e4e2",
   "metadata": {},
   "source": [
    "# 3. Data extraction\n",
    "Use `news_articles_df` and `press_releases_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86f93c4d-1043-4371-b796-6add2c7e3207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted features from 214 news articles and 111 press releases.\n",
      "Combined data saved to 'accident.csv' with 325 records.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Function to extract date and time from text\n",
    "def extract_date_time(text):\n",
    "    # Common date patterns\n",
    "    date_patterns = [\n",
    "        r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2,4})',  # DD/MM/YYYY or MM/DD/YYYY\n",
    "        r'(\\d{1,2})(?:st|nd|rd|th)? (?:of )?(January|February|March|April|May|June|July|August|September|October|November|December),? (\\d{4})',\n",
    "        r'(January|February|March|April|May|June|July|August|September|October|November|December) (\\d{1,2})(?:st|nd|rd|th)?,? (\\d{4})'\n",
    "    ]\n",
    "    \n",
    "    # Time patterns\n",
    "    time_patterns = [\n",
    "        r'(\\d{1,2}):(\\d{2}) ?([aApP][mM])?',\n",
    "        r'(\\d{1,2}) ?([aApP][mM])'\n",
    "    ]\n",
    "    \n",
    "    # Extract date\n",
    "    date = None\n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            date = match.group(0)\n",
    "            break\n",
    "    \n",
    "    # Extract time\n",
    "    time = None\n",
    "    for pattern in time_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            time = match.group(0)\n",
    "            break\n",
    "    \n",
    "    return date, time\n",
    "\n",
    "# Function to extract location information\n",
    "def extract_location(text):\n",
    "    # Look for common location indicators\n",
    "    location_patterns = [\n",
    "        r'in ([A-Z][a-z]+ ?[A-Z]?[a-z]*)',\n",
    "        r'at ([A-Z][a-z]+ ?[A-Z]?[a-z]*)',\n",
    "        r'near ([A-Z][a-z]+ ?[A-Z]?[a-z]*)',\n",
    "        r'on ([A-Z][a-z]+ ?[A-Z]?[a-z]* Road|Street|Avenue|Highway|Hwy|Freeway|Fwy)'\n",
    "    ]\n",
    "    \n",
    "    location = None\n",
    "    locality = None\n",
    "    \n",
    "    # Extract specific location\n",
    "    for pattern in location_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            location = match.group(1)\n",
    "            break\n",
    "    \n",
    "    # Extract locality (city/town)\n",
    "    locality_pattern = r'in ([A-Z][a-z]+ ?[A-Z]?[a-z]*)'\n",
    "    match = re.search(locality_pattern, text)\n",
    "    if match:\n",
    "        locality = match.group(1)\n",
    "    \n",
    "    return locality, location\n",
    "\n",
    "# Function to determine accident type\n",
    "def determine_accident_type(text):\n",
    "    accident_types = {\n",
    "        'collision': ['collision', 'crash', 'hit', 'struck'],\n",
    "        'fire': ['fire', 'burn', 'flames'],\n",
    "        'derailment': ['derail', 'derailment', 'off the track'],\n",
    "        'explosion': ['explosion', 'explode', 'blast'],\n",
    "        'spill': ['spill', 'leak', 'contamination']\n",
    "    }\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for acc_type, keywords in accident_types.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in text_lower:\n",
    "                return acc_type\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "# Process each dataframe and extract features\n",
    "def extract_features(df, source_type):\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        article_id = f\"{source_type}_{idx}\"\n",
    "        \n",
    "        # Get text content\n",
    "        text = row.get('content', '') if 'content' in df.columns else row.get('text', '')\n",
    "        if pd.isna(text):\n",
    "            text = ''\n",
    "        \n",
    "        # Extract features\n",
    "        date, time = extract_date_time(text)\n",
    "        locality, location = extract_location(text)\n",
    "        accident_type = determine_accident_type(text)\n",
    "        \n",
    "        results.append({\n",
    "            'article_id': article_id,\n",
    "            'accident_date': date,\n",
    "            'accident_time': time,\n",
    "            'locality': locality,\n",
    "            'location': location,\n",
    "            'accident_type': accident_type\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process both dataframes\n",
    "try:\n",
    "    news_features = extract_features(news_articles_df, 'news')\n",
    "    press_features = extract_features(press_releases_df, 'press')\n",
    "    \n",
    "    # Combine results\n",
    "    combined_features = pd.concat([news_features, press_features], ignore_index=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_features.to_csv('accident.csv', index=False)\n",
    "    \n",
    "    print(f\"Successfully extracted features from {len(news_features)} news articles and {len(press_features)} press releases.\")\n",
    "    print(f\"Combined data saved to 'accident.csv' with {len(combined_features)} records.\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Error: One or both of the required dataframes (news_articles_df, press_releases_df) are not defined.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84b368-5376-4834-bb20-c0f4dd7eb824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e9bf5-3c1b-445d-ac07-6c429dec03bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94bc8f2-ebb8-4110-bc82-bca4198cb6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
