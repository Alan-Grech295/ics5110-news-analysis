{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:27.357870Z",
     "start_time": "2025-12-03T07:41:26.984155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union, Any, Callable, Literal\n",
    "\n",
    "from pydantic.main import IncEx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "from anthropic import AnthropicFoundry, beta_tool\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from dotenv import load_dotenv\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "id": "967e254963ec671e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\alang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\alang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:27.376705Z",
     "start_time": "2025-12-03T07:41:27.363248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load datasets as raw CSVs\n",
    "news_articles_df = pd.read_csv(\"data/local_news_articles.csv\")\n",
    "press_releases_df = pd.read_csv(\"data/police_press_releases.csv\")"
   ],
   "id": "96d7bc4a3fd10f1c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:27.426532Z",
     "start_time": "2025-12-03T07:41:27.380904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mapping = str.maketrans({\n",
    "    \"ċ\": \"c\",\n",
    "    \"Ċ\": \"C\",\n",
    "    \"ġ\": \"g\",\n",
    "    \"Ġ\": \"G\",\n",
    "    \"ħ\": \"h\",\n",
    "    \"Ħ\": \"H\",\n",
    "    \"ż\": \"z\",\n",
    "    \"Ż\": \"Z\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"‘\": \"'\",\n",
    "    \"’\": \"'\",\n",
    "    \"—\": \"-\",\n",
    "    \"–\": \"-\",\n",
    "    \"…\": \"...\",\n",
    "})\n",
    "\n",
    "def clean_articles(df: pd.DataFrame, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).str.translate(mapping)\n",
    "    return df\n",
    "\n",
    "news_articles_df = clean_articles(news_articles_df, [\"title\", \"subtitle\", \"content\"])\n",
    "press_releases_df = clean_articles(press_releases_df, [\"title\", \"content\"])"
   ],
   "id": "cab8d8f36780746b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:45.395447Z",
     "start_time": "2025-12-03T07:41:27.430557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accident_verbs = {\"crash\", \"collide\", \"hit\", \"strike\", \"overturn\"}\n",
    "accident_nouns = {\"car\", \"vehicle\", \"truck\", \"lorry\", \"pedestrian\", \"motorcycle\", \"van\", \"bus\", \"bicycle\", \"traffic\"}\n",
    "accident_type_nouns = {\"accident\", \"crash\", \"collision\"}\n",
    "\n",
    "subjects = {\"driver\", \"motorcyclist\", \"motorcycle\", \"biker\", \"pedestrian\", \"cyclist\", \"rider\", \"man\", \"woman\", \"child\"}\n",
    "injury_verbs = {\"hurt\", \"injure\", \"wound\", \"kill\", \"die\", \"collide\", \"crash\", \"strike\", \"hit\", \"rushed\"}\n",
    "injury_adjs = {\"injured\", \"hurt\", \"wounded\", \"dead\", \"killed\", \"hit\"}\n",
    "\n",
    "transport_verbs = {\"rush\", \"take\", \"airlift\", \"transport\", \"bring\", \"escort\"}\n",
    "\n",
    "hospital_targets = {\"hospital\", \"clinic\", \"center\", \"medical\", \"emergency\", \"icu\"}\n",
    "\n",
    "def is_rushed_to_hospital(doc):\n",
    "    for token in doc:\n",
    "        # 1️⃣ Check if token is a transport verb in passive form\n",
    "        if token.lemma_ in transport_verbs and token.dep_ == \"ROOT\":\n",
    "\n",
    "            # Look for passive subject: e.g. \"motorcyclist\" in \"was rushed\"\n",
    "            subj = [child for child in token.children if child.dep_ in {\"nsubjpass\"}]\n",
    "            if not subj:\n",
    "                continue\n",
    "\n",
    "            subj_lemma = subj[0].lemma_\n",
    "            if subj_lemma not in subjects:\n",
    "                continue\n",
    "\n",
    "            for child in token.children:\n",
    "                if child.lemma_ in hospital_targets:\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def refers_to_accident(text: str) -> bool:\n",
    "    doc = nlp(text)\n",
    "    has_subj = False\n",
    "    has_verb = False\n",
    "    has_vehicle = False\n",
    "    for token in doc:\n",
    "        if token.lemma_ in accident_verbs:\n",
    "            for child in token.children:\n",
    "                if child.lemma_ in accident_nouns:\n",
    "                    return True\n",
    "\n",
    "        # 1️⃣ Active voice: \"driver hurt\", \"motorcyclist crashed\"\n",
    "        if token.lemma_ in injury_verbs:\n",
    "            for child in token.children:\n",
    "                if child.lemma_ in subjects and child.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "                    return True\n",
    "\n",
    "        # 2️⃣ Passive voice: \"driver was injured\", \"motorcyclist was hit\"\n",
    "        if token.lemma_ in injury_adjs:\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"nsubj\" and child.lemma_ in subjects:\n",
    "                    return True\n",
    "\n",
    "        # 3️⃣ Copular constructions: \"the motorcyclist is injured\"\n",
    "        # token = adjective (\"injured\"), child = \"motorcyclist\"\n",
    "        if token.lemma_ in injury_adjs:\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"nsubj\" and child.lemma_ in subjects:\n",
    "                    return True\n",
    "\n",
    "        # 4️⃣ Subject → verb pattern: \"driver suffered injuries\"\n",
    "        if token.lemma_ in subjects:\n",
    "            for child in token.children:\n",
    "                if child.lemma_ in injury_verbs and child.dep_ == \"verb\":\n",
    "                    return True\n",
    "\n",
    "        if token.lemma_ in accident_type_nouns:\n",
    "            for child in token.children:\n",
    "                if child.lemma_ in accident_nouns and child.dep_ == \"compound\":\n",
    "                    return True\n",
    "\n",
    "        lemma = token.lemma_.lower()\n",
    "        if token.dep_ in {\"nsubj\", \"nsubjpass\"} and lemma in subjects:\n",
    "                has_subj = True\n",
    "\n",
    "        if lemma in (accident_verbs.union(accident_type_nouns, injury_verbs)):\n",
    "            has_verb = True\n",
    "\n",
    "        if lemma in accident_nouns:\n",
    "            has_vehicle = True\n",
    "\n",
    "    if has_subj and has_verb and has_vehicle:\n",
    "        return True\n",
    "\n",
    "    return is_rushed_to_hospital(doc)\n",
    "\n",
    "non_related_news_article_ids = []\n",
    "\n",
    "for id, text in news_articles_df[[\"id\", \"content\"]].values:\n",
    "    is_accident = refers_to_accident(text)\n",
    "    if not is_accident:\n",
    "        non_related_news_article_ids.append(id)\n",
    "\n",
    "os.makedirs(\"data/intermediate\", exist_ok=True)\n",
    "\n",
    "if not os.path.exists(\"data/intermediate/local_news_articles_exclusions.csv\"):\n",
    "    non_related_news_article_df = news_articles_df[news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "    non_related_news_article_df.to_csv(\"data/intermediate/local_news_articles_exclusions.csv\", index=False)\n",
    "\n",
    "non_related_press_release_ids = []\n",
    "for id, text in press_releases_df[[\"id\", \"content\"]].values:\n",
    "    is_accident = refers_to_accident(text)\n",
    "    if not is_accident:\n",
    "        non_related_press_release_ids.append(id)\n",
    "\n",
    "if not os.path.exists(\"data/intermediate/police_press_releases_exclusions.csv\"):\n",
    "    non_related_press_release_df = press_releases_df[press_releases_df[\"id\"].isin(non_related_press_release_ids)]\n",
    "    non_related_press_release_df.to_csv(\"data/intermediate/police_press_releases_exclusions.csv\", index=False)"
   ],
   "id": "507144bfe0768b4c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T14:37:38.240495Z",
     "start_time": "2025-12-01T14:37:38.238165Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# Police\n",
    "- 19\n",
    "- 86\n",
    "- 109\n",
    "\n",
    "# Local News\n",
    "- 4034\n",
    "- 3989\n",
    "- 3966\n",
    "- 3699\n",
    "- 3563\n",
    "- 2666\n",
    "- 1274\n",
    "- 974\n",
    "- 685\n",
    "- 654\n",
    "- 490008\n",
    "- 490632\n",
    "- 491011\n",
    "- 491334\n",
    "- 491557\n",
    "- 492962\n",
    "- 493044\n",
    "- 494363\n",
    "- 496274\n",
    "\n",
    "What do we define as accident? Does driving without license count? (490267)"
   ],
   "id": "53ec2f1d8751e46c",
   "attachments": {
    "89981b78-c5a5-4636-bf21-30cce1d50b2a.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAA9CAIAAABgC60zAAABUUlEQVR4Xu2UwRHDIAwEXZcLop5UQzP+pBNHIAwCZJJ7xY/bmcyEs0BrYLy9H8kmv/NhUAuBWgjUQqAWArUQqIVALQRqIVALgVoI1EKgFgK1EKiFQC0EaiFQC4FaCNRCoBYCtRCohbDSOl77toV4DWOQYWIPr6OWtFALY9BxY9dqb7ofniutLNW0UrsQ09RD/5ZMm+as9Ld0pWV6e1k3zNxqyXuEcC2q8+o0WS4p1J427KiZfViXckPlRkuqpMj27XcrL9drDQsnPNNcN6Vj6GpJkdM36qnma6QLNFU9xb7d2ErrzfRF6GlJoT37nJke7W7lo75ubLK2u+Xsnub9y/rhrGV2vtX2PdyOQ+jWKO6jPpy00uMR2eCvWsORre5dGblhYdKydKdVb07d8JuDHWbW8VTqhsqvWnnm9OnzsoS5CBduqRsmllr/g1oIz9Z6IB8GJbas+s/OlAAAAABJRU5ErkJggg=="
    }
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:45.488641Z",
     "start_time": "2025-12-03T07:41:45.478633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "news_articles_excluded_ids = pd.read_csv(\"data/intermediate/local_news_articles_exclusions.csv\")[\"id\"]\n",
    "news_articles_df = news_articles_df[~news_articles_df[\"id\"].isin(news_articles_excluded_ids)]\n",
    "\n",
    "press_releases_excluded_ids = pd.read_csv(\"data/intermediate/police_press_releases_exclusions.csv\")[\"id\"]\n",
    "press_releases_df = press_releases_df[~press_releases_df[\"id\"].isin(press_releases_excluded_ids)]"
   ],
   "id": "75072c82489b5f5b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:46.101328Z",
     "start_time": "2025-12-03T07:41:45.493430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "endpoint = \"https://news-analysis-resource.openai.azure.com/openai/v1/\"\n",
    "\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token_provider,\n",
    ")\n",
    "\n",
    "anthropic_client = AnthropicFoundry(\n",
    "    base_url=\"https://news-analysis-resource.openai.azure.com/anthropic\",\n",
    "    api_key=os.getenv(\"AZURE_API_KEY\")\n",
    ")"
   ],
   "id": "bb25c1ae9683630d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:46.120641Z",
     "start_time": "2025-12-03T07:41:46.106831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Gender(Enum):\n",
    "    Male = \"male\"\n",
    "    Female = \"female\"\n",
    "    Other = \"other\"\n",
    "    NA = \"na\"\n",
    "\n",
    "class Injury(Enum):\n",
    "    none = \"none\"\n",
    "    Slight = \"slight\"\n",
    "    Grievous = \"grievous\"\n",
    "    Critical = \"critical\"\n",
    "    Dead = \"dead\"\n",
    "\n",
    "class VehicleType(str, Enum):\n",
    "    Car = \"car\"\n",
    "    Truck = \"truck\"\n",
    "    Bus = \"bus\"\n",
    "    Motorcycle = \"motorcycle\"\n",
    "    Bicycle = \"bicycle\"\n",
    "    Van = \"van\"\n",
    "    Pedestrian = \"pedestrian\"\n",
    "\n",
    "class RoadCondition(str, Enum):\n",
    "    Dry = \"dry\"\n",
    "    Wet = \"wet\"\n",
    "    Flood = \"flood\"\n",
    "    Oil = \"oil\"\n",
    "    Debris = \"debris\"\n",
    "    Uneven = \"uneven\"\n",
    "    Obstructed = \"obstructed\"\n",
    "\n",
    "class HumanFactor(str, Enum):\n",
    "    Drunk = \"drunk\"\n",
    "    Fatigue = \"fatigue\"\n",
    "    Speeding = \"speeding\"\n",
    "\n",
    "class Infrastructure(str, Enum):\n",
    "    Intersection = \"intersection\"\n",
    "    Roundabout = \"roundabout\"\n",
    "    Junction = \"junction\"\n",
    "    Highway = \"highway\"\n",
    "    Residential_Road = \"residential_road\"\n",
    "    Bridge = \"bridge\"\n",
    "    Tunnel = \"tunnel\"\n",
    "    Crosswalk = \"crosswalk\"\n",
    "\n",
    "class AccidentRecord(BaseModel, extra=\"forbid\"):\n",
    "    model_config = ConfigDict(use_enum_values=True)\n",
    "\n",
    "    id: int = Field(description=\"\"\"The id of the accident. This is the same as the given CSV.\"\"\")\n",
    "    date_of_accident: Optional[datetime] = Field(description=\"\"\"The date and time that the accident occurred\"\"\")\n",
    "    num_people_involved: int = Field(description=\"\"\"The number of people involved in the accident\"\"\")\n",
    "    num_drivers: int = Field(description=\"\"\"The number of drivers involved in the accident\"\"\")\n",
    "    driver_ages: List[Optional[int]] = Field(description=\"\"\"The ages of the drivers involved. The length of this list must be the same as `num_drivers`. Leave as None if not available\"\"\")\n",
    "    driver_genders: List[Union[Gender, str]] = Field(description=\"\"\"The genders of the drivers involved in the accident. The length of this list must be the same as `num_drivers`\"\"\")\n",
    "    injury_severities: List[Union[Injury, str]] = Field(description=\"\"\"The severity of injuries of each person. The length of this list must be the same as `num_people_involved`\"\"\")\n",
    "    # accident_type: Literal[\"head-on\", \"rear-end\", \"multi-vehicle\", \"other\"]\n",
    "    # \"\"\"The type of accident\"\"\"\n",
    "    num_vehicles_involved: int = Field(description=\"\"\"The number of vehicles involved in the accident\"\"\")\n",
    "    vehicle_types: List[Union[VehicleType, str]] = Field(description=\"\"\"The types of vehicles involved in the accident. The length of this list must be the same as `num_vehicles_involved`\"\"\")\n",
    "    road_conditions: List[Union[RoadCondition, str]] = Field(description=\"\"\"The condition of the road that the accident occurred\"\"\")\n",
    "    human_factors: List[Union[HumanFactor, str]] = Field(description=\"\"\"The human factors which caused the accident to occur\"\"\")\n",
    "    infrastructure: List[Union[Infrastructure, str]] = Field(description=\"\"\"The infrastructure where the accident occurred\"\"\")\n",
    "    road_name: Optional[str] = Field(description=\"\"\"The name of the road that the accident occurred\"\"\")\n",
    "    locality: Optional[str] = Field(description=\"\"\"The name of the locality that the accident occurred\"\"\")\n",
    "    valid_accident: bool = Field(description=\"\"\"Whether or not the record refers to a vehicle accident\"\"\")\n",
    "\n",
    "    def model_dump(\n",
    "        self,\n",
    "        *,\n",
    "        mode: Literal['json', 'python'] | str = 'python',\n",
    "        include: IncEx | None = None,\n",
    "        exclude: IncEx | None = None,\n",
    "        context: Any | None = None,\n",
    "        by_alias: bool | None = None,\n",
    "        exclude_unset: bool = False,\n",
    "        exclude_defaults: bool = False,\n",
    "        exclude_none: bool = False,\n",
    "        exclude_computed_fields: bool = False,\n",
    "        round_trip: bool = False,\n",
    "        warnings: bool | Literal['none', 'warn', 'error'] = True,\n",
    "        fallback: Callable[[Any], Any] | None = None,\n",
    "        serialize_as_any: bool = False,\n",
    "    ) -> dict[str, Any]:\n",
    "        data = super().model_dump(mode=mode, include=include, exclude=exclude, context=context,\n",
    "                                  by_alias=by_alias, exclude_unset=exclude_unset, exclude_defaults=exclude_defaults,\n",
    "                                  exclude_none=exclude_none, exclude_computed_fields=exclude_computed_fields,\n",
    "                                  round_trip=round_trip, warnings=warnings, fallback=fallback,\n",
    "                                  serialize_as_any=serialize_as_any)\n",
    "        data.pop(\"valid_accident\")\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def empty(id: int):\n",
    "        return AccidentRecord(\n",
    "            id=id,\n",
    "            date_of_accident=None,\n",
    "            num_people_involved=0,\n",
    "            num_drivers=0,\n",
    "            driver_ages=[],\n",
    "            driver_genders=[],\n",
    "            injury_severities=[],\n",
    "            num_vehicles_involved=0,\n",
    "            vehicle_types=[],\n",
    "            road_conditions=[],\n",
    "            human_factors=[],\n",
    "            infrastructure=[],\n",
    "            road_name=None,\n",
    "            locality=None,\n",
    "            valid_accident=False\n",
    "        )\n",
    "\n",
    "\n",
    "class Accidents(BaseModel, extra=\"forbid\"):\n",
    "    accidents: List[AccidentRecord]\n",
    "    \"\"\"A list of all accidents\"\"\"\n",
    "\n",
    "    def to_dict_list(self):\n",
    "        return [a.model_dump() for a in self.accidents if a.valid_accident]"
   ],
   "id": "237e1af0f34acc1c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:46.130153Z",
     "start_time": "2025-12-03T07:41:46.125410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [(\"gpt-5-mini\", 50), (\"grok-4-fast-non-reasoning\", 50), (\"DeepSeek-R1-0528\", 20)]\n",
    "\n",
    "system_msg = lambda dataset_type: f\"\"\"\n",
    "You are given a number of rows of a semi-structured CSV dataset of {dataset_type} about car accidents.\n",
    "Your task is to extract information from the content of each record and return a list of accidents with their corresponding data.\n",
    "Make sure that the number of elements in the list in the returned JSON match the number of rows in the given CSV.\n",
    "\n",
    "IMPORTANT: You MUST return the data by calling the `set_accidents` function.\n",
    "VERY IMPORTANT: YOU MUST RETURN THE SAME NUMBER OF ELEMENTS AS ROWS GIVEN IN THE CSV.\n",
    "If you cannot find a value for a record or field, return a default value:\n",
    "- int: 0\n",
    "- str: \"\"\n",
    "- Optional[...]: None\n",
    "- List: []\n",
    "\n",
    "The ID MUST be copied from the given CSV.\n",
    "If a record is not of a vehicle accident (i.e. of other news), set `valid_accident` to False. Otherwise, set it to True.\n",
    "If you cannot fill in any values, set `valid_accident` to False. DO NOT RETURN A FULLY EMPTY ROW WITH `valid_accident` as True.\n",
    "\n",
    "DO NOT CREATE ANY INFORMATION NOT PRESENT IN THE CSV.\n",
    "DO NOT include any introduction such as \"Here is the answer...\" or conclusion, only the JSON.\n",
    "\"\"\"\n",
    "\n",
    "csv_prompt = lambda dataset_csv, start_rows, end_rows, total_rows, extra=None: f\"\"\"\n",
    "{f\"IMPORTANT: {extra}\" if extra else \"\"}\n",
    "\n",
    "Make sure to return EXACTLY {end_rows - start_rows} rows.\n",
    "\n",
    "Accident CSV Data ({start_rows}-{end_rows}/{total_rows} rows):\n",
    "\n",
    "{dataset_csv}\n",
    "\"\"\"\n",
    "\n",
    "datasets = [\n",
    "    (\"local news articles\", news_articles_df),\n",
    "    (\"police press releases\", press_releases_df),\n",
    "]\n"
   ],
   "id": "6819c4bbbe2fec02",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:46.142210Z",
     "start_time": "2025-12-03T07:41:46.134851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = Path(\"data/llm_processed\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "result_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"set_accidents\",\n",
    "        \"description\": \"Set the accident data obtained from the given CSV\",\n",
    "        \"parameters\": Accidents.model_json_schema()\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [result_function]\n",
    "\n",
    "def to_snake_case(text: str):\n",
    "    return text.lower().replace(\" \", \"_\").replace(\"-\", \"_\")"
   ],
   "id": "c900127611ec6f27",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:40:58.884475Z",
     "start_time": "2025-12-03T07:41:46.149693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tqdm.set_lock(threading.RLock())\n",
    "\n",
    "def process_model(model, num_rows_per_request, dataset_df: pd.DataFrame, system_prompt):\n",
    "    save_path = data_dir / f\"{to_snake_case(dataset_type)}_{to_snake_case(model)}.csv\"\n",
    "    if not os.path.exists(save_path):\n",
    "        open(save_path, \"w\").close()\n",
    "\n",
    "    with tqdm(range(0, len(dataset_df.index), num_rows_per_request), desc=f\"Querying {model}\", total=len(dataset_df.index), unit=\" rows\") as pbar:\n",
    "        i = 0\n",
    "        extra = None\n",
    "        while i < len(dataset_df.index):\n",
    "            try:\n",
    "                start = i\n",
    "                end = min(i + num_rows_per_request, len(dataset_df.index))\n",
    "\n",
    "                df_section: pd.DataFrame = dataset_df.iloc[start:end]\n",
    "                df_section_csv = df_section.to_csv(index=False)\n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": csv_prompt(df_section_csv, start + 1, end + 1, len(dataset_df.index), extra),\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                result: Accidents = Accidents.model_validate_json(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "                for accident in result.accidents:\n",
    "                    if accident.id not in dataset_df[\"id\"].values:\n",
    "                        raise ValueError(f\"ID {accident.id} not in dataset {dataset_type}\")\n",
    "\n",
    "                if len(result.accidents) != end - start:\n",
    "                    cur_ids = set(accident.id for accident in result.accidents)\n",
    "                    df_ids = set(df_section[\"id\"])\n",
    "                    missing_ids = [str(id) for id in df_ids - cur_ids]\n",
    "                    extra = f\"Make sure to include ids: {\", \".join(missing_ids)}\"\n",
    "                    raise ValueError(f\"Received a different number of rows (expected: {end - start}, received: {len(result.accidents)})\")\n",
    "\n",
    "                result_list = result.to_dict_list()\n",
    "                with open(save_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.DictWriter(f, result_list[0].keys())\n",
    "                    if os.path.getsize(save_path) == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    writer.writerows(result_list)\n",
    "\n",
    "                pbar.update(end - start)\n",
    "                i += num_rows_per_request\n",
    "                extra = None\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to query {model}: {e}\")\n",
    "\n",
    "for dataset_type, dataset_df in datasets:\n",
    "    system_prompt = system_msg(dataset_type)\n",
    "    with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_model,\n",
    "                model,\n",
    "                num_rows_per_request,\n",
    "                dataset_df,\n",
    "                system_prompt\n",
    "            )\n",
    "            for model, num_rows_per_request in models\n",
    "        ]\n",
    "\n",
    "        # Optional: wait for all to complete\n",
    "        for f in futures:\n",
    "            f.result()\n"
   ],
   "id": "fc962ef71b2d6b3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Querying grok-4-fast-non-reasoning:   0%|          | 0/252 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04be476396ba4f5a8fd94f3d36967016"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Querying gpt-5-mini:   0%|          | 0/252 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90999553c8bc490096056c7d83ab8b04"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Querying DeepSeek-R1-0528:   0%|          | 0/252 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28d76ad4833c49699a8475e43afc2484"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to query grok-4-fast-non-reasoning: Request timed out.\n",
      "WARNING:root:Failed to query gpt-5-mini: 2 validation errors for Accidents\n",
      "accidents.0.date_of_accident\n",
      "  Input should be a valid datetime or date, unexpected extra characters at the end of the input [type=datetime_from_date_parsing, input_value='2025-10-09 15:07:23.444052+00', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/datetime_from_date_parsing\n",
      "accidents.1.date_of_accident\n",
      "  Input should be a valid datetime or date, unexpected extra characters at the end of the input [type=datetime_from_date_parsing, input_value='2025-10-10 12:22:30.72069+00', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/datetime_from_date_parsing\n",
      "WARNING:root:Failed to query DeepSeek-R1-0528: 'NoneType' object is not subscriptable\n",
      "WARNING:root:Failed to query DeepSeek-R1-0528: 'NoneType' object is not subscriptable\n",
      "WARNING:root:Failed to query grok-4-fast-non-reasoning: Model service is unavailable.\n",
      "WARNING:root:Failed to query grok-4-fast-non-reasoning: Model service is unavailable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Querying gpt-5-mini:   0%|          | 0/111 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a26ba5983564658b1d13a745b57ce99"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Querying grok-4-fast-non-reasoning:   0%|          | 0/111 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "046088ff91db4d0bb1057944dd35428e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Querying DeepSeek-R1-0528:   0%|          | 0/111 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "575ba53a1dfe4c8db518f8a93d0d5a08"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to query DeepSeek-R1-0528: 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:40:59.093734Z",
     "start_time": "2025-12-03T08:40:59.011320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "# Validate/Clean CSVs\n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    replace_vals = {\n",
    "        \"unknown\": \"na\"\n",
    "    }\n",
    "\n",
    "    def replace_values(iter):\n",
    "        return [replace_vals[v] if v in replace_vals else v for v in iter]\n",
    "\n",
    "    df[\"date_of_accident\"] = pd.to_datetime(df[\"date_of_accident\"], format=\"mixed\", dayfirst=True, utc=True)\n",
    "    df[\"driver_ages\"] = df[\"driver_ages\"].apply(lambda x: replace_values(ast.literal_eval(x)))\n",
    "    df[\"driver_genders\"] = df[\"driver_genders\"].apply(lambda x: replace_values(ast.literal_eval(x)))\n",
    "    df[\"injury_severities\"] = df[\"injury_severities\"].apply(lambda x: replace_values(ast.literal_eval(x)))\n",
    "    df[\"vehicle_types\"] = df[\"vehicle_types\"].apply(lambda x: replace_values(ast.literal_eval(x)))\n",
    "    df[\"road_conditions\"] = df[\"road_conditions\"].apply(lambda x: replace_values(sorted(ast.literal_eval(x))))\n",
    "    df[\"human_factors\"] = df[\"human_factors\"].apply(lambda x: replace_values(sorted(ast.literal_eval(x))))\n",
    "    df[\"infrastructure\"] = df[\"infrastructure\"].apply(lambda x: replace_values(sorted(ast.literal_eval(x))))\n",
    "\n",
    "    return df\n",
    "\n",
    "csv_paths = [\n",
    "    data_dir / f\"{to_snake_case(dataset_type)}_{to_snake_case(model)}.csv\" for (dataset_type, _), (model, _) in itertools.product(datasets, models)\n",
    "]\n",
    "\n",
    "dfs = {}\n",
    "for dataset_type, _ in datasets:\n",
    "    dfs[dataset_type] = {}\n",
    "    for model, _ in models:\n",
    "        csv_path = data_dir / f\"{to_snake_case(dataset_type)}_{to_snake_case(model)}.csv\"\n",
    "        dfs[dataset_type][model] = clean_df(pd.read_csv(csv_path))"
   ],
   "id": "57b3268a9e770dac",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:41:00.276603Z",
     "start_time": "2025-12-03T08:40:59.099010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_final_and_confidence(values: np.ndarray, validate_func: Optional[Callable[[Any], bool]]):\n",
    "    total = len(values)\n",
    "    values = [v for v in values if pd.notna(v) and (validate_func is None or validate_func(v))]\n",
    "\n",
    "    if len(values) == 0:\n",
    "        return np.nan, 0\n",
    "\n",
    "    counts = Counter(values)\n",
    "    final_value, freq = counts.most_common(1)[0]\n",
    "\n",
    "    confidence = freq / total\n",
    "\n",
    "    return final_value, confidence\n",
    "\n",
    "def compute_list_final_and_confidence(values: np.ndarray[list], preserve_order: bool, validate_func: Optional[Callable[[Any], bool]]):\n",
    "    total = len(values)\n",
    "    values = [v for v in values if isinstance(v, list)]\n",
    "    if len(values) == 0:\n",
    "        return np.nan, 0\n",
    "\n",
    "    if not preserve_order:\n",
    "        values = [v for vals in values for v in set(vals)]\n",
    "        counts = Counter(values)\n",
    "        final_values, freqs = list(counts.keys()), list(counts.values())\n",
    "        confidences = [freq / total for freq in freqs]\n",
    "        return final_values, confidences\n",
    "    else:\n",
    "        max_index = max(len(v) for v in values)\n",
    "        counters = [Counter([vals[i] for vals in values if len(vals) > i]) for i in range(max_index)]\n",
    "        final_values = [count.most_common(1)[0][0] for count in counters]\n",
    "        confidences = [count.most_common(1)[0][1] / total for count in counters]\n",
    "        return final_values, confidences\n",
    "\n",
    "def combine_field(field_name: str, datasets: dict[str, pd.DataFrame], confidence_func: Callable[[list, Optional[Callable[[Any], bool]]], tuple[Any, Any]], validate_func: Optional[Callable[[Any], bool]] = None) -> pd.DataFrame:\n",
    "    renamed_dfs = []\n",
    "    data_cols = []\n",
    "    for df_name, df in datasets.items():\n",
    "        sub: pd.DataFrame = df[[\"id\", field_name]].copy()\n",
    "        sub = sub.rename(columns={field_name: f\"{field_name}_{df_name}\"})\n",
    "        data_cols.append(f\"{field_name}_{df_name}\")\n",
    "        renamed_dfs.append(sub)\n",
    "\n",
    "    merged = reduce(lambda left, right: pd.merge(left, right, on=\"id\", how=\"outer\"), renamed_dfs)\n",
    "\n",
    "    extra_rows = []\n",
    "    for _, row in merged.iterrows():\n",
    "        values = row[data_cols].values\n",
    "\n",
    "        final_value, confidence = confidence_func(values, validate_func)\n",
    "        if isinstance(confidence, list):\n",
    "            confidence = [f\"{c:.2f}\" for c in confidence]\n",
    "        else:\n",
    "            confidence = f\"{confidence:.2f}\"\n",
    "\n",
    "        extra_rows.append({\n",
    "            f\"{field_name}_final_value\": final_value,\n",
    "            f\"{field_name}_confidence\": confidence,\n",
    "        })\n",
    "\n",
    "    return pd.concat([merged, pd.DataFrame(extra_rows)], axis=1)\n",
    "\n",
    "gt_0 = lambda v: v > 0\n",
    "\n",
    "# TODO: Add field specific comparisons\n",
    "data_cols = [\n",
    "    (\"date_of_accident\", compute_final_and_confidence, None),\n",
    "    (\"num_people_involved\", compute_final_and_confidence, gt_0),\n",
    "    (\"num_drivers\", compute_final_and_confidence, gt_0),\n",
    "    (\"driver_ages\", lambda x, val_func: compute_list_final_and_confidence(x, True, val_func), gt_0),\n",
    "    (\"driver_genders\", lambda x, val_func: compute_list_final_and_confidence(x, True, val_func), None),\n",
    "    (\"injury_severities\", lambda x, val_func: compute_list_final_and_confidence(x, True, val_func), None),\n",
    "    (\"num_vehicles_involved\", compute_final_and_confidence, gt_0),\n",
    "    (\"vehicle_types\", lambda x, val_func: compute_list_final_and_confidence(x, True, val_func), None),\n",
    "    (\"road_conditions\", lambda x, val_func: compute_list_final_and_confidence(x, False, val_func), None),\n",
    "    (\"human_factors\", lambda x, val_func: compute_list_final_and_confidence(x, False, val_func), None),\n",
    "    (\"infrastructure\", lambda x, val_func: compute_list_final_and_confidence(x, False, val_func), None),\n",
    "    (\"road_name\", compute_final_and_confidence, None),\n",
    "    (\"locality\", compute_final_and_confidence, None),\n",
    "]\n",
    "\n",
    "os.makedirs(\"data/merged\", exist_ok=True)\n",
    "\n",
    "for dataset_type, dataset_df in datasets:\n",
    "    dataset_dfs = dfs[dataset_type]\n",
    "    combined_fields = [combine_field(field_name, dataset_dfs, compute_func, validate_func).set_index(\"id\") for field_name, compute_func, validate_func in data_cols]\n",
    "    merged = pd.concat(combined_fields, axis=1).reset_index()\n",
    "    dataset_ids = set(dataset_df[\"id\"])\n",
    "    merged_ids = set(merged[\"id\"])\n",
    "    extra_ids = merged_ids - dataset_ids\n",
    "\n",
    "    if len(extra_ids) > 0:\n",
    "        merged = merged[~merged[\"id\"].isin(extra_ids)]\n",
    "\n",
    "    merged.to_csv(Path(\"data/merged\") / f\"{to_snake_case(dataset_type)}_combined.csv\", index=False)\n"
   ],
   "id": "1acf9c96bda114c8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Review merged\n",
    "- Find empty rows and check if they are relevant\n",
    "- Check non-confident values and manually set the final value\n",
    "\n",
    "# Ambiguities\n",
    "- 751: Date - Only \"night\" was mentioned, no specific time. Set time to 8pm\n",
    "- 1274: Date - Only morning was given and that traffic subsided by 11:15am. Set time to 8am.\n",
    "- 2101: Date - No time mentioned. Leaving time at midnight\n",
    "- 2201: Date - No time mentioned. Leaving time at midnight"
   ],
   "id": "5c0c295ac5f949bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
