{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6343c350fa0753b7",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "This script contains all the code to process the original CSV files into a structured dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a986a59419019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:32:44.562065Z",
     "start_time": "2025-12-12T16:32:27.578150Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import subprocess\n",
    "import logging\n",
    "import os\n",
    "import threading\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Any, TypedDict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from spacy.tokens import Span, Doc, Token\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "from word2number import w2n\n",
    "import fastcoref_spacy\n",
    "from collections import deque\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Downloads spaCy model\n",
    "subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "# Loads spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7bc4a3fd10f1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:32:44.959249Z",
     "start_time": "2025-12-12T16:32:44.570836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load original datasets\n",
    "news_articles_df = pd.read_csv(\"data/original/local_news_articles.csv\")\n",
    "press_releases_df = pd.read_csv(\"data/original/police_press_releases.csv\")\n",
    "\n",
    "# Rename news article `article_id` column to `id`\n",
    "news_articles_df.rename(columns={\"article_id\": \"id\"}, inplace=True)\n",
    "\n",
    "# Add `id` column to police press releases, continuing from the news articles ids\n",
    "start = news_articles_df[\"id\"].max() + 1\n",
    "press_releases_df.insert(0, \"id\", range(start, start + len(press_releases_df)))\n",
    "\n",
    "# We can save the police press releases as is; they are all valid accidents\n",
    "press_releases_df.to_csv(\"data/police_press_releases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c818ab5dfbcec",
   "metadata": {},
   "source": [
    "# 1. Replace special characters\n",
    "## Why?\n",
    "1. Special characters are not always supported by NLP libraries.\n",
    "2. Special characters are not always converted to lowercase successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8d8f36780746b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:32:45.010364Z",
     "start_time": "2025-12-12T16:32:44.964893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map special characters to ASCII\n",
    "mapping = str.maketrans({\n",
    "    \"ċ\": \"c\",\n",
    "    \"Ċ\": \"C\",\n",
    "    \"ġ\": \"g\",\n",
    "    \"Ġ\": \"G\",\n",
    "    \"ħ\": \"h\",\n",
    "    \"Ħ\": \"H\",\n",
    "    \"ż\": \"z\",\n",
    "    \"Ż\": \"Z\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"‘\": \"'\",\n",
    "    \"’\": \"'\",\n",
    "    \"—\": \"-\",\n",
    "    \"–\": \"-\",\n",
    "    \"…\": \"...\",\n",
    "})\n",
    "\n",
    "def clean_articles(df: pd.DataFrame, columns):\n",
    "    \"\"\"Replaces special characters in the given dataframe columns with their ASCII counterparts\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).str.translate(mapping)\n",
    "    return df\n",
    "\n",
    "# Clean the two datasets\n",
    "news_articles_df = clean_articles(news_articles_df, [\"title\", \"subtitle\", \"content\"])\n",
    "press_releases_df = clean_articles(press_releases_df, [\"title\", \"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f54bcfbeff479",
   "metadata": {},
   "source": [
    "# 2.a. Remove non-related articles\n",
    "Some articles in the dataset do not refer to vehicle accidents (e.g. refers to work accidents or new accident prevention policies). We need to remove these.\n",
    "\n",
    "This is done in two ways:\n",
    "1. Matching accident phrases (e.g. car crash)\n",
    "2. Finding reference to a person, vehicle and accident or injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507144bfe0768b4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:32:59.648757Z",
     "start_time": "2025-12-12T16:32:45.016527Z"
    }
   },
   "outputs": [],
   "source": [
    "people_subj = {\"man\", \"woman\", \"child\", \"driver\", \"motorist\", \"motorcyclist\", \"pedestrian\"}\n",
    "vehicles = {\"car\", \"motorcycle\", \"truck\", \"van\", \"bus\", \"bicycle\",\n",
    "            \"Audi\", \"BMW\", \"Chevrolet\", \"Citroen\", \"Dodge\", \"Fiat\", \"Ford\", \"Honda\", \"Hyundai\", \"Isuzu\",\n",
    "            \"Jaguar\", \"Jeep\", \"Kia\", \"Kymco\", \"Mercedes\", \"Mercedes-Benz\", \"Mini\", \"Mitsubishi\", \"Nissan\",\n",
    "            \"Peugeot\", \"Renault\", \"Skoda\", \"Subaru\", \"Suzuki\", \"Toyota\", \"Volkswagen\", \"VW\", \"Volvo\"}\n",
    "accident = {\"accident\", \"crash\", \"collision\"}\n",
    "injuries = {\"injure\", \"die\"}\n",
    "\n",
    "accident_phrases = [\n",
    "    \"car crash\", \"traffic accident\", \"road accident\", \"collision\",\n",
    "    \"crashed\", \"crash\", \"hit by a car\", \"motorcycle accident\",\n",
    "    \"injured in a crash\", \"overturned\", \"run over\", \"lost control\"\n",
    "]\n",
    "\n",
    "accident_matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = [nlp(text) for text in accident_phrases]\n",
    "accident_matcher.add(\"ACCIDENT_PATTERNS\", patterns)\n",
    "\n",
    "def refers_to_accident(text: str) -> bool:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    matches = accident_matcher(doc)\n",
    "\n",
    "    # If any accident phrases are found, assume it is a valid article\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "\n",
    "    has_people = False\n",
    "    has_vehicles = False\n",
    "    has_accident = False\n",
    "    has_injury = False\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ in people_subj:\n",
    "            has_people = True\n",
    "\n",
    "        if token.lemma_ in vehicles:\n",
    "            has_vehicles = True\n",
    "\n",
    "        if token.lemma_ in accident:\n",
    "            has_accident = True\n",
    "\n",
    "        if token.lemma_ in injuries:\n",
    "            has_injury = True\n",
    "\n",
    "        # If people, vehicles and accident or injury is mentioned, assume it is a valid article\n",
    "        if has_people and has_vehicles and (has_accident or has_injury):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# IDs of articles not referring to vehicle accidents\n",
    "non_related_news_article_ids = []\n",
    "\n",
    "for id, text in news_articles_df[[\"id\", \"content\"]].values:\n",
    "    is_accident = refers_to_accident(text)\n",
    "    if not is_accident:\n",
    "        non_related_news_article_ids.append(id)\n",
    "\n",
    "os.makedirs(\"data/intermediate\", exist_ok=True)\n",
    "\n",
    "non_related_news_article_df = news_articles_df[news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "related_news_article_df = news_articles_df[~news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "\n",
    "# Save dataframes as CSVs to view results\n",
    "non_related_news_article_df.to_csv(\"data/intermediate/local_news_articles_exclusions.csv\", index=False)\n",
    "related_news_article_df.to_csv(\"data/intermediate/local_news_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e56a312ffe763b",
   "metadata": {},
   "source": [
    "# 2.b. Using LLMs to flag non-related articles\n",
    "While the previous method works quite well, some articles still get through.\n",
    "To catch these, we pass the remaining articles through 3 LLMs (GPT 5 Mini, Grok 4 Fast, Deepseek R1).\n",
    "\n",
    "The LLMs were set up through [Microsoft Foundry](https://ai.azure.com/) to have a unified API to communicate with different LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25c1ae9683630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialising API\"\"\"\n",
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "endpoint = \"https://news-analysis-resource.openai.azure.com/openai/v1/\"\n",
    "\n",
    "# Need to login using `az login --use-device-code`\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token_provider,\n",
    ")\n",
    "\n",
    "try:\n",
    "    token_provider()\n",
    "    run_cell = True\n",
    "except:\n",
    "    run_cell = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e9ea8a57544a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not run_cell\n",
    "\n",
    "models = [(\"gpt-5-mini\", 50), (\"grok-4-fast-non-reasoning\", 50), (\"DeepSeek-R1-0528\", 20)]\n",
    "\n",
    "class NonAccidentIDs(BaseModel):\n",
    "    ids: List[int] = Field(description=\"A list of ids of news articles that are not accidents\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a senior data scientist reviewing a semi-structured dataset of vehicle accidents news articles. The articles were obtained by simple web scraping (ex: on the tag of the article) which means that some articles do not refer to actual accidents (for example, they refer to new accident/traffic measures).\n",
    "\n",
    "Your job is to review the given accident CSV and return a list of news article IDs that do not refer to accidents.\n",
    "Be very critical! Any article which mentions a specific accident and provides details on it should not be removed.\n",
    "\n",
    "IMPORTANT: You MUST return the data by calling the `set_non_accident_ids` function.\n",
    "\n",
    "Do not return anything other than a function call.\n",
    "\"\"\"\n",
    "\n",
    "csv_prompt = lambda dataset_csv, start_rows, end_rows, total_rows: f\"\"\"\n",
    "MAKE SURE THAT THE RETURNED IDS EXIST IN THIS CSV!\n",
    "\n",
    "Accident CSV Data ({start_rows}-{end_rows}/{total_rows} rows):\n",
    "\n",
    "{dataset_csv}\n",
    "\"\"\"\n",
    "\n",
    "# LLM function definition\n",
    "result_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"set_non_accident_ids\",\n",
    "        \"description\": \"Set the IDs of the news articles which do not refer to an accident\",\n",
    "        \"parameters\": NonAccidentIDs.model_json_schema()\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [result_function]\n",
    "\n",
    "tqdm.set_lock(threading.RLock())\n",
    "\n",
    "def process_model(model, num_rows_per_request, dataset_df: pd.DataFrame, system_prompt) -> set[int]:\n",
    "    total_ids = set()\n",
    "\n",
    "    with tqdm(range(0, len(dataset_df.index), num_rows_per_request), desc=f\"Querying {model}\", total=len(dataset_df.index), unit=\" rows\") as pbar:\n",
    "        i = 0\n",
    "        while i < len(dataset_df.index):\n",
    "            try:\n",
    "                # Get row range as the LLMs cannot process the entire file at once\n",
    "                start = i\n",
    "                end = min(i + num_rows_per_request, len(dataset_df.index))\n",
    "                df_section: pd.DataFrame = dataset_df.iloc[start:end]\n",
    "                df_section_csv = df_section.to_csv(index=False)\n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": csv_prompt(df_section_csv, start + 1, end + 1, len(dataset_df.index)),\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                result: NonAccidentIDs = NonAccidentIDs.model_validate_json(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "\n",
    "                for id in result.ids:\n",
    "                    # Throw an error if a returned ID is not in the dataset\n",
    "                    if id not in dataset_df[\"id\"].values:\n",
    "                        raise ValueError(f\"ID {id} not in dataset\")\n",
    "\n",
    "                total_ids.update(result.ids)\n",
    "\n",
    "                # Update tqdm progress bar\n",
    "                pbar.update(end - start)\n",
    "                i += num_rows_per_request\n",
    "            except Exception as e:\n",
    "                # If we get an error, retry the model (i.e. do not increment i)\n",
    "                logging.warning(f\"Failed to query {model}: {e}\")\n",
    "\n",
    "    return total_ids\n",
    "\n",
    "# Run LLMs in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_model,\n",
    "            model,\n",
    "            num_rows_per_request,\n",
    "            related_news_article_df,\n",
    "            system_prompt\n",
    "        ): model\n",
    "        for model, num_rows_per_request in models\n",
    "    }\n",
    "\n",
    "    model_ids = {}\n",
    "\n",
    "    for f in futures.keys():\n",
    "        result = f.result()\n",
    "        model_ids[futures[f]] = result\n",
    "\n",
    "all_ids = list(model_ids.values())\n",
    "# Combine IDs from all LLMs\n",
    "ids_union = all_ids[0].union(*all_ids[1:])\n",
    "\n",
    "# Save the LLM excluded articles as a CSV to review\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_exclusions.csv\", index=False)\n",
    "# Save the same LLM excluded articles as a CSV. This CSV will be manually checked and modified\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555b459dc1ee45f",
   "metadata": {},
   "source": [
    "# 2.c. Manually review LLM exclusions\n",
    "The LLMs tend to flag valid articles as invalid. Since this is a short list (because we already removed the bulk of invalid articles in step 1), we can go through the list manually and remove the valid articles.\n",
    "\n",
    "## Removed IDs\n",
    "- 3699\n",
    "- 1352\n",
    "- 370\n",
    "- 287\n",
    "- 489424\n",
    "- 491118\n",
    "- 491371\n",
    "- 494102\n",
    "- 495320\n",
    "- 495942\n",
    "- 496362\n",
    "\n",
    "*Note: `local_news_articles_llm_exclusions.csv` contains the excluded rows as given by the LLMs. `local_news_articles_llm_manual_exclusions.csv` contains the same excluded rows given by the LLMs, except that any rows referring to valid accidents were manually removed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eeb00bf883dcff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:33:00.073859Z",
     "start_time": "2025-12-12T16:33:00.032645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Manual removal of valid rows\n",
    "manual_ids = {3699, 1352, 370, 287, 489424, 491118, 491371, 494102, 495320, 495942, 496362}\n",
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "llm_manual_exclusions = llm_manual_exclusions[~llm_manual_exclusions[\"id\"].isin(manual_ids)]\n",
    "try:\n",
    "  llm_manual_exclusions.to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a91b49de2f99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:33:00.118330Z",
     "start_time": "2025-12-12T16:33:00.108900Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "\n",
    "# Filtering the original news articles\n",
    "news_articles_df = related_news_article_df[~related_news_article_df[\"id\"].isin(llm_manual_exclusions[\"id\"])]\n",
    "\n",
    "try:\n",
    "  news_articles_df.to_csv(\"data/local_news_articles.csv\", index=False)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3514f310ae288e",
   "metadata": {},
   "source": [
    " # Per-person data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094fab8bdd44477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if \"patched_fastcoref\" not in {n for n, _ in nlp.pipeline}:\n",
    "    nlp.add_pipe(\"patched_fastcoref\", config={'model_architecture': 'LingMessCoref', 'model_path': 'biu-nlp/lingmess-coref', 'device': device})\n",
    "\n",
    "class CheckDir(Enum):\n",
    "    BACKWARD = 0\n",
    "    FORWARD = 1\n",
    "    ALL = 2\n",
    "\n",
    "def remove_contained_spans(spans: list[tuple[Any, Span]]) -> list[tuple[Any, Span]]:\n",
    "    filtered_spans = []\n",
    "    spans = sorted(spans, key=lambda s: (s[1].end - s[1].start), reverse=True)\n",
    "\n",
    "    for x, span in spans:\n",
    "        contained = False\n",
    "        for _, kept in filtered_spans:\n",
    "            if span.start >= kept.start and span.end <= kept.end:\n",
    "                contained = True\n",
    "                break\n",
    "        if not contained:\n",
    "            filtered_spans.append((x, span))\n",
    "\n",
    "    return list(sorted(filtered_spans, key=lambda s: s[1].start))\n",
    "\n",
    "age_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "age_patterns = [\n",
    "    [{\"LIKE_NUM\": True}, {\"LOWER\": \"year\"}, {\"LOWER\": \"old\"}],\n",
    "    [{\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LOWER\": \"year\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"LOWER\": \"old\"}],\n",
    "    [{\"TEXT\": \",\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}]\n",
    "]\n",
    "\n",
    "age_matcher.add(\"AGE\", age_patterns)\n",
    "\n",
    "severity_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "severity_patterns = [\n",
    "    [{\"LIKE_NUM\": True}, {\"LOWER\": \"year\"}, {\"LOWER\": \"old\"}],\n",
    "    [{\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LOWER\": \"year\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"old\"}],\n",
    "    [{\"TEXT\": \",\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}]\n",
    "]\n",
    "\n",
    "severity_matcher.add(\"SEVERITY\", age_patterns)\n",
    "\n",
    "def get_ages(doc: Doc) -> list[tuple[int, Span]]:\n",
    "    ages = []\n",
    "    matches = age_matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span: Span = doc[start:end]\n",
    "        age_int = -1\n",
    "        for token in span:\n",
    "            if token.like_num:\n",
    "                try:\n",
    "                    age_int = int(token.text)\n",
    "                except ValueError:\n",
    "                    age_int = w2n.word_to_num(token.text)\n",
    "                break\n",
    "        if age_int <= 0 or age_int >= 120:\n",
    "            continue\n",
    "        ages.append((age_int, span))\n",
    "\n",
    "    return ages\n",
    "\n",
    "gender_words = {\n",
    "    \"man\": \"male\", \"male\": \"male\", \"he\": \"male\", \"him\": \"male\", \"husband\": \"male\",\n",
    "    \"woman\": \"female\", \"female\": \"female\", \"she\": \"female\", \"her\": \"female\", \"wife\": \"female\"\n",
    "}\n",
    "\n",
    "def get_genders(doc: Doc) -> list[tuple[str, Span]]:\n",
    "    genders = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ in gender_words:\n",
    "            genders.append((gender_words[token.lemma_], Span(doc, token.i, token.i + 1)))\n",
    "    return genders\n",
    "\n",
    "accepted_injuries = {\n",
    "    \"serious\": \"serious\",\n",
    "    \"slight\": \"minor\",\n",
    "    \"grievous\": \"serious\",\n",
    "    \"light\": \"minor\",\n",
    "    \"critical\": \"serious\",\n",
    "    \"fatal\": \"fatal\",\n",
    "    \"bad\": \"serious\"\n",
    " }\n",
    "\n",
    "def get_severities(doc: Doc) -> list[tuple[str, Span]]:\n",
    "    severities = []\n",
    "    def adverb_to_adj(word: str):\n",
    "        w = word.lower()\n",
    "        if w.endswith(\"ously\"):\n",
    "            return w[:-2]\n",
    "        if w.endswith(\"ally\"):\n",
    "            return w[:-2]\n",
    "        if w.endswith(\"ly\"):\n",
    "            return w[:-2]\n",
    "        return w\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ == \"injure\" or token.lemma_ == \"injury\":\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"advmod\" or child.dep_ == \"amod\":\n",
    "                    injury_adj = adverb_to_adj(child.lemma_)\n",
    "                    if injury_adj not in accepted_injuries:\n",
    "                        continue\n",
    "                    severities.append((accepted_injuries[injury_adj], Span(doc, child.i, child.i + 1)))\n",
    "        elif token.lemma_ in [\"dead\", \"die\", \"death\"]:\n",
    "            severities.append((\"fatal\", Span(doc, token.i, token.i + 1)))\n",
    "\n",
    "    return severities\n",
    "\n",
    "driver_matcher = Matcher(nlp.vocab)\n",
    "vehicle_dict = {\n",
    "    \"car\": \"car\", \n",
    "    \"bus\": \"bus\", \n",
    "    \"taxi\": \"car\", \n",
    "    \"motorcycle\": \"motorcycle\", \n",
    "    \"motorbike\": \"motorcycle\", \n",
    "    \"bike\": \"motorcycle\", \n",
    "    \"moped\": \"motorcycle\", \n",
    "    \"bicycle\": \"bicycle\", \n",
    "    \"van\": \"van\", \n",
    "    \"truck\": \"truck\", \n",
    "    \"scooter\": \"motorcycle\", \n",
    "    \"pedestrian\": \"pedestrian\", \n",
    "    \"vehicle\": None\n",
    "}\n",
    "driver_nouns = {\"motorcyclist\": \"motorcycle\", \"motorist\": \"car\", \"biker\": \"motorcycle\"}\n",
    "driver_patterns = [\n",
    "    [{\"LOWER\": {\"IN\": list(vehicle_dict.keys())}, \"OP\": \"?\"}, {\"LOWER\": \"driver\"}],\n",
    "    [{\"LOWER\": {\"IN\": list(driver_nouns.keys())}}]\n",
    "]\n",
    "driver_matcher.add(\"DRIVER\", driver_patterns)\n",
    "\n",
    "# Derived through ChatGPT and Gemini\n",
    "vehicle_type_map = {\n",
    "    \"Jeep\": \"car\",\n",
    "    \"Skoda Fabia\": \"car\",\n",
    "    \"Toyota Tercel\": \"car\",\n",
    "    \"Peugeot 307\": \"car\",\n",
    "    \"Suzuki SX4\": \"car\",\n",
    "    \"Mercedes\": \"car\",\n",
    "    \"Toyota Hilux\": \"truck\",\n",
    "    \"Ford B Max\": \"car\",\n",
    "    \"Vauxhall\": \"car\",\n",
    "    \"Honda Accord\": \"car\",\n",
    "    \"Dacia Duster\": \"car\",\n",
    "    \"Mazda Demio\": \"car\",\n",
    "    \"Toyota Vitz\": \"car\",\n",
    "    \"Toyota Ractis\": \"car\",\n",
    "    \"BMW 525\": \"car\",\n",
    "    \"BMW 1 Series\": \"car\",\n",
    "    \"Peugeot 306\": \"car\",\n",
    "    \"Toyota Passo\": \"car\",\n",
    "    \"Nissan\": \"car\",\n",
    "    \"Peugeot Partner\": \"van\",\n",
    "    \"Renault Captur\": \"car\",\n",
    "    \"Toyota Aygo\": \"car\",\n",
    "    \"Volkswagen Polo\": \"car\",\n",
    "    \"Seat Leon\": \"car\",\n",
    "    \"Mercedes C180\": \"car\",\n",
    "    \"Mercedes Sprinter\": \"van\",\n",
    "    \"Toyota Funcargo\": \"car\", # or van\n",
    "    \"Honda Civic\": \"car\",\n",
    "    \"Honda Fit\": \"car\",\n",
    "    \"Hyundai Getz\": \"car\",\n",
    "    \"Kia Sportage\": \"car\",\n",
    "    \"Toyota Auris\": \"car\",\n",
    "    \"Toyota Yaris\": \"car\",\n",
    "    \"Subaru Impreza\": \"car\",\n",
    "    \"Volvo XC60\": \"car\",\n",
    "    \"BMW i13\": \"car\",\n",
    "    \"Renault Megane\": \"car\",\n",
    "    \"Hyundai i10\": \"car\",\n",
    "    \"Fiat Scudo\": \"van\",\n",
    "    \"Maserati Levante\": \"car\",\n",
    "    \"Peugeot 2008\": \"car\",\n",
    "    \"BMW 318\": \"car\",\n",
    "    \"BMW\": \"car\",\n",
    "    \"Citroen DS3\": \"car\",\n",
    "    \"Mazda 2\": \"car\",\n",
    "    \"Hyundai i30\": \"car\",\n",
    "    \"Suzuki Baleno\": \"car\",\n",
    "    \"Toyota Aqua\": \"car\",\n",
    "    \"Toyota Passo\": \"car\",\n",
    "    \"Kia Picanto\": \"car\",\n",
    "    \"Yamaha T115\": \"motorcycle\",\n",
    "    \"Kawasaki Ninja\": \"motorcycle\",\n",
    "    \"Kawazaki ZR750\": \"motorcycle\",\n",
    "    \"Honda NSC110\": \"motorcycle\",\n",
    "    \"Hyosung XRX\": \"motorcycle\",\n",
    "    \"Renault Master\": \"van\",\n",
    "    \"Suzuki Alto\": \"car\",\n",
    "    \"Chevrolet Aveo\": \"car\",\n",
    "    \"Toyota Vitz\": \"car\",\n",
    "    \"Suzuki\": \"car\",\n",
    "    \"Renault Captur\": \"car\",\n",
    "    \"Ford Transit\": \"van\",\n",
    "    \"Daihatsu Terios\": \"car\",\n",
    "    \"Vespa\": \"motorcycle\",\n",
    "    \"Surron Light Bee\": \"motorcycle\",\n",
    "    \"Ford Fiesta\": \"car\",\n",
    "    \"Honda CBR\": \"motorcycle\",\n",
    "    \"Mercedes B180\": \"car\",\n",
    "    \"Benelli bike\": \"motorcycle\",\n",
    "    \"Mitsubishi L200\": \"truck\",\n",
    "    \"Kymco\": \"motorcycle\",\n",
    "    \"Alfa Romeo\": \"car\",\n",
    "    \"Jaguar\": \"car\",\n",
    "    \"Toyota Aqua\": \"car\",\n",
    "    \"Honda Fit\": \"car\",\n",
    "    \"Ford Transit\": \"van\",\n",
    "    \"Volvo XC60\": \"car\",\n",
    "    \"Kia Avella\": \"car\",\n",
    "    \"Renault Trafic\": \"van\",\n",
    "    \"Peugeot Tweet\": \"motorcycle\",\n",
    "    \"Peugeot 508\": \"car\",\n",
    "    \"Toyota Corolla\": \"car\",\n",
    "    \"Jaguar XF\": \"car\",\n",
    "    \"Audi Q3\": \"car\",\n",
    "    \"Smart Fortwo\": \"car\",\n",
    "    **vehicle_dict\n",
    "}\n",
    "\n",
    "# Some articles refer to the vehicle type using only part of the name, so we add those as well\n",
    "additional = {}\n",
    "for k, v in vehicle_type_map.items():\n",
    "    words = k.split()\n",
    "    if len(words) > 1:\n",
    "        for word in words:\n",
    "            if word in additional and additional[word] != v:\n",
    "                additional[word] = None\n",
    "                continue\n",
    "            \n",
    "            # Skip short words and non-alphabetic words\n",
    "            if len(word) < 3 or not any(c.isalpha() for c in word):\n",
    "                continue\n",
    "            \n",
    "            additional[word] = v\n",
    "            \n",
    "vehicle_type_map.update(additional)\n",
    "\n",
    "vehicle_type_map = {\n",
    "    k.lower(): v for k, v in vehicle_type_map.items()\n",
    "}\n",
    "\n",
    "vehicle_type_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "vehicle_docs = [nlp.make_doc(name) for name in vehicle_type_map.keys()]\n",
    "max_vehicle_tokens = max(len(doc) for doc in vehicle_docs)\n",
    "vehicle_type_matcher.add(\"VEHICLES\", vehicle_docs)\n",
    "\n",
    "def get_drivers(doc: Doc) -> list[tuple[Optional[str], Span, CheckDir]]:\n",
    "    def get_vehicle_from_vehicle_span(vehicle_span: Span):\n",
    "        vehicle_matches = vehicle_type_matcher(vehicle_span)\n",
    "        if len(vehicle_matches) > 0:\n",
    "            vehicle_spans = spacy.util.filter_spans([doc[start:end] for _, start, end in vehicle_matches])\n",
    "            vehicle_types = [vehicle_type_map[vs.text.lower()] for vs in vehicle_spans]\n",
    "            for i, vehicle_type in enumerate(vehicle_types):\n",
    "                if vehicle_type is not None:\n",
    "                    return vehicle_types[i]\n",
    "            return \"unknown\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    drivers = []\n",
    "\n",
    "    matches = driver_matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span: Span = doc[start:end]\n",
    "\n",
    "        vehicle_type = None\n",
    "        for token in span:\n",
    "            if token.lower_ in vehicle_dict:\n",
    "                vehicle_type = vehicle_dict[token.lower_]\n",
    "                break\n",
    "            if token.lower_ in driver_nouns:\n",
    "                vehicle_type = driver_nouns[token.lower_]\n",
    "                break\n",
    "\n",
    "        drivers.append((vehicle_type, Span(doc, start, end), CheckDir.ALL))\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\" and token.lemma_.startswith((\"driv\", \"rid\")):\n",
    "            if token.lemma_.startswith(\"rid\") and Span(doc, token.i, token.i + 2).text.lower() == \"riding pillion\":\n",
    "                continue\n",
    "            span_start = min(token.i, token.head.i)\n",
    "            span_end = token.i + 1\n",
    "            for child in list(token.head.children) + list(token.children):\n",
    "                if child.dep_ in [\"compound\", \"nsubjpass\", \"dobj\"]:\n",
    "                    span_start = min(span_start, child.i)\n",
    "                    span_end = max(span_end, child.i)\n",
    "\n",
    "            vehicle_span = Span(doc, span_start, span_end + 1)\n",
    "            vehicle_type = None\n",
    "            for tok in vehicle_span:\n",
    "                if tok.lower_ in vehicle_dict:\n",
    "                    vehicle_type = vehicle_dict[tok.lower_]\n",
    "                    break\n",
    "\n",
    "            if vehicle_type is None:\n",
    "                vehicle_type = get_vehicle_from_vehicle_span(vehicle_span)\n",
    "                # If we have an invalid vehicle span\n",
    "                if vehicle_type is None:\n",
    "                    # Increase span size, maybe we missed part of the name\n",
    "                    vehicle_span.start = max(0, vehicle_span.start - max_vehicle_tokens)\n",
    "                    vehicle_span.end = min(len(doc), vehicle_span.end + max_vehicle_tokens)\n",
    "                    vehicle_type = get_vehicle_from_vehicle_span(vehicle_span)\n",
    "                    vehicle_type = vehicle_type if vehicle_type is not None else vehicle_span.text\n",
    "\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"agent\":\n",
    "                    for c in child.children:\n",
    "                        if c.dep_ == \"pobj\":\n",
    "                            span_end = max(span_end, c.i)\n",
    "\n",
    "            tense = token.morph.get(\"Tense\")\n",
    "            dir = CheckDir.ALL\n",
    "            if \"Past\" in tense:\n",
    "                dir = CheckDir.FORWARD\n",
    "            elif \"Pres\" in tense:\n",
    "                dir = CheckDir.BACKWARD\n",
    "\n",
    "            span = Span(doc, span_start, span_end)\n",
    "            drivers.append((vehicle_type, span, dir))\n",
    "            # for parent in token.ancestors:\n",
    "            #     if parent\n",
    "\n",
    "    return drivers\n",
    "\n",
    "drunk_phrases = [\n",
    "    \"drunk\", \"drunk driving\", \"driving under the influence\", \"under the influence\", \"intoxicated driving\",\n",
    "    \"drink-driver\", \"drink driving\", \"intoxicated driver\", \"DUI\", \"DWI\"\n",
    "]\n",
    "drunk_phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "drunk_phrase_matcher.add(\"DRUNK_PHRASES\", [nlp(text) for text in drunk_phrases])\n",
    "\n",
    "drunk_patterns = [\n",
    "    [{\"LOWER\": {\"NOT_IN\": [\"not\", \"never\", \"no\"]}}, {\"LOWER\": \"over\"}, {\"LOWER\": \"the\"}, {\"LOWER\": {\"IN\": [\"alcohol\", \"legal\"]}}, {\"LOWER\": \"limit\"}],\n",
    "]\n",
    "drunk_pattern_matcher = Matcher(nlp.vocab)\n",
    "drunk_pattern_matcher.add(\"DRUNK_PATTERNS\", drunk_patterns)\n",
    "\n",
    "def get_driver_states(doc: Doc) -> list[tuple[str, Span]]:\n",
    "    states = []\n",
    "    matches = drunk_phrase_matcher(doc) + drunk_pattern_matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span: Span = doc[start:end]\n",
    "        states.append((\"drunk\", span))\n",
    "    return states\n",
    "\n",
    "person_matcher = Matcher(nlp.vocab)\n",
    "person_nouns = [\"man\", \"woman\", \"boy\", \"girl\", \"teen\", \"teenager\", \"baby\", \"person\", \"driver\", \"passenger\",\n",
    "    \"pedestrian\", \"victim\", \"motorcyclist\", \"cyclist\", \"rider\",\n",
    "    \"motorist\", \"resident\", \"teenager\", \"youth\"]\n",
    "vehicle_nouns = [\"car\", \"motorcycle\", \"truck\", \"van\", \"bus\", \"bicycle\"]\n",
    "person_patterns = [\n",
    "    [{\"LOWER\": {\"IN\": [\"a\", \"an\", \"the\", \"one\"]}, \"OP\": \"?\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LOWER\": \"year\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"LOWER\": \"old\"}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"LOWER\": {\"IN\": person_nouns}, \"OP\": \"?\"}],\n",
    "    [{\"LOWER\": {\"IN\": [\"a\", \"an\", \"the\", \"one\"]}}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"LOWER\": {\"IN\": person_nouns}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"a\", \"an\", \"the\", \"one\"]}}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"LOWER\": {\"IN\": vehicle_nouns}}, {\"LOWER\": \"driver\"}],\n",
    "    [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"TEXT\": \",\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}],\n",
    "]\n",
    "\n",
    "person_matcher.add(\"PERSON\", person_patterns)\n",
    "\n",
    "def get_people(doc: Doc) -> list[list[tuple[str, Span]]]:\n",
    "    matches = person_matcher(doc)\n",
    "    cluster_spans: list[list[Span]] = [\n",
    "        [doc.char_span(start, end, alignment_mode=\"expand\") for start, end in cluster] for cluster in doc._.coref_clusters\n",
    "    ]\n",
    "\n",
    "    people = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        people.append((span.text, span))\n",
    "\n",
    "    cleaned = remove_contained_spans(people)\n",
    "\n",
    "    def calc_overlap(a: Span, b: Span):\n",
    "        start = max(a.start, b.start)\n",
    "        end = min(a.end, b.end)\n",
    "\n",
    "        if end <= start:\n",
    "            return 0\n",
    "\n",
    "        return end - start\n",
    "\n",
    "    cluster_indices = []\n",
    "    for text, span in cleaned:\n",
    "        max_overlap = 0\n",
    "        max_overlap_i = -1\n",
    "        for i in range(len(cluster_spans)):\n",
    "            max_cluster_overlap = max(calc_overlap(c, span) for c in cluster_spans[i])\n",
    "            if max_cluster_overlap > max_overlap:\n",
    "                max_overlap = max_cluster_overlap\n",
    "                max_overlap_i = i\n",
    "\n",
    "        cluster_indices.append(max_overlap_i)\n",
    "\n",
    "    clustered_people = [\n",
    "        [(c.text, c) for c in cluster]\n",
    "        for i, cluster in enumerate(cluster_spans) if i in cluster_indices\n",
    "    ]\n",
    "\n",
    "    for i, person in zip(cluster_indices, cleaned):\n",
    "        if i >= 0:\n",
    "            continue\n",
    "\n",
    "        clustered_people.append([person])\n",
    "\n",
    "    return clustered_people\n",
    "\n",
    "def get_span_ancestors(span: Span):\n",
    "    def get_ancestors(token: Token):\n",
    "        ancestors: dict[Token, int] = {token: 0}\n",
    "        depth = 1\n",
    "        current = token.head\n",
    "        \n",
    "        # Walk upward until we reach the root token\n",
    "        while current.dep_ != \"ROOT\":\n",
    "            ancestors[current] = depth\n",
    "            depth += 1\n",
    "            current = current.head\n",
    "            \n",
    "        ancestors[current] = depth  # Add the root token\n",
    "                \n",
    "        return ancestors\n",
    "    \n",
    "    ancestors = {}\n",
    "    for tok in span:\n",
    "        anc = get_ancestors(tok)\n",
    "        # Combine ancestor depths, taking the minimum depth for each ancestor\n",
    "        ancestors = {\n",
    "            k: min(anc.get(k, float('inf')), ancestors.get(k, float('inf')))\n",
    "            for k in set(anc) | set(ancestors)\n",
    "        }\n",
    "        \n",
    "    return ancestors\n",
    "\n",
    "def get_people_ancestors(people: list[dict[str, Span]]):\n",
    "    ancestors: list[dict[str, Span]] = []\n",
    "    \n",
    "    for person in people:\n",
    "        person_ancestors = {}\n",
    "        for _, mention in person:\n",
    "            mention_ancestors = get_span_ancestors(mention)\n",
    "            person_ancestors = {\n",
    "                k: min(mention_ancestors.get(k, float('inf')), person_ancestors.get(k, float('inf')))\n",
    "                for k in set(mention_ancestors.keys()) | set(person_ancestors.keys())\n",
    "            }\n",
    "                \n",
    "        ancestors.append(person_ancestors)\n",
    "        \n",
    "    return ancestors\n",
    "\n",
    "\n",
    "def assign_feature_to_person(feature: Span, people: list[list[tuple[str, Span]]], people_ancestors: list[dict[Token, int]], tree_keywords: list[str] = None, check_dir: CheckDir = CheckDir.ALL) -> int:\n",
    "    # Try assigning feature based on dependency tree relationships first\n",
    "    feature_ancestors = get_span_ancestors(feature)\n",
    "    min_depth = 10**9\n",
    "    assigned_person_i = -1\n",
    "    for i, anc in enumerate(people_ancestors):\n",
    "        # Check if any ancestor tokens are shared between the feature and the person mentions\n",
    "        common_ancestors = set(feature_ancestors.keys()).intersection(set(anc.keys()))\n",
    "        if tree_keywords and len({t.lemma_ for t in common_ancestors}.intersection(tree_keywords)) > 0:\n",
    "            keywords = [t for t in common_ancestors if t.lemma_ in tree_keywords]\n",
    "            assigned_person = False\n",
    "            for key in keywords:\n",
    "                if check_dir == CheckDir.BACKWARD and feature.end < key.i:\n",
    "                    continue\n",
    "                elif check_dir == CheckDir.FORWARD and feature.start > key.i:\n",
    "                    continue\n",
    "                assigned_person_i = i\n",
    "                assigned_person = True\n",
    "                break\n",
    "            \n",
    "            if assigned_person:\n",
    "                break\n",
    "\n",
    "        person_min_depth = 10**9\n",
    "        for key in common_ancestors:\n",
    "            if check_dir == CheckDir.BACKWARD and feature.end < key.i:\n",
    "                continue\n",
    "            elif check_dir == CheckDir.FORWARD and feature.start > key.i:\n",
    "                continue\n",
    "            person_min_depth = min(person_min_depth, anc[key] + feature_ancestors[key])\n",
    "            \n",
    "        if person_min_depth < min_depth:\n",
    "            min_depth = person_min_depth\n",
    "            assigned_person_i = i\n",
    "            \n",
    "    if assigned_person_i >= 0:\n",
    "        return assigned_person_i\n",
    "    \n",
    "    # If not tree related, fallback to distance-based assignment\n",
    "    def span_distance(span_a: Span, span_b: Span, signed: bool = False):\n",
    "        \"\"\"\n",
    "        span_a, span_b are (start, end) token index tuples\n",
    "        representing half-open intervals [start, end)\n",
    "        \"\"\"\n",
    "        a_start, a_end = span_a.start, span_a.end\n",
    "        b_start, b_end = span_b.start, span_b.end\n",
    "\n",
    "        # If spans overlap or touch, distance = 0\n",
    "        if a_end >= b_start and b_end >= a_start:\n",
    "            return 0\n",
    "\n",
    "        # Otherwise distance is the gap between them\n",
    "        if a_end < b_start:\n",
    "            return b_start - a_end\n",
    "        else:\n",
    "            return -(a_start - b_end) if signed else a_start - b_end\n",
    "\n",
    "    f_sent = feature.sent\n",
    "\n",
    "    best_person_i = -1\n",
    "    best_dist = 10**9\n",
    "\n",
    "    for i, person in enumerate(people):\n",
    "        for _, mention in person:\n",
    "            if check_dir == CheckDir.BACKWARD and feature.end < mention.start:\n",
    "                continue\n",
    "            elif check_dir == CheckDir.FORWARD and feature.start > mention.end:\n",
    "                continue\n",
    "            sent_dist = min(\n",
    "                abs(mention.sent.start - f_sent.start), # Same sentence\n",
    "                abs(mention.sent.start - f_sent.end),   # Next sentence\n",
    "                abs(f_sent.sent.start - mention.end)    # Previous sentence\n",
    "            )\n",
    "\n",
    "            # def negative_to_max(x):\n",
    "            #     return x if x >= 0 else 10 ** 9\n",
    "\n",
    "            # sent_dist = min(\n",
    "            #     negative_to_max(mention.sent.start - f_sent.start),\n",
    "            #     negative_to_max(f_sent.sent.start - mention.end)\n",
    "            # )\n",
    "\n",
    "            if sent_dist <= 1:\n",
    "                token_dist = span_distance(mention, feature)\n",
    "                if 0 <= token_dist < best_dist:\n",
    "                    best_person_i = i\n",
    "                    best_dist = token_dist\n",
    "\n",
    "    return best_person_i\n",
    "\n",
    "test_doc = nlp(\"The police said in a statement the accident was reported at 8.30pm and involved a Toyota Tercel driven by an 86-year-old woman from Xewkija and a Benelli bike driven by a 45-year-old from Sannat\")\n",
    "ppl = get_people(test_doc)\n",
    "state = get_drivers(test_doc)\n",
    "ppl_ancestors = get_people_ancestors(ppl)\n",
    "for state_value, span, dir in state:\n",
    "    idx = assign_feature_to_person(span, ppl, ppl_ancestors, check_dir=dir)\n",
    "    print(f\"Assigned state {state_value} to person {idx}\")\n",
    "displacy.render(test_doc)\n",
    "\n",
    "@dataclass\n",
    "class ValueSpan[T]:\n",
    "    value: T\n",
    "    span: Span\n",
    "\n",
    "@dataclass\n",
    "class PersonData:\n",
    "    article_id: int\n",
    "    person_id: int = -1\n",
    "    # name: Optional[ValueSpan[str]] = None\n",
    "    age: Optional[ValueSpan[int]] = None\n",
    "    gender: Optional[ValueSpan[str]] = None\n",
    "    severity: Optional[ValueSpan[str]] = field(default_factory=lambda: ValueSpan(\"none\", None))\n",
    "    is_driver: Optional[ValueSpan[bool]] = field(default_factory=lambda: ValueSpan(False, None))\n",
    "    vehicle_type: Optional[ValueSpan[str]] = None\n",
    "    person_state: Optional[ValueSpan[str]] = None\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        vars = self.__dict__.copy()\n",
    "        vars.pop(\"article_id\")\n",
    "        vars.pop(\"person_id\")\n",
    "        dict_vars = {}\n",
    "        for k, v in vars.items():\n",
    "            dict_vars[k] = v.value if v is not None else None\n",
    "            dict_vars[f\"{k}_span\"] = (v.span.start, v.span.end) if v is not None and v.span is not None else None\n",
    "        return {\n",
    "            \"person_id\": self.person_id,\n",
    "            \"article_id\": self.article_id,\n",
    "            **dict_vars\n",
    "        }\n",
    "\n",
    "    def combine(self, other: \"PersonData\") -> \"PersonData\":\n",
    "        vars = self.__dict__.copy()\n",
    "        vars.pop(\"article_id\")\n",
    "        vars.pop(\"person_id\")\n",
    "        \n",
    "        default = PersonData(article_id=self.article_id)\n",
    "\n",
    "        for var in vars.keys():\n",
    "            if vars[var] != getattr(default, var):\n",
    "                continue\n",
    "\n",
    "            setattr(self, var, getattr(other, var))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def is_empty(self) -> bool:\n",
    "        vars = self.__dict__.copy()\n",
    "        vars.pop(\"article_id\")\n",
    "        vars.pop(\"person_id\")\n",
    "\n",
    "        default = PersonData(article_id=self.article_id)\n",
    "\n",
    "        for var in vars.keys():\n",
    "            if vars[var] != getattr(default, var):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "def compute_person_similarity(person1: PersonData, person2: PersonData) -> int:\n",
    "    similarity = 0\n",
    "\n",
    "    def is_same(a, b, default=None):\n",
    "        return a is not None and b is not None and (default is None or (a.value != default and b.value != default)) and a.value == b.value\n",
    "\n",
    "    def is_different(a, b, default=None):\n",
    "        return a is not None and b is not None and (default is None or (a.value != default and b.value != default)) and a.value != b.value\n",
    "\n",
    "    # if is_same(person1.name, person2.name):\n",
    "    #     similarity += 10\n",
    "    # elif is_different(person1.name, person2.name):\n",
    "    #     return -1\n",
    "\n",
    "    if is_same(person1.age, person2.age):\n",
    "        similarity += 2\n",
    "    elif is_different(person1.age, person2.age):\n",
    "        return -1\n",
    "\n",
    "    if is_same(person1.gender, person2.gender):\n",
    "        similarity += 1\n",
    "    elif is_different(person1.gender, person2.gender):\n",
    "        return -1\n",
    "\n",
    "    if is_same(person1.severity, person2.severity):\n",
    "        similarity += 1\n",
    "    elif is_different(person1.severity, person2.severity, \"none\"):\n",
    "        return -1\n",
    "\n",
    "    # No elif for is_driver as it is automatically false if no driver is mentioned\n",
    "    if is_same(person1.is_driver, person2.is_driver):\n",
    "        similarity += 1\n",
    "\n",
    "    if is_same(person1.vehicle_type, person2.vehicle_type):\n",
    "        similarity += 1\n",
    "    elif is_different(person1.vehicle_type, person2.vehicle_type):\n",
    "        return -1\n",
    "    \n",
    "    if is_same(person1.person_state, person2.person_state):\n",
    "        similarity += 2\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def build_person_graph(article_person_data: list[PersonData], threshold = 3):\n",
    "    person_adj_graph = {i: [] for i in range(len(article_person_data))}\n",
    "\n",
    "    for i in range(len(article_person_data) - 1):\n",
    "        for j in range(i + 1, len(article_person_data)):\n",
    "            sim = compute_person_similarity(article_person_data[i], article_person_data[j])\n",
    "            if sim >= threshold:\n",
    "                person_adj_graph[i].append(j)\n",
    "                person_adj_graph[j].append(i)\n",
    "\n",
    "    return person_adj_graph\n",
    "\n",
    "def get_connected_people(article_person_data: list[PersonData], threshold = 3):\n",
    "    person_adj_graph = build_person_graph(article_person_data, threshold)\n",
    "    visited = set()\n",
    "    components = []\n",
    "\n",
    "    for node in person_adj_graph:\n",
    "        if node not in visited:\n",
    "            stack = [node]\n",
    "            comp = []\n",
    "            visited.add(node)\n",
    "\n",
    "            while stack:\n",
    "                cur = stack.pop()\n",
    "                comp.append(cur)\n",
    "                for n in person_adj_graph[cur]:\n",
    "                    if n not in visited:\n",
    "                        visited.add(n)\n",
    "                        stack.append(n)\n",
    "\n",
    "            components.append(comp)\n",
    "\n",
    "    return components\n",
    "\n",
    "def combine_people(article_person_data: list[PersonData], connected_persons: list[list[int]]) -> list[PersonData]:\n",
    "    combined_data = []\n",
    "\n",
    "    for connected in connected_persons:\n",
    "        person = article_person_data[connected[0]]\n",
    "        if len(connected) > 1:\n",
    "            for idx in connected[1:]:\n",
    "                person.combine(article_person_data[idx])\n",
    "\n",
    "        combined_data.append(person)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def process_article(row):\n",
    "    content = row[\"content\"]\n",
    "    doc = nlp(content)\n",
    "\n",
    "    # displacy.render(doc)\n",
    "    ages = get_ages(doc)\n",
    "    genders = get_genders(doc)\n",
    "    severities = get_severities(doc)\n",
    "    drivers = get_drivers(doc)\n",
    "    driver_states = get_driver_states(doc)\n",
    "    people = get_people(doc)\n",
    "    people_ancestors = get_people_ancestors(people)\n",
    "\n",
    "    article_person_data: list[PersonData] = [PersonData(article_id=row[\"id\"]) for _ in people]\n",
    "\n",
    "    # Check gender\n",
    "    for i, person in enumerate(people):\n",
    "        gender_list = []\n",
    "        gender_spans = {}\n",
    "        for _, mention in person:\n",
    "            for token in mention:\n",
    "                if token.lemma_ not in gender_words:\n",
    "                    continue\n",
    "\n",
    "                gender = gender_words[token.lemma_]\n",
    "                gender_list.append(gender)\n",
    "                gender_spans[gender] = Span(doc, token.i, token.i+1)\n",
    "\n",
    "        if len(gender_list) > 0:\n",
    "            count = Counter(gender_list)\n",
    "            common_gender = count.most_common(1)[0][0]\n",
    "            article_person_data[i].gender = ValueSpan(value=common_gender, span=gender_spans[common_gender])\n",
    "\n",
    "    for txt, span in severities:\n",
    "        person_idx = assign_feature_to_person(span, people, people_ancestors)\n",
    "        if person_idx >= 0:\n",
    "            article_person_data[person_idx].severity = ValueSpan(value=txt, span=span)\n",
    "\n",
    "    for age, span in ages:\n",
    "        person_idx = assign_feature_to_person(span, people, people_ancestors)\n",
    "        if person_idx >= 0:\n",
    "            article_person_data[person_idx].age = ValueSpan(value=age, span=span)\n",
    "\n",
    "    for gender, span in genders:\n",
    "        person_idx = assign_feature_to_person(span, people, people_ancestors)\n",
    "        if person_idx >= 0 and article_person_data[person_idx].gender is None:\n",
    "            # assert article_person_data[person_idx].gender is None or gender == article_person_data[person_idx].gender.value\n",
    "            article_person_data[person_idx].gender = ValueSpan(value=gender, span=span)\n",
    "\n",
    "    for vehicle_type, span, dir in drivers:\n",
    "        person_idx = assign_feature_to_person(span, people, people_ancestors, tree_keywords=[\"drive\", \"driver\"], check_dir=dir)\n",
    "        if person_idx >= 0:\n",
    "            article_person_data[person_idx].is_driver = ValueSpan(value=True, span=span)\n",
    "            article_person_data[person_idx].vehicle_type = ValueSpan(value=vehicle_type, span=span)\n",
    "            \n",
    "    for state, span in driver_states:\n",
    "        person_idx = assign_feature_to_person(span, people, people_ancestors)\n",
    "        if person_idx >= 0:\n",
    "            article_person_data[person_idx].person_state = ValueSpan(value=state, span=span)\n",
    "\n",
    "    connected_people = get_connected_people(article_person_data, threshold=2)\n",
    "    article_person_data = combine_people(article_person_data, connected_people)\n",
    "\n",
    "    return article_person_data\n",
    "\n",
    "max_workers = math.ceil(os.cpu_count() / 2) if device == 'cuda' else 1\n",
    "\n",
    "def process_dataset(df: pd.DataFrame) -> list[PersonData]:\n",
    "    person_data: list[PersonData] = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for idx, row in df.iterrows():\n",
    "            futures.append(executor.submit(process_article, row))\n",
    "\n",
    "        for f in tqdm(futures, desc=\"Processing articles\"):\n",
    "            article_person_data = f.result()\n",
    "            person_data.extend(article_person_data)\n",
    "            \n",
    "    person_data = [p for p in person_data if not p.is_empty()]\n",
    "    for i, person in enumerate(person_data):\n",
    "        person.person_id = i\n",
    "        \n",
    "    return person_data\n",
    "\n",
    "def save_article(data: list[PersonData], filename: str):\n",
    "    person_dict_data = [p.to_dict() for p in data]\n",
    "    with open(f\"data/intermediate/{filename}.csv\", \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, person_dict_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(person_dict_data)\n",
    "        \n",
    "    if not os.path.exists(f\"data/intermediate/{filename}_manual.csv\"):\n",
    "        with open(f\"data/intermediate/{filename}_manual.csv\", \"w\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, person_dict_data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(person_dict_data)\n",
    "\n",
    "news_article_person_data = process_dataset(news_articles_df)\n",
    "press_releases_person_data = process_dataset(press_releases_df)\n",
    "save_article(news_article_person_data, \"local_news_articles_persons\")\n",
    "save_article(press_releases_person_data, \"police_press_releases_persons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7b5dd",
   "metadata": {},
   "source": [
    "# Highlighted article visualisation\n",
    "To help with manual refinement of the dataset, the articles are outputted as HTML files for the following reasons:\n",
    "1. Features in the article are highlighted, colour coded based on the person they refer to. This helps with quickly identifying whether features are assigned to the right person.\n",
    "2. Each token shows its token index when hovered to help with manually inputting features and the corresponding token spans.\n",
    "3. Additional data not in the datasets (ex: specifying which persons are referred to in the article) is displayed to provide all necessary data for efficient manual validation and refinement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import html\n",
    "import random\n",
    "\n",
    "if \"patched_fastcoref\" in nlp.pipe_names:\n",
    "    nlp.remove_pipe(\"patched_fastcoref\")\n",
    "\n",
    "angle = 0\n",
    "step = 40\n",
    "def random_pastel():\n",
    "    global angle, step\n",
    "    angle += step\n",
    "    return f\"hsl({angle}, 80%, 85%)\"\n",
    "\n",
    "def collect_person_spans(person: PersonData):\n",
    "    spans = []\n",
    "\n",
    "    def add(label, vs: ValueSpan):\n",
    "        if vs and vs.span:\n",
    "            start_char, end_char = vs.span.start_char, vs.span.end_char\n",
    "            spans.append({\n",
    "                \"start\": start_char,\n",
    "                \"end\": end_char,\n",
    "                \"start_token\": vs.span.start,\n",
    "                \"end_token\": vs.span.end,\n",
    "                \"label\": label,\n",
    "                \"value\": vs.value\n",
    "            })\n",
    "\n",
    "    add(\"age\", person.age)\n",
    "    add(\"gender\", person.gender)\n",
    "    add(\"severity\", person.severity)\n",
    "    add(\"driver\", person.is_driver)\n",
    "    add(\"vehicle\", person.vehicle_type)\n",
    "    add(\"state\", person.person_state)\n",
    "\n",
    "    return spans\n",
    "\n",
    "def tokenise_text(doc: Doc, start: int, end: int):\n",
    "    span = doc.char_span(start, end, alignment_mode=\"expand\")\n",
    "    text = \"\"\n",
    "    for token in span:\n",
    "        text += f\"<span class='token' title='Token {token.i}'>{html.escape(token.text)}</span>\"\n",
    "    return text\n",
    "\n",
    "def highlight_article(article: str, people: list[PersonData], other_spans: list[Span] = None):\n",
    "    # text = html.escape(article)\n",
    "    doc = nlp(article)\n",
    "    \n",
    "    highlights = []\n",
    "    color_map = {}\n",
    "    \n",
    "    for person in people:\n",
    "        color_map[person.person_id] = random_pastel()\n",
    "        \n",
    "        tooltip = []\n",
    "        for field in [\n",
    "            \"age\", \"gender\", \"severity\",\n",
    "            \"is_driver\", \"vehicle_type\", \"person_state\"\n",
    "        ]:\n",
    "            vs = getattr(person, field)\n",
    "            if vs and vs.value is not None:\n",
    "                tooltip.append(f\"{field}: {vs.value}\")\n",
    "                \n",
    "        tooltip_text = \" | \".join(tooltip)\n",
    "        \n",
    "        for span in collect_person_spans(person):\n",
    "            highlights.append({\n",
    "                \"start\": span[\"start\"],\n",
    "                \"end\": span[\"end\"],\n",
    "                \"start_token\": span[\"start_token\"],\n",
    "                \"end_token\": span[\"end_token\"],\n",
    "                \"color\": color_map[person.person_id],\n",
    "                \"tooltip\": tooltip_text,\n",
    "                \"person_id\": person.person_id\n",
    "            })\n",
    "            \n",
    "    highlights.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "    text = \"\"\n",
    "    prev_end = 0\n",
    "    for h in highlights:\n",
    "        s, e = h[\"start\"], h[\"end\"]\n",
    "        if prev_end >= e or prev_end >= s:\n",
    "            continue\n",
    "        text += tokenise_text(doc, prev_end, s) + f\"<span class='hl' style='background:{h['color']}' \" + f\"title='Person {h['person_id']} | {h['tooltip']} | {h['start_token']} - {h['end_token']}'>\" + article[s:e] + \"</span>\"\n",
    "        prev_end = e\n",
    "    text += tokenise_text(doc, prev_end, len(article))\n",
    "\n",
    "    return f\"<div class='highlighted-article'>{text}</div>\"\n",
    "\n",
    "def generate_html(articles: dict[int, str], people: list[PersonData], output_file=\"highlighted_articles.html\"):\n",
    "    rows = []\n",
    "\n",
    "    for article_id, content in tqdm(articles.items(), desc=\"Generating HTML\", total=len(articles)):\n",
    "        article_people = [p for p in people if p.article_id == article_id]\n",
    "        people_text = \"\"\n",
    "        if len(article_people) > 0:\n",
    "            if len(article_people) == 1:\n",
    "                people_text = f\"{article_people[0].person_id} (1 person)\"\n",
    "            else:\n",
    "                min_id, max_id = min(p.person_id for p in article_people), max(p.person_id for p in article_people)\n",
    "                people_text = f\"{min_id} - {max_id} ({len(article_people)} people)\"\n",
    "        else:\n",
    "            people_text = \"No people identified\"\n",
    "        highlighted = highlight_article(content, article_people)\n",
    "\n",
    "        rows.append(f\"\"\"\n",
    "        <tr>\n",
    "            <td>{article_id}</td>\n",
    "            <td>{people_text}</td>\n",
    "            <td>{highlighted}</td>\n",
    "        </tr>\n",
    "        \"\"\")\n",
    "\n",
    "    html_doc = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <style>\n",
    "            body {{ font-family: Arial; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; }}\n",
    "            td, th {{ border: 1px solid #ccc; padding: 10px; }}\n",
    "            .hl {{ border-radius: 4px; padding: 2px 4px; }}\n",
    "            .token {{ white-space: pre; display: inline; vertical-align: bottom; }}\n",
    "            .highlighted-article {{ display: flex; flex-wrap: wrap; gap: 0.25rem; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2>Token-Level Person Highlighting</h2>\n",
    "        <table>\n",
    "            <tr><th>ID</th><th>People IDs (# people)</th><th>Content</th></tr>\n",
    "            {''.join(rows)}\n",
    "        </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_doc)\n",
    "\n",
    "    print(f\"✅ HTML written to {output_file}\")\n",
    "    \n",
    "article_dict = {row[\"id\"]: row[\"content\"] for idx, row in news_articles_df.iterrows()}\n",
    "press_releases_dict = {row[\"id\"]: row[\"content\"] for idx, row in press_releases_df.iterrows()}\n",
    "generate_html(article_dict, news_article_person_data, output_file=\"data/intermediate/highlighted_local_news_articles.html\")\n",
    "generate_html(press_releases_dict, press_releases_person_data, output_file=\"data/intermediate/highlighted_local_news_press_releases.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec9a1c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
