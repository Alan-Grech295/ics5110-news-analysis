{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "This script contains all the code to process the original CSV files into a structured dataset."
   ],
   "id": "6343c350fa0753b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import subprocess\n",
    "import logging\n",
    "import os\n",
    "import threading\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Any, TypedDict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from spacy.tokens import Span, Doc\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "from word2number import w2n\n",
    "import fastcoref_spacy\n",
    "\n",
    "# Downloads spaCy model\n",
    "subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "# Loads spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)"
   ],
   "id": "972a986a59419019",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96d7bc4a3fd10f1c",
   "metadata": {},
   "source": [
    "# Load original datasets\n",
    "news_articles_df = pd.read_csv(\"data/original/local_news_articles.csv\")\n",
    "press_releases_df = pd.read_csv(\"data/original/police_press_releases.csv\")\n",
    "\n",
    "# Rename news article `article_id` column to `id`\n",
    "news_articles_df.rename(columns={\"article_id\": \"id\"}, inplace=True)\n",
    "\n",
    "# Add `id` column to police press releases, continuing from the news articles ids\n",
    "start = news_articles_df[\"id\"].max() + 1\n",
    "press_releases_df.insert(0, \"id\", range(start, start + len(press_releases_df)))\n",
    "\n",
    "# We can save the police press releases as is; they are all valid accidents\n",
    "press_releases_df.to_csv(\"data/police_press_releases.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Replace special characters\n",
    "## Why?\n",
    "1. Special characters are not always supported by NLP libraries.\n",
    "2. Special characters are not always converted to lowercase successfully."
   ],
   "id": "423c818ab5dfbcec"
  },
  {
   "cell_type": "code",
   "id": "cab8d8f36780746b",
   "metadata": {},
   "source": [
    "# Map special characters to ASCII\n",
    "mapping = str.maketrans({\n",
    "    \"ċ\": \"c\",\n",
    "    \"Ċ\": \"C\",\n",
    "    \"ġ\": \"g\",\n",
    "    \"Ġ\": \"G\",\n",
    "    \"ħ\": \"h\",\n",
    "    \"Ħ\": \"H\",\n",
    "    \"ż\": \"z\",\n",
    "    \"Ż\": \"Z\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"‘\": \"'\",\n",
    "    \"’\": \"'\",\n",
    "    \"—\": \"-\",\n",
    "    \"–\": \"-\",\n",
    "    \"…\": \"...\",\n",
    "})\n",
    "\n",
    "def clean_articles(df: pd.DataFrame, columns):\n",
    "    \"\"\"Replaces special characters in the given dataframe columns with their ASCII counterparts\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).str.translate(mapping)\n",
    "    return df\n",
    "\n",
    "# Clean the two datasets\n",
    "news_articles_df = clean_articles(news_articles_df, [\"title\", \"subtitle\", \"content\"])\n",
    "press_releases_df = clean_articles(press_releases_df, [\"title\", \"content\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.a. Remove non-related articles\n",
    "Some articles in the dataset do not refer to vehicle accidents (e.g. refers to work accidents or new accident prevention policies). We need to remove these.\n",
    "\n",
    "This is done in two ways:\n",
    "1. Matching accident phrases (e.g. car crash)\n",
    "2. Finding reference to a person, vehicle and accident or injury"
   ],
   "id": "ff0f54bcfbeff479"
  },
  {
   "cell_type": "code",
   "id": "507144bfe0768b4c",
   "metadata": {},
   "source": [
    "people_subj = {\"man\", \"woman\", \"child\", \"driver\", \"motorist\", \"motorcyclist\", \"pedestrian\"}\n",
    "vehicles = {\"car\", \"motorcycle\", \"truck\", \"van\", \"bus\", \"bicycle\",\n",
    "            \"Audi\", \"BMW\", \"Chevrolet\", \"Citroen\", \"Dodge\", \"Fiat\", \"Ford\", \"Honda\", \"Hyundai\", \"Isuzu\",\n",
    "            \"Jaguar\", \"Jeep\", \"Kia\", \"Kymco\", \"Mercedes\", \"Mercedes-Benz\", \"Mini\", \"Mitsubishi\", \"Nissan\",\n",
    "            \"Peugeot\", \"Renault\", \"Skoda\", \"Subaru\", \"Suzuki\", \"Toyota\", \"Volkswagen\", \"VW\", \"Volvo\"}\n",
    "accident = {\"accident\", \"crash\", \"collision\"}\n",
    "injuries = {\"injure\", \"die\"}\n",
    "\n",
    "accident_phrases = [\n",
    "    \"car crash\", \"traffic accident\", \"road accident\", \"collision\",\n",
    "    \"crashed\", \"crash\", \"hit by a car\", \"motorcycle accident\",\n",
    "    \"injured in a crash\", \"overturned\", \"run over\", \"lost control\"\n",
    "]\n",
    "\n",
    "accident_matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = [nlp(text) for text in accident_phrases]\n",
    "accident_matcher.add(\"ACCIDENT_PATTERNS\", patterns)\n",
    "\n",
    "def refers_to_accident(text: str) -> bool:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    matches = accident_matcher(doc)\n",
    "\n",
    "    # If any accident phrases are found, assume it is a valid article\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "\n",
    "    has_people = False\n",
    "    has_vehicles = False\n",
    "    has_accident = False\n",
    "    has_injury = False\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ in people_subj:\n",
    "            has_people = True\n",
    "\n",
    "        if token.lemma_ in vehicles:\n",
    "            has_vehicles = True\n",
    "\n",
    "        if token.lemma_ in accident:\n",
    "            has_accident = True\n",
    "\n",
    "        if token.lemma_ in injuries:\n",
    "            has_injury = True\n",
    "\n",
    "        # If people, vehicles and accident or injury is mentioned, assume it is a valid article\n",
    "        if has_people and has_vehicles and (has_accident or has_injury):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# IDs of articles not referring to vehicle accidents\n",
    "non_related_news_article_ids = []\n",
    "\n",
    "for id, text in news_articles_df[[\"id\", \"content\"]].values:\n",
    "    is_accident = refers_to_accident(text)\n",
    "    if not is_accident:\n",
    "        non_related_news_article_ids.append(id)\n",
    "\n",
    "os.makedirs(\"data/intermediate\", exist_ok=True)\n",
    "\n",
    "non_related_news_article_df = news_articles_df[news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "related_news_article_df = news_articles_df[~news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "\n",
    "# Save dataframes as CSVs to view results\n",
    "non_related_news_article_df.to_csv(\"data/intermediate/local_news_articles_exclusions.csv\", index=False)\n",
    "related_news_article_df.to_csv(\"data/intermediate/local_news_articles.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.b. Using LLMs to flag non-related articles\n",
    "While the previous method works quite well, some articles still get through.\n",
    "To catch these, we pass the remaining articles through 3 LLMs (GPT 5 Mini, Grok 4 Fast, Deepseek R1).\n",
    "\n",
    "The LLMs were set up through [Microsoft Foundry](https://ai.azure.com/) to have a unified API to communicate with different LLMs."
   ],
   "id": "94e56a312ffe763b"
  },
  {
   "cell_type": "code",
   "id": "bb25c1ae9683630d",
   "metadata": {},
   "source": [
    "\"\"\"Initialising API\"\"\"\n",
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "endpoint = \"https://news-analysis-resource.openai.azure.com/openai/v1/\"\n",
    "\n",
    "# Need to login using `az login --use-device-code`\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token_provider,\n",
    ")\n",
    "\n",
    "try:\n",
    "    token_provider()\n",
    "    run_cell = True\n",
    "except:\n",
    "    run_cell = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c00e9ea8a57544a3",
   "metadata": {},
   "source": [
    "%%skip_if not run_cell\n",
    "\n",
    "models = [(\"gpt-5-mini\", 50), (\"grok-4-fast-non-reasoning\", 50), (\"DeepSeek-R1-0528\", 20)]\n",
    "\n",
    "class NonAccidentIDs(BaseModel):\n",
    "    ids: List[int] = Field(description=\"A list of ids of news articles that are not accidents\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a senior data scientist reviewing a semi-structured dataset of vehicle accidents news articles. The articles were obtained by simple web scraping (ex: on the tag of the article) which means that some articles do not refer to actual accidents (for example, they refer to new accident/traffic measures).\n",
    "\n",
    "Your job is to review the given accident CSV and return a list of news article IDs that do not refer to accidents.\n",
    "Be very critical! Any article which mentions a specific accident and provides details on it should not be removed.\n",
    "\n",
    "IMPORTANT: You MUST return the data by calling the `set_non_accident_ids` function.\n",
    "\n",
    "Do not return anything other than a function call.\n",
    "\"\"\"\n",
    "\n",
    "csv_prompt = lambda dataset_csv, start_rows, end_rows, total_rows: f\"\"\"\n",
    "MAKE SURE THAT THE RETURNED IDS EXIST IN THIS CSV!\n",
    "\n",
    "Accident CSV Data ({start_rows}-{end_rows}/{total_rows} rows):\n",
    "\n",
    "{dataset_csv}\n",
    "\"\"\"\n",
    "\n",
    "# LLM function definition\n",
    "result_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"set_non_accident_ids\",\n",
    "        \"description\": \"Set the IDs of the news articles which do not refer to an accident\",\n",
    "        \"parameters\": NonAccidentIDs.model_json_schema()\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [result_function]\n",
    "\n",
    "tqdm.set_lock(threading.RLock())\n",
    "\n",
    "def process_model(model, num_rows_per_request, dataset_df: pd.DataFrame, system_prompt) -> set[int]:\n",
    "    total_ids = set()\n",
    "\n",
    "    with tqdm(range(0, len(dataset_df.index), num_rows_per_request), desc=f\"Querying {model}\", total=len(dataset_df.index), unit=\" rows\") as pbar:\n",
    "        i = 0\n",
    "        while i < len(dataset_df.index):\n",
    "            try:\n",
    "                # Get row range as the LLMs cannot process the entire file at once\n",
    "                start = i\n",
    "                end = min(i + num_rows_per_request, len(dataset_df.index))\n",
    "                df_section: pd.DataFrame = dataset_df.iloc[start:end]\n",
    "                df_section_csv = df_section.to_csv(index=False)\n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": csv_prompt(df_section_csv, start + 1, end + 1, len(dataset_df.index)),\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                result: NonAccidentIDs = NonAccidentIDs.model_validate_json(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "\n",
    "                for id in result.ids:\n",
    "                    # Throw an error if a returned ID is not in the dataset\n",
    "                    if id not in dataset_df[\"id\"].values:\n",
    "                        raise ValueError(f\"ID {id} not in dataset\")\n",
    "\n",
    "                total_ids.update(result.ids)\n",
    "\n",
    "                # Update tqdm progress bar\n",
    "                pbar.update(end - start)\n",
    "                i += num_rows_per_request\n",
    "            except Exception as e:\n",
    "                # If we get an error, retry the model (i.e. do not increment i)\n",
    "                logging.warning(f\"Failed to query {model}: {e}\")\n",
    "\n",
    "    return total_ids\n",
    "\n",
    "# Run LLMs in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_model,\n",
    "            model,\n",
    "            num_rows_per_request,\n",
    "            related_news_article_df,\n",
    "            system_prompt\n",
    "        ): model\n",
    "        for model, num_rows_per_request in models\n",
    "    }\n",
    "\n",
    "    model_ids = {}\n",
    "\n",
    "    for f in futures.keys():\n",
    "        result = f.result()\n",
    "        model_ids[futures[f]] = result\n",
    "\n",
    "all_ids = list(model_ids.values())\n",
    "# Combine IDs from all LLMs\n",
    "ids_union = all_ids[0].union(*all_ids[1:])\n",
    "\n",
    "# Save the LLM excluded articles as a CSV to review\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_exclusions.csv\", index=False)\n",
    "# Save the same LLM excluded articles as a CSV. This CSV will be manually checked and modified\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b555b459dc1ee45f",
   "metadata": {},
   "source": [
    "# 2.c. Manually review LLM exclusions\n",
    "The LLMs tend to flag valid articles as invalid. Since this is a short list (because we already removed the bulk of invalid articles in step 1), we can go through the list manually and remove the valid articles.\n",
    "\n",
    "## Removed IDs\n",
    "- 3699\n",
    "- 1352\n",
    "- 370\n",
    "- 287\n",
    "- 489424\n",
    "- 491118\n",
    "- 491371\n",
    "- 494102\n",
    "- 495320\n",
    "- 495942\n",
    "- 496362\n",
    "\n",
    "*Note: `local_news_articles_llm_exclusions.csv` contains the excluded rows as given by the LLMs. `local_news_articles_llm_manual_exclusions.csv` contains the same excluded rows given by the LLMs, except that any rows referring to valid accidents were manually removed.*"
   ]
  },
  {
   "cell_type": "code",
   "id": "53eeb00bf883dcff",
   "metadata": {},
   "source": [
    "# Manual removal of valid rows\n",
    "manual_ids = {3699, 1352, 370, 287, 489424, 491118, 491371, 494102, 495320, 495942, 496362}\n",
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "llm_manual_exclusions = llm_manual_exclusions[~llm_manual_exclusions[\"id\"].isin(manual_ids)]\n",
    "llm_manual_exclusions.to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f5a91b49de2f99f",
   "metadata": {},
   "source": [
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "\n",
    "# Filtering the original news articles\n",
    "news_articles_df = related_news_article_df[~related_news_article_df[\"id\"].isin(llm_manual_exclusions[\"id\"])]\n",
    "news_articles_df.to_csv(\"data/local_news_articles.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Per-person data",
   "id": "7f3514f310ae288e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if \"patched_fastcoref\" not in {n for n, _ in nlp.pipeline}:\n",
    "    nlp.add_pipe(\"patched_fastcoref\", config={'model_architecture': 'LingMessCoref', 'model_path': 'biu-nlp/lingmess-coref', 'device': 'cpu'})\n",
    "\n",
    "def remove_contained_spans(spans: list[tuple[Any, Span]]) -> list[tuple[Any, Span]]:\n",
    "    filtered_spans = []\n",
    "    spans = sorted(spans, key=lambda s: (s[1].end - s[1].start), reverse=True)\n",
    "\n",
    "    for x, span in spans:\n",
    "        contained = False\n",
    "        for _, kept in filtered_spans:\n",
    "            if span.start >= kept.start and span.end <= kept.end:\n",
    "                contained = True\n",
    "                break\n",
    "        if not contained:\n",
    "            filtered_spans.append((x, span))\n",
    "\n",
    "    return list(sorted(filtered_spans, key=lambda s: s[1].start))\n",
    "\n",
    "age_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "age_patterns = [\n",
    "    [{\"LIKE_NUM\": True}, {\"LOWER\": \"year\"}, {\"LOWER\": \"old\"}],\n",
    "    [{\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LOWER\": \"year\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"old\"}],\n",
    "    [{\"TEXT\": \",\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}]\n",
    "]\n",
    "\n",
    "age_matcher.add(\"AGE\", age_patterns)\n",
    "\n",
    "severity_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "severity_patterns = [\n",
    "    [{\"LIKE_NUM\": True}, {\"LOWER\": \"year\"}, {\"LOWER\": \"old\"}],\n",
    "    [{\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LOWER\": \"year\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"old\"}],\n",
    "    [{\"TEXT\": \",\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}]\n",
    "]\n",
    "\n",
    "severity_matcher.add(\"SEVERITY\", age_patterns)\n",
    "\n",
    "def get_ages(doc: Doc) -> list[tuple[int, Span]]:\n",
    "    ages = []\n",
    "    matches = age_matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span: Span = doc[start:end]\n",
    "        age_int = -1\n",
    "        for token in span:\n",
    "            if token.like_num:\n",
    "                try:\n",
    "                    age_int = int(token.text)\n",
    "                except ValueError:\n",
    "                    age_int = w2n.word_to_num(token.text)\n",
    "                break\n",
    "        if age_int <= 0 or age_int >= 120:\n",
    "            continue\n",
    "        ages.append((age_int, span))\n",
    "\n",
    "    return ages\n",
    "\n",
    "gender_words = {\n",
    "    \"man\": \"male\", \"male\": \"male\", \"he\": \"male\", \"him\": \"male\",\n",
    "    \"woman\": \"female\", \"female\": \"female\", \"she\": \"female\", \"her\": \"female\",\n",
    "}\n",
    "\n",
    "def get_genders(doc: Doc) -> list[tuple[str, Span]]:\n",
    "    genders = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ in gender_words:\n",
    "            genders.append((gender_words[token.lemma_], Span(doc, token.i, token.i + 1)))\n",
    "    return genders\n",
    "\n",
    "accepted_injuries = {\n",
    "    \"serious\": \"serious\",\n",
    "    \"slight\": \"light\",\n",
    "    \"grievous\": \"grievous\",\n",
    "    \"light\": \"light\",\n",
    "    \"critical\": \"critical\",\n",
    "    \"fatal\": \"dead\",\n",
    "    \"bad\": \"serious\"\n",
    " }\n",
    "\n",
    "def get_severities(doc: Doc) -> list[tuple[str, Span]]:\n",
    "    severities = []\n",
    "    def adverb_to_adj(word: str):\n",
    "        w = word.lower()\n",
    "        if w.endswith(\"ously\"):\n",
    "            return w[:-2]\n",
    "        if w.endswith(\"ally\"):\n",
    "            return w[:-2]\n",
    "        if w.endswith(\"ly\"):\n",
    "            return w[:-2]\n",
    "        return w\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ == \"injure\" or token.lemma_ == \"injury\":\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"advmod\" or child.dep_ == \"amod\":\n",
    "                    injury_adj = adverb_to_adj(child.lemma_)\n",
    "                    if injury_adj not in accepted_injuries:\n",
    "                        continue\n",
    "                    severities.append((accepted_injuries[injury_adj], Span(doc, child.i, child.i + 1)))\n",
    "        elif token.lemma_ == \"dead\" or token.lemma_ == \"die\":\n",
    "            severities.append((\"dead\", Span(doc, token.i, token.i + 1)))\n",
    "\n",
    "    return severities\n",
    "\n",
    "driver_matcher = Matcher(nlp.vocab)\n",
    "vehicle_dict = {\"car\": \"car\", \"bus\": \"bus\", \"taxi\": \"car\", \"motorcycle\": \"motorcycle\", \"motorbike\": \"motorcycle\", \"bike\": \"motorcycle\", \"bicycle\": \"bicycle\", \"van\": \"van\", \"truck\": \"truck\", \"vehicle\": None}\n",
    "driver_nouns = {\"motorcyclist\": \"motorcycle\", \"motorist\": \"motorcycle\", \"biker\": \"motorcycle\"}\n",
    "driver_patterns = [\n",
    "    [{\"LOWER\": {\"IN\": list(vehicle_dict.keys())}, \"OP\": \"?\"}, {\"LOWER\": \"driver\"}],\n",
    "    [{\"LOWER\": {\"IN\": list(driver_nouns.keys())}}]\n",
    "]\n",
    "driver_matcher.add(\"DRIVER\", driver_patterns)\n",
    "\n",
    "# Derived through ChatGPT and Gemini\n",
    "vehicle_type_map = {\n",
    "    \"Jeep\": \"car\",\n",
    "    \"Skoda Fabia\": \"car\",\n",
    "    \"Toyota Tercel\": \"car\",\n",
    "    \"Peugeot 307\": \"car\",\n",
    "    \"Suzuki SX4\": \"car\",\n",
    "    \"Mercedes\": \"car\",\n",
    "    \"Toyota Hilux\": \"truck\",\n",
    "    \"Ford B Max\": \"car\",\n",
    "    \"Vauxhall\": \"car\",\n",
    "    \"Honda Accord\": \"car\",\n",
    "    \"Dacia Duster\": \"car\",\n",
    "    \"Mazda Demio\": \"car\",\n",
    "    \"Toyota Vitz\": \"car\",\n",
    "    \"Toyota Ractis\": \"car\",\n",
    "    \"BMW 525\": \"car\",\n",
    "    \"BMW 1 Series\": \"car\",\n",
    "    \"Peugeot 306\": \"car\",\n",
    "    \"Toyota Passo\": \"car\",\n",
    "    \"Nissan\": \"car\",\n",
    "    \"Peugeot Partner\": \"van\",\n",
    "    \"Renault Captur\": \"car\",\n",
    "    \"Toyota Aygo\": \"car\",\n",
    "    \"Volkswagen Polo\": \"car\",\n",
    "    \"Seat Leon\": \"car\",\n",
    "    \"Mercedes C180\": \"car\",\n",
    "    \"Toyota Funcargo\": \"car\", # or van\n",
    "    \"Honda Civic\": \"car\",\n",
    "    \"Honda Fit\": \"car\",\n",
    "    \"Hyundai Getz\": \"car\",\n",
    "    \"Kia Sportage\": \"car\",\n",
    "    \"Toyota Auris\": \"car\",\n",
    "    \"Subaru Impreza\": \"car\",\n",
    "    \"Volvo XC60\": \"car\",\n",
    "    \"BMW i13\": \"car\",\n",
    "    \"Renault Megane\": \"car\",\n",
    "    \"Hyundai i10\": \"car\",\n",
    "    \"Fiat Scudo\": \"van\",\n",
    "    \"Maserati Levante\": \"car\",\n",
    "    \"Peugeot 2008\": \"car\",\n",
    "    \"BMW 318\": \"car\",\n",
    "    \"Citroen DS3\": \"car\",\n",
    "    \"Mazda 2\": \"car\",\n",
    "    \"Hyundai i30\": \"car\",\n",
    "    \"Suzuki Baleno\": \"car\",\n",
    "    \"Toyota Aqua\": \"car\",\n",
    "    \"Kia Picanto\": \"car\",\n",
    "    \"Yamaha T115\": \"motorcycle\",\n",
    "    \"Kawasaki Ninja\": \"motorcycle\",\n",
    "    \"Honda NSC110\": \"motorcycle\",\n",
    "    \"Hyosung XRX\": \"motorcycle\"\n",
    "}\n",
    "\n",
    "vehicle_type_map = {\n",
    "    k.lower(): v for k, v in vehicle_type_map.items()\n",
    "}\n",
    "\n",
    "vehicle_type_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "vehicle_docs = [nlp.make_doc(name) for name in vehicle_type_map.keys()]\n",
    "max_vehicle_tokens = max(len(doc) for doc in vehicle_docs)\n",
    "vehicle_type_matcher.add(\"VEHICLES\", vehicle_docs)\n",
    "\n",
    "def get_drivers(doc: Doc) -> list[tuple[Optional[str], Span]]:\n",
    "    def get_vehicle_from_vehicle_span(vehicle_span: Span):\n",
    "        vehicle_matches = vehicle_type_matcher(vehicle_span)\n",
    "        if len(vehicle_matches) > 0:\n",
    "            vehicle_span = spacy.util.filter_spans([doc[start:end] for _, start, end in vehicle_matches])[0]\n",
    "            return vehicle_type_map[vehicle_span.text.lower()]\n",
    "\n",
    "        return None\n",
    "\n",
    "    drivers = []\n",
    "\n",
    "    matches = driver_matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span: Span = doc[start:end]\n",
    "\n",
    "        vehicle_type = None\n",
    "        for token in span:\n",
    "            if token.lower_ in vehicle_dict:\n",
    "                vehicle_type = vehicle_dict[token.lower_]\n",
    "                break\n",
    "            elif token.lower_ in driver_nouns:\n",
    "                vehicle_type = driver_nouns[token.lower_]\n",
    "                break\n",
    "\n",
    "        drivers.append((vehicle_type, Span(doc, start, end)))\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\" and token.lower_ == \"driven\" and token.dep_ == \"acl\":\n",
    "            span_start = token.head.i\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ == \"compound\":\n",
    "                    span_start = min(span_start, child.i)\n",
    "\n",
    "            vehicle_span = Span(doc, span_start, token.i)\n",
    "            vehicle_type = None\n",
    "            for token in vehicle_span:\n",
    "                if token.lower_ in vehicle_dict:\n",
    "                    vehicle_type = vehicle_dict[token.lower_]\n",
    "                    break\n",
    "\n",
    "            if vehicle_type is None:\n",
    "                vehicle_type = get_vehicle_from_vehicle_span(vehicle_span)\n",
    "                # If we have an invalid vehicle span\n",
    "                if vehicle_type is None:\n",
    "                    # Increase span size, maybe we missed part of the name\n",
    "                    vehicle_span.start = max(0, vehicle_span.start - max_vehicle_tokens)\n",
    "                    vehicle_type = get_vehicle_from_vehicle_span(vehicle_span)\n",
    "                    vehicle_type = vehicle_type if vehicle_type is not None else vehicle_span.text\n",
    "\n",
    "            span_end = token.i\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"agent\":\n",
    "                    for c in child.children:\n",
    "                        if c.dep_ == \"pobj\":\n",
    "                            span_end = max(span_end, c.i)\n",
    "\n",
    "            span = Span(doc, span_start, span_end)\n",
    "            drivers.append((vehicle_type, span))\n",
    "            # for parent in token.ancestors:\n",
    "            #     if parent\n",
    "\n",
    "    return drivers\n",
    "\n",
    "person_matcher = Matcher(nlp.vocab)\n",
    "person_nouns = [\"man\", \"woman\", \"boy\", \"girl\", \"teen\", \"teenager\", \"baby\", \"person\", \"driver\", \"passenger\",\n",
    "    \"pedestrian\", \"victim\", \"motorcyclist\", \"cyclist\", \"rider\",\n",
    "    \"motorist\", \"resident\", \"teenager\", \"youth\"]\n",
    "vehicle_nouns = [\"car\", \"motorcycle\", \"truck\", \"van\", \"bus\", \"bicycle\"]\n",
    "person_patterns = [\n",
    "    [{\"LOWER\": {\"IN\": [\"a\", \"an\", \"the\", \"one\"]}, \"OP\": \"?\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LOWER\": \"year\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"old\"}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"LOWER\": {\"IN\": person_nouns}, \"OP\": \"?\"}],\n",
    "    [{\"LOWER\": {\"IN\": [\"a\", \"an\", \"the\", \"one\"]}}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"LOWER\": {\"IN\": person_nouns}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"a\", \"an\", \"the\", \"one\"]}}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"LOWER\": {\"IN\": vehicle_nouns}}, {\"LOWER\": \"driver\"}],\n",
    "    [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"TEXT\": \",\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}],\n",
    "]\n",
    "\n",
    "person_matcher.add(\"PERSON\", person_patterns)\n",
    "\n",
    "def get_persons(doc: Doc) -> list[list[tuple[str, Span]]]:\n",
    "    matches = person_matcher(doc)\n",
    "    cluster_spans: list[list[Span]] = [\n",
    "        [doc.char_span(start, end, alignment_mode=\"expand\") for start, end in cluster] for cluster in doc._.coref_clusters\n",
    "    ]\n",
    "\n",
    "    people = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        people.append((span.text, span))\n",
    "\n",
    "    cleaned = remove_contained_spans(people)\n",
    "\n",
    "    def calc_overlap(a: Span, b: Span):\n",
    "        start = max(a.start, b.start)\n",
    "        end = min(a.end, b.end)\n",
    "\n",
    "        if end <= start:\n",
    "            return 0\n",
    "\n",
    "        return end - start\n",
    "\n",
    "    cluster_indices = []\n",
    "    for text, span in cleaned:\n",
    "        max_overlap = 0\n",
    "        max_overlap_i = -1\n",
    "        for i in range(len(cluster_spans)):\n",
    "            max_cluster_overlap = max(calc_overlap(c, span) for c in cluster_spans[i])\n",
    "            if max_cluster_overlap > max_overlap:\n",
    "                max_overlap = max_cluster_overlap\n",
    "                max_overlap_i = i\n",
    "\n",
    "        cluster_indices.append(max_overlap_i)\n",
    "\n",
    "    clustered_people = [\n",
    "        [(c.text, c) for c in cluster]\n",
    "        for i, cluster in enumerate(cluster_spans) if i in cluster_indices\n",
    "    ]\n",
    "\n",
    "    for i, person in zip(cluster_indices, cleaned):\n",
    "        if i >= 0:\n",
    "            continue\n",
    "\n",
    "        clustered_people.append([person])\n",
    "\n",
    "    return clustered_people\n",
    "\n",
    "# test_doc = nlp(\"The police said in a statement the accident was reported at 8.30pm and involved a Toyota Versace Tercel driven by an 86-year-old woman from Xewkija and a Benelli bike driven by a 45-year-old from Sannat\")\n",
    "# print(get_drivers(test_doc))\n",
    "# displacy.render(test_doc)\n",
    "#\n",
    "# for token in test_doc:\n",
    "#     print(token, \"|\", token.lemma_)\n",
    "\n",
    "def assign_feature_to_person(feature: Span, people: list[list[tuple[str, Span]]]) -> int:\n",
    "    def span_distance(span_a: Span, span_b: Span):\n",
    "        \"\"\"\n",
    "        span_a, span_b are (start, end) token index tuples\n",
    "        representing half-open intervals [start, end)\n",
    "        \"\"\"\n",
    "        a_start, a_end = span_a.start, span_a.end\n",
    "        b_start, b_end = span_b.start, span_b.end\n",
    "\n",
    "        # If spans overlap or touch, distance = 0\n",
    "        if a_end >= b_start and b_end >= a_start:\n",
    "            return 0\n",
    "\n",
    "        # Otherwise distance is the gap between them\n",
    "        if a_end < b_start:\n",
    "            return b_start - a_end\n",
    "        else:\n",
    "            return a_start - b_end\n",
    "\n",
    "    f_sent = feature.sent\n",
    "\n",
    "    best_person_i = -1\n",
    "    best_dist = 10**9\n",
    "\n",
    "    for i, person in enumerate(people):\n",
    "        for _, mention in person:\n",
    "            sent_dist = min(\n",
    "                abs(mention.sent.start - f_sent.start), # Same sentence\n",
    "                abs(mention.sent.start - f_sent.end),   # Next sentence\n",
    "                abs(f_sent.sent.start - mention.end)    # Previous sentence\n",
    "            )\n",
    "\n",
    "            if sent_dist <= 1:\n",
    "                token_dist = span_distance(feature, mention)\n",
    "                if token_dist < best_dist:\n",
    "                    best_person_i = i\n",
    "                    best_dist = token_dist\n",
    "\n",
    "    return best_person_i\n",
    "\n",
    "@dataclass\n",
    "class ValueSpan[T]:\n",
    "    value: T\n",
    "    span: Span\n",
    "\n",
    "@dataclass\n",
    "class PersonData:\n",
    "    article_id: int\n",
    "    name: Optional[ValueSpan[str]] = None\n",
    "    age: Optional[ValueSpan[int]] = None\n",
    "    gender: Optional[ValueSpan[str]] = None\n",
    "    severity: Optional[ValueSpan[str]] = None\n",
    "    is_driver: Optional[ValueSpan[bool]] = field(default_factory=lambda: ValueSpan(False, None))\n",
    "    vehicle_type: Optional[ValueSpan[str]] = None\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        vars = self.__dict__.copy()\n",
    "        vars.pop(\"article_id\")\n",
    "        return {\n",
    "            \"article_id\": self.article_id,\n",
    "            **{k: v.value if v is not None else None for k, v in vars.items()}\n",
    "        }\n",
    "\n",
    "    def combine(self, other: \"PersonData\") -> \"PersonData\":\n",
    "        vars = self.__dict__.copy()\n",
    "        vars.pop(\"article_id\")\n",
    "\n",
    "        for var in vars.keys():\n",
    "            if vars[var] is not None:\n",
    "                continue\n",
    "\n",
    "            setattr(self, var, getattr(other, var))\n",
    "\n",
    "        return self\n",
    "\n",
    "def compute_person_similarity(person1: PersonData, person2: PersonData) -> int:\n",
    "    similarity = 0\n",
    "\n",
    "    def is_same(a, b):\n",
    "        return a is not None and b is not None and a.value == b.value\n",
    "\n",
    "    def is_different(a, b):\n",
    "        return a is not None and b is not None and a.value != b.value\n",
    "\n",
    "    if is_same(person1.name, person2.name):\n",
    "        similarity += 10\n",
    "    elif is_different(person1.name, person2.name):\n",
    "        return -1\n",
    "\n",
    "    if is_same(person1.age, person2.age):\n",
    "        similarity += 2\n",
    "    elif is_different(person1.age, person2.age):\n",
    "        return -1\n",
    "\n",
    "    if is_same(person1.gender, person2.gender):\n",
    "        similarity += 1\n",
    "    elif is_different(person1.gender, person2.gender):\n",
    "        return -1\n",
    "\n",
    "    if is_same(person1.severity, person2.severity):\n",
    "        similarity += 1\n",
    "    elif is_different(person1.severity, person2.severity):\n",
    "        return -1\n",
    "\n",
    "    # No elif for is_driver as it is automatically false if no driver is mentioned\n",
    "    if is_same(person1.is_driver, person2.is_driver):\n",
    "        similarity += 1\n",
    "\n",
    "    if is_same(person1.vehicle_type, person2.vehicle_type):\n",
    "        similarity += 1\n",
    "    elif is_different(person1.vehicle_type, person2.vehicle_type):\n",
    "        return -1\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def build_person_graph(article_person_data: list[PersonData], threshold = 3):\n",
    "    person_adj_graph = {i: [] for i in range(len(article_person_data))}\n",
    "\n",
    "    for i in range(len(article_person_data) - 1):\n",
    "        for j in range(i + 1, len(article_person_data)):\n",
    "            sim = compute_person_similarity(article_person_data[i], article_person_data[j])\n",
    "            if sim >= threshold:\n",
    "                person_adj_graph[i].append(j)\n",
    "                person_adj_graph[j].append(i)\n",
    "\n",
    "    return person_adj_graph\n",
    "\n",
    "def get_connected_people(article_person_data: list[PersonData], threshold = 3):\n",
    "    person_adj_graph = build_person_graph(article_person_data, threshold)\n",
    "    visited = set()\n",
    "    components = []\n",
    "\n",
    "    for node in person_adj_graph:\n",
    "        if node not in visited:\n",
    "            stack = [node]\n",
    "            comp = []\n",
    "            visited.add(node)\n",
    "\n",
    "            while stack:\n",
    "                cur = stack.pop()\n",
    "                comp.append(cur)\n",
    "                for n in person_adj_graph[cur]:\n",
    "                    if n not in visited:\n",
    "                        visited.add(n)\n",
    "                        stack.append(n)\n",
    "\n",
    "            components.append(comp)\n",
    "\n",
    "    return components\n",
    "\n",
    "def combine_people(article_person_data: list[PersonData], connected_persons: list[list[int]]) -> list[PersonData]:\n",
    "    combined_data = []\n",
    "\n",
    "    for connected in connected_persons:\n",
    "        person = article_person_data[connected[0]]\n",
    "        if len(connected) > 1:\n",
    "            for idx in connected[1:]:\n",
    "                person.combine(article_person_data[idx])\n",
    "\n",
    "        combined_data.append(person)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "person_data: list[PersonData] = []\n",
    "\n",
    "for idx, row in tqdm(news_articles_df.iterrows(), total=len(news_articles_df), desc=\"Processing articles\"):\n",
    "    content = row[\"content\"]\n",
    "    doc = nlp(content)\n",
    "\n",
    "    # displacy.render(doc)\n",
    "    ages = get_ages(doc)\n",
    "    genders = get_genders(doc)\n",
    "    severities = get_severities(doc)\n",
    "    drivers = get_drivers(doc)\n",
    "    people = get_persons(doc)\n",
    "\n",
    "    article_person_data: list[PersonData] = [PersonData(article_id=row[\"id\"]) for _ in people]\n",
    "\n",
    "    # Check gender\n",
    "    for i, person in enumerate(people):\n",
    "        gender_list = []\n",
    "        gender_spans = {}\n",
    "        for _, mention in person:\n",
    "            for token in mention:\n",
    "                if token.lemma_ not in gender_words:\n",
    "                    continue\n",
    "\n",
    "                gender = gender_words[token.lemma_]\n",
    "                gender_list.append(gender)\n",
    "                gender_spans[gender] = token\n",
    "\n",
    "        if len(gender_list) > 0:\n",
    "            count = Counter(gender_list)\n",
    "            common_gender = count.most_common(1)[0][0]\n",
    "            article_person_data[i].gender = ValueSpan(value=common_gender, span=gender_spans[common_gender])\n",
    "\n",
    "    for txt, span in severities:\n",
    "        person_idx = assign_feature_to_person(span, people)\n",
    "        if person_idx >= 0:\n",
    "            article_person_data[person_idx].severity = ValueSpan(value=txt, span=span)\n",
    "\n",
    "    for age, span in ages:\n",
    "        person_idx = assign_feature_to_person(span, people)\n",
    "        if person_idx >= 0:\n",
    "            article_person_data[person_idx].age = ValueSpan(value=age, span=span)\n",
    "\n",
    "    for gender, span in genders:\n",
    "        person_idx = assign_feature_to_person(span, people)\n",
    "        if person_idx >= 0 and article_person_data[person_idx].gender is None:\n",
    "            # assert article_person_data[person_idx].gender is None or gender == article_person_data[person_idx].gender.value\n",
    "            article_person_data[person_idx].gender = ValueSpan(value=gender, span=span)\n",
    "\n",
    "    for vehicle_type, span in drivers:\n",
    "        person_idx = assign_feature_to_person(span, people)\n",
    "        if person_idx >= 0:\n",
    "            article_person_data[person_idx].is_driver = ValueSpan(value=True, span=span)\n",
    "            article_person_data[person_idx].vehicle_type = ValueSpan(value=vehicle_type, span=span)\n",
    "\n",
    "    connected_people = get_connected_people(article_person_data)\n",
    "    article_person_data = combine_people(article_person_data, connected_people)\n",
    "\n",
    "    person_data.extend(article_person_data)\n",
    "\n",
    "person_dict_data = [p.to_dict() for p in person_data]\n",
    "with open(\"data/intermediate/persons.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, person_dict_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(person_dict_data)\n"
   ],
   "id": "b094fab8bdd44477",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
