{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "This script contains all the code to process the original CSV files into a structured dataset."
   ],
   "id": "6343c350fa0753b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import logging\n",
    "import os\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Any\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Downloads spaCy model\n",
    "subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "# Loads spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)"
   ],
   "id": "972a986a59419019",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96d7bc4a3fd10f1c",
   "metadata": {},
   "source": [
    "# Load original datasets\n",
    "news_articles_df = pd.read_csv(\"data/original/local_news_articles.csv\")\n",
    "press_releases_df = pd.read_csv(\"data/original/police_press_releases.csv\")\n",
    "\n",
    "# Rename news article `article_id` column to `id`\n",
    "news_articles_df.rename(columns={\"article_id\": \"id\"}, inplace=True)\n",
    "\n",
    "# Add `id` column to police press releases, continuing from the news articles ids\n",
    "start = news_articles_df[\"id\"].max() + 1\n",
    "press_releases_df.insert(0, \"id\", range(start, start + len(press_releases_df)))\n",
    "\n",
    "# We can save the police press releases as is; they are all valid accidents\n",
    "press_releases_df.to_csv(\"data/police_press_releases.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Replace special characters\n",
    "## Why?\n",
    "1. Special characters are not always supported by NLP libraries.\n",
    "2. Special characters are not always converted to lowercase successfully."
   ],
   "id": "423c818ab5dfbcec"
  },
  {
   "cell_type": "code",
   "id": "cab8d8f36780746b",
   "metadata": {},
   "source": [
    "# Map special characters to ASCII\n",
    "mapping = str.maketrans({\n",
    "    \"ċ\": \"c\",\n",
    "    \"Ċ\": \"C\",\n",
    "    \"ġ\": \"g\",\n",
    "    \"Ġ\": \"G\",\n",
    "    \"ħ\": \"h\",\n",
    "    \"Ħ\": \"H\",\n",
    "    \"ż\": \"z\",\n",
    "    \"Ż\": \"Z\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"‘\": \"'\",\n",
    "    \"’\": \"'\",\n",
    "    \"—\": \"-\",\n",
    "    \"–\": \"-\",\n",
    "    \"…\": \"...\",\n",
    "})\n",
    "\n",
    "def clean_articles(df: pd.DataFrame, columns):\n",
    "    \"\"\"Replaces special characters in the given dataframe columns with their ASCII counterparts\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).str.translate(mapping)\n",
    "    return df\n",
    "\n",
    "# Clean the two datasets\n",
    "news_articles_df = clean_articles(news_articles_df, [\"title\", \"subtitle\", \"content\"])\n",
    "press_releases_df = clean_articles(press_releases_df, [\"title\", \"content\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.a. Remove non-related articles\n",
    "Some articles in the dataset do not refer to vehicle accidents (e.g. refers to work accidents or new accident prevention policies). We need to remove these.\n",
    "\n",
    "This is done in two ways:\n",
    "1. Matching accident phrases (e.g. car crash)\n",
    "2. Finding reference to a person, vehicle and accident or injury"
   ],
   "id": "ff0f54bcfbeff479"
  },
  {
   "cell_type": "code",
   "id": "507144bfe0768b4c",
   "metadata": {},
   "source": [
    "people_subj = {\"man\", \"woman\", \"child\", \"driver\", \"motorist\", \"motorcyclist\", \"pedestrian\"}\n",
    "vehicles = {\"car\", \"motorcycle\", \"truck\", \"van\", \"bus\", \"bicycle\",\n",
    "            \"Audi\", \"BMW\", \"Chevrolet\", \"Citroen\", \"Dodge\", \"Fiat\", \"Ford\", \"Honda\", \"Hyundai\", \"Isuzu\",\n",
    "            \"Jaguar\", \"Jeep\", \"Kia\", \"Kymco\", \"Mercedes\", \"Mercedes-Benz\", \"Mini\", \"Mitsubishi\", \"Nissan\",\n",
    "            \"Peugeot\", \"Renault\", \"Skoda\", \"Subaru\", \"Suzuki\", \"Toyota\", \"Volkswagen\", \"VW\", \"Volvo\"}\n",
    "accident = {\"accident\", \"crash\", \"collision\"}\n",
    "injuries = {\"injure\", \"die\"}\n",
    "\n",
    "accident_phrases = [\n",
    "    \"car crash\", \"traffic accident\", \"road accident\", \"collision\",\n",
    "    \"crashed\", \"crash\", \"hit by a car\", \"motorcycle accident\",\n",
    "    \"injured in a crash\", \"overturned\", \"run over\", \"lost control\"\n",
    "]\n",
    "\n",
    "accident_matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = [nlp(text) for text in accident_phrases]\n",
    "accident_matcher.add(\"ACCIDENT_PATTERNS\", patterns)\n",
    "\n",
    "def refers_to_accident(text: str) -> bool:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    matches = accident_matcher(doc)\n",
    "\n",
    "    # If any accident phrases are found, assume it is a valid article\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "\n",
    "    has_people = False\n",
    "    has_vehicles = False\n",
    "    has_accident = False\n",
    "    has_injury = False\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ in people_subj:\n",
    "            has_people = True\n",
    "\n",
    "        if token.lemma_ in vehicles:\n",
    "            has_vehicles = True\n",
    "\n",
    "        if token.lemma_ in accident:\n",
    "            has_accident = True\n",
    "\n",
    "        if token.lemma_ in injuries:\n",
    "            has_injury = True\n",
    "\n",
    "        # If people, vehicles and accident or injury is mentioned, assume it is a valid article\n",
    "        if has_people and has_vehicles and (has_accident or has_injury):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# IDs of articles not referring to vehicle accidents\n",
    "non_related_news_article_ids = []\n",
    "\n",
    "for id, text in news_articles_df[[\"id\", \"content\"]].values:\n",
    "    is_accident = refers_to_accident(text)\n",
    "    if not is_accident:\n",
    "        non_related_news_article_ids.append(id)\n",
    "\n",
    "os.makedirs(\"data/intermediate\", exist_ok=True)\n",
    "\n",
    "non_related_news_article_df = news_articles_df[news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "related_news_article_df = news_articles_df[~news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "\n",
    "# Save dataframes as CSVs to view results\n",
    "non_related_news_article_df.to_csv(\"data/intermediate/local_news_articles_exclusions.csv\", index=False)\n",
    "related_news_article_df.to_csv(\"data/intermediate/local_news_articles.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.b. Using LLMs to flag non-related articles\n",
    "While the previous method works quite well, some articles still get through.\n",
    "To catch these, we pass the remaining articles through 3 LLMs (GPT 5 Mini, Grok 4 Fast, Deepseek R1).\n",
    "\n",
    "The LLMs were set up through [Microsoft Foundry](https://ai.azure.com/) to have a unified API to communicate with different LLMs."
   ],
   "id": "94e56a312ffe763b"
  },
  {
   "cell_type": "code",
   "id": "bb25c1ae9683630d",
   "metadata": {},
   "source": [
    "\"\"\"Initialising API\"\"\"\n",
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "endpoint = \"https://news-analysis-resource.openai.azure.com/openai/v1/\"\n",
    "\n",
    "# Need to login using `az login --use-device-code`\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token_provider,\n",
    ")\n",
    "\n",
    "try:\n",
    "    token_provider()\n",
    "    run_cell = True\n",
    "except:\n",
    "    run_cell = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c00e9ea8a57544a3",
   "metadata": {},
   "source": [
    "%%skip_if not run_cell\n",
    "\n",
    "models = [(\"gpt-5-mini\", 50), (\"grok-4-fast-non-reasoning\", 50), (\"DeepSeek-R1-0528\", 20)]\n",
    "\n",
    "class NonAccidentIDs(BaseModel):\n",
    "    ids: List[int] = Field(description=\"A list of ids of news articles that are not accidents\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a senior data scientist reviewing a semi-structured dataset of vehicle accidents news articles. The articles were obtained by simple web scraping (ex: on the tag of the article) which means that some articles do not refer to actual accidents (for example, they refer to new accident/traffic measures).\n",
    "\n",
    "Your job is to review the given accident CSV and return a list of news article IDs that do not refer to accidents.\n",
    "Be very critical! Any article which mentions a specific accident and provides details on it should not be removed.\n",
    "\n",
    "IMPORTANT: You MUST return the data by calling the `set_non_accident_ids` function.\n",
    "\n",
    "Do not return anything other than a function call.\n",
    "\"\"\"\n",
    "\n",
    "csv_prompt = lambda dataset_csv, start_rows, end_rows, total_rows: f\"\"\"\n",
    "MAKE SURE THAT THE RETURNED IDS EXIST IN THIS CSV!\n",
    "\n",
    "Accident CSV Data ({start_rows}-{end_rows}/{total_rows} rows):\n",
    "\n",
    "{dataset_csv}\n",
    "\"\"\"\n",
    "\n",
    "# LLM function definition\n",
    "result_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"set_non_accident_ids\",\n",
    "        \"description\": \"Set the IDs of the news articles which do not refer to an accident\",\n",
    "        \"parameters\": NonAccidentIDs.model_json_schema()\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [result_function]\n",
    "\n",
    "tqdm.set_lock(threading.RLock())\n",
    "\n",
    "def process_model(model, num_rows_per_request, dataset_df: pd.DataFrame, system_prompt) -> set[int]:\n",
    "    total_ids = set()\n",
    "\n",
    "    with tqdm(range(0, len(dataset_df.index), num_rows_per_request), desc=f\"Querying {model}\", total=len(dataset_df.index), unit=\" rows\") as pbar:\n",
    "        i = 0\n",
    "        while i < len(dataset_df.index):\n",
    "            try:\n",
    "                # Get row range as the LLMs cannot process the entire file at once\n",
    "                start = i\n",
    "                end = min(i + num_rows_per_request, len(dataset_df.index))\n",
    "                df_section: pd.DataFrame = dataset_df.iloc[start:end]\n",
    "                df_section_csv = df_section.to_csv(index=False)\n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": csv_prompt(df_section_csv, start + 1, end + 1, len(dataset_df.index)),\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                result: NonAccidentIDs = NonAccidentIDs.model_validate_json(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "\n",
    "                for id in result.ids:\n",
    "                    # Throw an error if a returned ID is not in the dataset\n",
    "                    if id not in dataset_df[\"id\"].values:\n",
    "                        raise ValueError(f\"ID {id} not in dataset\")\n",
    "\n",
    "                total_ids.update(result.ids)\n",
    "\n",
    "                # Update tqdm progress bar\n",
    "                pbar.update(end - start)\n",
    "                i += num_rows_per_request\n",
    "            except Exception as e:\n",
    "                # If we get an error, retry the model (i.e. do not increment i)\n",
    "                logging.warning(f\"Failed to query {model}: {e}\")\n",
    "\n",
    "    return total_ids\n",
    "\n",
    "# Run LLMs in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_model,\n",
    "            model,\n",
    "            num_rows_per_request,\n",
    "            related_news_article_df,\n",
    "            system_prompt\n",
    "        ): model\n",
    "        for model, num_rows_per_request in models\n",
    "    }\n",
    "\n",
    "    model_ids = {}\n",
    "\n",
    "    for f in futures.keys():\n",
    "        result = f.result()\n",
    "        model_ids[futures[f]] = result\n",
    "\n",
    "all_ids = list(model_ids.values())\n",
    "# Combine IDs from all LLMs\n",
    "ids_union = all_ids[0].union(*all_ids[1:])\n",
    "\n",
    "# Save the LLM excluded articles as a CSV to review\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_exclusions.csv\", index=False)\n",
    "# Save the same LLM excluded articles as a CSV. This CSV will be manually checked and modified\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b555b459dc1ee45f",
   "metadata": {},
   "source": [
    "# 2.c. Manually review LLM exclusions\n",
    "The LLMs tend to flag valid articles as invalid. Since this is a short list (because we already removed the bulk of invalid articles in step 1), we can go through the list manually and remove the valid articles.\n",
    "\n",
    "## Removed IDs\n",
    "- 3699\n",
    "- 1352\n",
    "- 370\n",
    "- 287\n",
    "- 489424\n",
    "- 491118\n",
    "- 491371\n",
    "- 494102\n",
    "- 495320\n",
    "- 495942\n",
    "- 496362\n",
    "\n",
    "*Note: `local_news_articles_llm_exclusions.csv` contains the excluded rows as given by the LLMs. `local_news_articles_llm_manual_exclusions.csv` contains the same excluded rows given by the LLMs, except that any rows referring to valid accidents were manually removed.*"
   ]
  },
  {
   "cell_type": "code",
   "id": "53eeb00bf883dcff",
   "metadata": {},
   "source": [
    "# Manual removal of valid rows\n",
    "manual_ids = {3699, 1352, 370, 287, 489424, 491118, 491371, 494102, 495320, 495942, 496362}\n",
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "llm_manual_exclusions = llm_manual_exclusions[~llm_manual_exclusions[\"id\"].isin(manual_ids)]\n",
    "llm_manual_exclusions.to_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f5a91b49de2f99f",
   "metadata": {},
   "source": [
    "llm_manual_exclusions = pd.read_csv(\"data/intermediate/local_news_articles_llm_manual_exclusions.csv\")\n",
    "\n",
    "# Filtering the original news articles\n",
    "news_articles_df = related_news_article_df[~related_news_article_df[\"id\"].isin(llm_manual_exclusions[\"id\"])]\n",
    "news_articles_df.to_csv(\"data/local_news_articles.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Per-person data",
   "id": "7f3514f310ae288e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "person_data = []\n",
    "\n",
    "def remove_contained_spans(spans: list[tuple[Any, Span]]) -> list[tuple[Any, Span]]:\n",
    "    filtered_spans = []\n",
    "    spans = sorted(spans, key=lambda s: (s[1].end - s[1].start), reverse=True)\n",
    "\n",
    "    for x, span in spans:\n",
    "        contained = False\n",
    "        for _, kept in filtered_spans:\n",
    "            if span.start >= kept.start and span.end <= kept.end:\n",
    "                contained = True\n",
    "                break\n",
    "        if not contained:\n",
    "            filtered_spans.append((x, span))\n",
    "\n",
    "    return list(sorted(filtered_spans, key=lambda s: s[1].start))\n",
    "\n",
    "age_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "age_patterns = [\n",
    "    [{\"LIKE_NUM\": True}, {\"LOWER\": \"year\"}, {\"LOWER\": \"old\"}],\n",
    "    [{\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LOWER\": \"year\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"old\"}],\n",
    "    [{\"TEXT\": \",\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}]\n",
    "]\n",
    "\n",
    "age_matcher.add(\"AGE\", age_patterns)\n",
    "\n",
    "severity_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "severity_patterns = [\n",
    "    [{\"LIKE_NUM\": True}, {\"LOWER\": \"year\"}, {\"LOWER\": \"old\"}],\n",
    "    [{\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LOWER\": \"year\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"old\"}],\n",
    "    [{\"TEXT\": \",\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}]\n",
    "]\n",
    "\n",
    "severity_matcher.add(\"SEVERITY\", age_patterns)\n",
    "\n",
    "def get_ages(doc: Doc) -> list[tuple[int, Span]]:\n",
    "    ages = []\n",
    "    matches = age_matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span: Span = doc[start:end]\n",
    "        age_int = -1\n",
    "        for token in span:\n",
    "            if token.like_num:\n",
    "                try:\n",
    "                    age_int = int(token.text)\n",
    "                except ValueError:\n",
    "                    age_int = w2n.word_to_num(token.text)\n",
    "                break\n",
    "        if age_int <= 0 or age_int >= 120:\n",
    "            continue\n",
    "        ages.append((age_int, span))\n",
    "\n",
    "    return ages\n",
    "\n",
    "gender_words = {\n",
    "    \"man\": \"male\", \"male\": \"male\", \"he\": \"male\", \"him\": \"male\",\n",
    "    \"woman\": \"female\", \"female\": \"female\", \"she\": \"female\", \"her\": \"female\",\n",
    "}\n",
    "\n",
    "def get_genders(doc: Doc) -> list[tuple[str, Span]]:\n",
    "    genders = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ in gender_words:\n",
    "            genders.append((gender_words[token.lemma_], Span(doc, token.i, token.i + 1)))\n",
    "    return genders\n",
    "\n",
    "accepted_injuries = {\"serious\", \"slight\", \"grievous\", \"light\", \"critical\", \"fatal\", \"slight\", \"bad\"}\n",
    "def get_severities(doc: Doc) -> list[tuple[str, Span]]:\n",
    "    severities = []\n",
    "    def adverb_to_adj(word: str):\n",
    "        w = word.lower()\n",
    "        if w.endswith(\"ously\"):\n",
    "            return w[:-2]\n",
    "        if w.endswith(\"ally\"):\n",
    "            return w[:-2]\n",
    "        if w.endswith(\"ly\"):\n",
    "            return w[:-2]\n",
    "        return w\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ == \"injure\" or token.lemma_ == \"injury\":\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"advmod\" or child.dep_ == \"amod\":\n",
    "                    injury_adj = adverb_to_adj(child.lemma_)\n",
    "                    if injury_adj not in accepted_injuries:\n",
    "                        continue\n",
    "                    severities.append((injury_adj, Span(doc, child.i, child.i + 1)))\n",
    "\n",
    "    return severities\n",
    "\n",
    "person_matcher = Matcher(nlp.vocab)\n",
    "person_nouns = [\"man\", \"woman\", \"boy\", \"girl\", \"teen\", \"teenager\", \"baby\", \"person\", \"driver\", \"passenger\",\n",
    "    \"pedestrian\", \"victim\", \"motorcyclist\", \"cyclist\", \"rider\",\n",
    "    \"motorist\", \"resident\", \"teenager\", \"youth\"]\n",
    "vehicle_nouns = [\"car\", \"motorcycle\", \"truck\", \"van\", \"bus\", \"bicycle\"]\n",
    "person_patterns = [\n",
    "    [{\"LOWER\": {\"IN\": [\"a\", \"an\", \"the\", \"one\"]}, \"OP\": \"?\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LOWER\": \"year\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"old\"}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"LOWER\": {\"IN\": person_nouns}, \"OP\": \"?\"}],\n",
    "    [{\"LOWER\": {\"IN\": [\"a\", \"an\", \"the\", \"one\"]}}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"LOWER\": {\"IN\": person_nouns}}],\n",
    "    [{\"LOWER\": {\"IN\": [\"a\", \"an\", \"the\", \"one\"]}}, {\"POS\": \"ADJ\", \"OP\": \"?\"}, {\"LOWER\": {\"IN\": vehicle_nouns}}, {\"LOWER\": \"driver\"}],\n",
    "    [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"TEXT\": \",\"}, {\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}],\n",
    "]\n",
    "\n",
    "person_matcher.add(\"PERSON\", person_patterns)\n",
    "\n",
    "def get_persons(doc: Doc) -> list[tuple[str, Span]]:\n",
    "    people = []\n",
    "    matches = person_matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        people.append((span.text, span))\n",
    "\n",
    "    return remove_contained_spans(people)\n",
    "\n",
    "test_doc = nlp(\"a 54-year-old English man from Mellieha\")\n",
    "# print(get_persons(test_doc))\n",
    "displacy.render(test_doc)\n",
    "#\n",
    "# for token in test_doc:\n",
    "#     print(token, \"|\", token.lemma_)\n",
    "\n",
    "for idx, row in news_articles_df.iterrows():\n",
    "    content = row[\"content\"]\n",
    "    doc = nlp(content)\n",
    "\n",
    "    # displacy.render(doc)\n",
    "    ages = get_ages(doc)\n",
    "    genders = get_genders(doc)\n",
    "    severities = get_severities(doc)\n",
    "    people = get_persons(doc)\n",
    "    print(row[\"id\"], people)\n",
    "\n"
   ],
   "id": "b094fab8bdd44477",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "784858e8090d1027",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
