{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T14:59:25.130767Z",
     "start_time": "2025-12-06T14:59:24.396144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union, Any, Callable, Literal\n",
    "\n",
    "from pydantic.main import IncEx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "# from anthropic import AnthropicFoundry, beta_tool\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from dotenv import load_dotenv\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "id": "967e254963ec671e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\alang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\alang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T14:59:27.608690Z",
     "start_time": "2025-12-06T14:59:27.563824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load datasets as raw CSVs\n",
    "news_articles_df = pd.read_csv(\"data/local_news_articles.csv\")\n",
    "press_releases_df = pd.read_csv(\"data/police_press_releases.csv\")"
   ],
   "id": "96d7bc4a3fd10f1c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T14:59:29.068212Z",
     "start_time": "2025-12-06T14:59:28.998849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mapping = str.maketrans({\n",
    "    \"ċ\": \"c\",\n",
    "    \"Ċ\": \"C\",\n",
    "    \"ġ\": \"g\",\n",
    "    \"Ġ\": \"G\",\n",
    "    \"ħ\": \"h\",\n",
    "    \"Ħ\": \"H\",\n",
    "    \"ż\": \"z\",\n",
    "    \"Ż\": \"Z\",\n",
    "    \"“\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"‘\": \"'\",\n",
    "    \"’\": \"'\",\n",
    "    \"—\": \"-\",\n",
    "    \"–\": \"-\",\n",
    "    \"…\": \"...\",\n",
    "})\n",
    "\n",
    "def clean_articles(df: pd.DataFrame, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).str.translate(mapping)\n",
    "    return df\n",
    "\n",
    "news_articles_df = clean_articles(news_articles_df, [\"title\", \"subtitle\", \"content\"])\n",
    "press_releases_df = clean_articles(press_releases_df, [\"title\", \"content\"])"
   ],
   "id": "cab8d8f36780746b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T14:59:30.770767Z",
     "start_time": "2025-12-06T14:59:30.731769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test = \"A man died\"\n",
    "doc = nlp(test)\n",
    "for token in doc:\n",
    "    print(token, \" | \",\n",
    "          spacy.explain(token.pos_),\n",
    "          \" | \", token.lemma_)\n",
    "displacy.render(doc, style='dep')"
   ],
   "id": "fb78a72893701534",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  |  determiner  |  a\n",
      "man  |  noun  |  man\n",
      "died  |  verb  |  die\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7bf55fee193242a69a9171b98256f188-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">A</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">died</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7bf55fee193242a69a9171b98256f188-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7bf55fee193242a69a9171b98256f188-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7bf55fee193242a69a9171b98256f188-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7bf55fee193242a69a9171b98256f188-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,91.5 L237,79.5 253,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T15:00:08.205849Z",
     "start_time": "2025-12-06T14:59:58.945519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "people_subj = {\"man\", \"woman\", \"child\", \"driver\", \"motorist\", \"motorcyclist\", \"pedestrian\"}\n",
    "vehicles = {\"car\", \"motorcycle\", \"truck\", \"van\", \"bus\", \"bicycle\",\n",
    "            \"Audi\", \"BMW\", \"Chevrolet\", \"Citroen\", \"Dodge\", \"Fiat\", \"Ford\", \"Honda\", \"Hyundai\", \"Isuzu\",\n",
    "            \"Jaguar\", \"Jeep\", \"Kia\", \"Kymco\", \"Mercedes\", \"Mercedes-Benz\", \"Mini\", \"Mitsubishi\", \"Nissan\",\n",
    "            \"Peugeot\", \"Renault\", \"Skoda\", \"Subaru\", \"Suzuki\", \"Toyota\", \"Volkswagen\", \"VW\", \"Volvo\"}\n",
    "accident = {\"accident\", \"crash\", \"collision\"}\n",
    "injuries = {\"injure\", \"die\"}\n",
    "\n",
    "phrases = [\n",
    "    \"car crash\", \"traffic accident\", \"road accident\", \"collision\",\n",
    "    \"crashed\", \"crash\", \"hit by a car\", \"motorcycle accident\",\n",
    "    \"injured in a crash\", \"overturned\", \"run over\", \"lost control\"\n",
    "]\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = [nlp(text) for text in phrases]\n",
    "matcher.add(\"ACCIDENT_PATTERNS\", patterns)\n",
    "\n",
    "def refers_to_accident(text: str) -> bool:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "\n",
    "    has_people = False\n",
    "    has_vehicles = False\n",
    "    has_accident = False\n",
    "    has_injury = False\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_ in people_subj:\n",
    "            has_people = True\n",
    "\n",
    "        if token.lemma_ in vehicles:\n",
    "            has_vehicles = True\n",
    "\n",
    "        if token.lemma_ in accident:\n",
    "            has_accident = True\n",
    "\n",
    "        if token.lemma_ in injuries:\n",
    "            has_injury = True\n",
    "\n",
    "        if has_people and has_vehicles and (has_accident or has_injury):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "non_related_news_article_ids = []\n",
    "\n",
    "for id, text in news_articles_df[[\"id\", \"content\"]].values:\n",
    "    is_accident = refers_to_accident(text)\n",
    "    if not is_accident:\n",
    "        non_related_news_article_ids.append(id)\n",
    "\n",
    "os.makedirs(\"data/intermediate\", exist_ok=True)\n",
    "\n",
    "non_related_news_article_df = news_articles_df[news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "related_news_article_df = news_articles_df[~news_articles_df[\"id\"].isin(non_related_news_article_ids)]\n",
    "non_related_news_article_df.to_csv(\"data/intermediate/local_news_articles_exclusions.csv\", index=False)\n",
    "related_news_article_df.to_csv(\"data/intermediate/local_news_articles.csv\", index=False)"
   ],
   "id": "507144bfe0768b4c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T15:00:11.243627Z",
     "start_time": "2025-12-06T15:00:10.970782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "endpoint = \"https://news-analysis-resource.openai.azure.com/openai/v1/\"\n",
    "\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token_provider,\n",
    ")"
   ],
   "id": "bb25c1ae9683630d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T15:33:23.936474Z",
     "start_time": "2025-12-06T15:24:41.255566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NonAccidentIDs(BaseModel):\n",
    "    ids: List[int] = Field(description=\"A list of ids of news articles that are not accidents\")\n",
    "\n",
    "models = [(\"gpt-5-mini\", 50), (\"grok-4-fast-non-reasoning\", 50), (\"DeepSeek-R1-0528\", 20)]\n",
    "\n",
    "system_msg = \"\"\"\n",
    "You are a senior data scientist reviewing a semi-structured dataset of vehicle accidents news articles. The articles were obtained by simple web scraping (ex: on the tag of the article) which means that some articles do not refer to actual accidents (for example, they refer to new accident/traffic measures).\n",
    "\n",
    "Your job is to review the given accident CSV and return a list of news article IDs that do not refer to accidents.\n",
    "Be very critical! Any article which mentions a specific accident and provides details on it should not be removed.\n",
    "\n",
    "IMPORTANT: You MUST return the data by calling the `set_non_accident_ids` function.\n",
    "\n",
    "Do not return anything other than a function call.\n",
    "\"\"\"\n",
    "\n",
    "csv_prompt = lambda dataset_csv, start_rows, end_rows, total_rows: f\"\"\"\n",
    "MAKE SURE THAT THE RETURNED IDS EXIST IN THIS CSV!\n",
    "\n",
    "Accident CSV Data ({start_rows}-{end_rows}/{total_rows} rows):\n",
    "\n",
    "{dataset_csv}\n",
    "\"\"\"\n",
    "\n",
    "result_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"set_non_accident_ids\",\n",
    "        \"description\": \"Set the IDs of the news articles which do not refer to an accident\",\n",
    "        \"parameters\": NonAccidentIDs.model_json_schema()\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [result_function]\n",
    "\n",
    "tqdm.set_lock(threading.RLock())\n",
    "\n",
    "def process_model(model, num_rows_per_request, dataset_df: pd.DataFrame, system_prompt) -> set[int]:\n",
    "    total_ids = set()\n",
    "\n",
    "    with tqdm(range(0, len(dataset_df.index), num_rows_per_request), desc=f\"Querying {model}\", total=len(dataset_df.index), unit=\" rows\") as pbar:\n",
    "        i = 0\n",
    "        while i < len(dataset_df.index):\n",
    "            try:\n",
    "                start = i\n",
    "                end = min(i + num_rows_per_request, len(dataset_df.index))\n",
    "\n",
    "                df_section: pd.DataFrame = dataset_df.iloc[start:end]\n",
    "                df_section_csv = df_section.to_csv(index=False)\n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": csv_prompt(df_section_csv, start + 1, end + 1, len(dataset_df.index)),\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                result: NonAccidentIDs = NonAccidentIDs.model_validate_json(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "                for id in result.ids:\n",
    "                    if id not in dataset_df[\"id\"].values:\n",
    "                        raise ValueError(f\"ID {id} not in dataset {dataset_type}\")\n",
    "\n",
    "                total_ids.update(result.ids)\n",
    "\n",
    "                pbar.update(end - start)\n",
    "                i += num_rows_per_request\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to query {model}: {e}\")\n",
    "\n",
    "    return total_ids\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_model,\n",
    "            model,\n",
    "            num_rows_per_request,\n",
    "            related_news_article_df,\n",
    "            system_msg\n",
    "        ): model\n",
    "        for model, num_rows_per_request in models\n",
    "    }\n",
    "\n",
    "    model_ids = {}\n",
    "\n",
    "    for f in futures.keys():\n",
    "        result = f.result()\n",
    "        model_ids[futures[f]] = result\n",
    "\n",
    "    print({\n",
    "        model: list(sorted(ids)) for model, ids in model_ids.items()\n",
    "    })\n",
    "\n",
    "all_ids = list(model_ids.values())\n",
    "ids_intersect = all_ids[0].intersection(*all_ids[1:])\n",
    "ids_union = all_ids[0].union(*all_ids[1:])\n",
    "for model, ids in model_ids.items():\n",
    "    related_news_article_df[related_news_article_df[\"id\"].isin(ids)].to_csv(f\"data/intermediate/local_news_articles_{model}_exclusions.csv\")\n",
    "\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_intersect)].to_csv(\"data/intermediate/local_news_articles_llm_intersect_exclusions.csv\")\n",
    "\n",
    "related_news_article_df[related_news_article_df[\"id\"].isin(ids_union)].to_csv(\"data/intermediate/local_news_articles_llm_union_exclusions.csv\")"
   ],
   "id": "c00e9ea8a57544a3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying gpt-5-mini:   0%|          | 0/242 [00:00<?, ? rows/s]00<?, ? rows/s]\u001B[A\n",
      "\n",
      "Querying DeepSeek-R1-0528:   0%|          | 0/242 [00:00<?, ? rows/s]\u001B[A\u001B[A\n",
      "Querying grok-4-fast-non-reasoning:  21%|██        | 50/242 [00:02<00:08, 22.19 rows/s]\u001B[A\n",
      "Querying grok-4-fast-non-reasoning:  41%|████▏     | 100/242 [00:03<00:05, 26.78 rows/s]\u001B[A\n",
      "Querying grok-4-fast-non-reasoning:  62%|██████▏   | 150/242 [00:05<00:02, 31.16 rows/s]\u001B[A\n",
      "Querying grok-4-fast-non-reasoning:  83%|████████▎ | 200/242 [00:06<00:01, 31.97 rows/s]\u001B[A\n",
      "Querying grok-4-fast-non-reasoning: 100%|██████████| 242/242 [00:08<00:00, 28.69 rows/s]\u001B[A\n",
      "Querying gpt-5-mini:  41%|████▏     | 100/242 [00:32<00:45,  3.15 rows/s]\n",
      "\n",
      "Querying gpt-5-mini:  62%|██████▏   | 150/242 [01:00<00:38,  2.36 rows/s]rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying gpt-5-mini:  83%|████████▎ | 200/242 [01:22<00:18,  2.30 rows/s]rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying gpt-5-mini: 100%|██████████| 242/242 [01:53<00:00,  2.13 rows/s]rows]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Querying DeepSeek-R1-0528:  33%|███▎      | 80/242 [02:45<05:57,  2.21s/ rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying DeepSeek-R1-0528:  41%|████▏     | 100/242 [02:57<03:50,  1.63s/ rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying DeepSeek-R1-0528:  50%|████▉     | 120/242 [04:04<04:28,  2.20s/ rows]\u001B[A\u001B[AWARNING:root:Failed to query DeepSeek-R1-0528: 'NoneType' object is not subscriptable\n",
      "\n",
      "\n",
      "Querying DeepSeek-R1-0528:  58%|█████▊    | 140/242 [05:05<04:12,  2.48s/ rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying DeepSeek-R1-0528:  66%|██████▌   | 160/242 [06:32<04:12,  3.08s/ rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying DeepSeek-R1-0528:  74%|███████▍  | 180/242 [06:55<02:33,  2.47s/ rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying DeepSeek-R1-0528:  83%|████████▎ | 200/242 [07:12<01:22,  1.97s/ rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying DeepSeek-R1-0528:  91%|█████████ | 220/242 [07:46<00:41,  1.88s/ rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying DeepSeek-R1-0528:  99%|█████████▉| 240/242 [08:10<00:03,  1.68s/ rows]\u001B[A\u001B[A\n",
      "\n",
      "Querying DeepSeek-R1-0528: 100%|██████████| 242/242 [08:42<00:00,  2.16s/ rows]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpt-5-mini': [46, 645, 894, 1387, 2022, 2092, 493223, 493920, 496068, 496154], 'grok-4-fast-non-reasoning': [46, 645, 894, 1352, 1387, 2022, 2092, 3604, 3699, 3809, 3823, 3827, 490929, 491047, 491118, 491371, 491380, 491437, 493212, 493223, 493920, 495320, 495442, 495942, 496006, 496154, 496206, 496362], 'DeepSeek-R1-0528': [46, 645, 695, 894, 1352, 1387, 2022, 2092, 2873, 3535, 3699, 3827, 467108, 467185, 467297, 489424, 490206, 490685, 490742, 490929, 491047, 491118, 491371, 491380, 491437, 493212, 493223, 493920, 496068]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "425224d740b9adae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:46.120641Z",
     "start_time": "2025-12-03T07:41:46.106831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Gender(Enum):\n",
    "    Male = \"male\"\n",
    "    Female = \"female\"\n",
    "    Other = \"other\"\n",
    "    NA = \"na\"\n",
    "\n",
    "class Injury(Enum):\n",
    "    none = \"none\"\n",
    "    Slight = \"slight\"\n",
    "    Grievous = \"grievous\"\n",
    "    Critical = \"critical\"\n",
    "    Dead = \"dead\"\n",
    "\n",
    "class VehicleType(str, Enum):\n",
    "    Car = \"car\"\n",
    "    Truck = \"truck\"\n",
    "    Bus = \"bus\"\n",
    "    Motorcycle = \"motorcycle\"\n",
    "    Bicycle = \"bicycle\"\n",
    "    Van = \"van\"\n",
    "    Pedestrian = \"pedestrian\"\n",
    "\n",
    "class RoadCondition(str, Enum):\n",
    "    Dry = \"dry\"\n",
    "    Wet = \"wet\"\n",
    "    Flood = \"flood\"\n",
    "    Oil = \"oil\"\n",
    "    Debris = \"debris\"\n",
    "    Uneven = \"uneven\"\n",
    "    Obstructed = \"obstructed\"\n",
    "\n",
    "class HumanFactor(str, Enum):\n",
    "    Drunk = \"drunk\"\n",
    "    Fatigue = \"fatigue\"\n",
    "    Speeding = \"speeding\"\n",
    "\n",
    "class Infrastructure(str, Enum):\n",
    "    Intersection = \"intersection\"\n",
    "    Roundabout = \"roundabout\"\n",
    "    Junction = \"junction\"\n",
    "    Highway = \"highway\"\n",
    "    Residential_Road = \"residential_road\"\n",
    "    Bridge = \"bridge\"\n",
    "    Tunnel = \"tunnel\"\n",
    "    Crosswalk = \"crosswalk\"\n",
    "\n",
    "class AccidentRecord(BaseModel, extra=\"forbid\"):\n",
    "    model_config = ConfigDict(use_enum_values=True)\n",
    "\n",
    "    id: int = Field(description=\"\"\"The id of the accident. This is the same as the given CSV.\"\"\")\n",
    "    date_of_accident: Optional[datetime] = Field(description=\"\"\"The date and time that the accident occurred\"\"\")\n",
    "    num_people_involved: int = Field(description=\"\"\"The number of people involved in the accident\"\"\")\n",
    "    num_drivers: int = Field(description=\"\"\"The number of drivers involved in the accident\"\"\")\n",
    "    driver_ages: List[Optional[int]] = Field(description=\"\"\"The ages of the drivers involved. The length of this list must be the same as `num_drivers`. Leave as None if not available\"\"\")\n",
    "    driver_genders: List[Union[Gender, str]] = Field(description=\"\"\"The genders of the drivers involved in the accident. The length of this list must be the same as `num_drivers`\"\"\")\n",
    "    injury_severities: List[Union[Injury, str]] = Field(description=\"\"\"The severity of injuries of each person. The length of this list must be the same as `num_people_involved`\"\"\")\n",
    "    # accident_type: Literal[\"head-on\", \"rear-end\", \"multi-vehicle\", \"other\"]\n",
    "    # \"\"\"The type of accident\"\"\"\n",
    "    num_vehicles_involved: int = Field(description=\"\"\"The number of vehicles involved in the accident\"\"\")\n",
    "    vehicle_types: List[Union[VehicleType, str]] = Field(description=\"\"\"The types of vehicles involved in the accident. The length of this list must be the same as `num_vehicles_involved`\"\"\")\n",
    "    road_conditions: List[Union[RoadCondition, str]] = Field(description=\"\"\"The condition of the road that the accident occurred\"\"\")\n",
    "    human_factors: List[Union[HumanFactor, str]] = Field(description=\"\"\"The human factors which caused the accident to occur\"\"\")\n",
    "    infrastructure: List[Union[Infrastructure, str]] = Field(description=\"\"\"The infrastructure where the accident occurred\"\"\")\n",
    "    road_name: Optional[str] = Field(description=\"\"\"The name of the road that the accident occurred\"\"\")\n",
    "    locality: Optional[str] = Field(description=\"\"\"The name of the locality that the accident occurred\"\"\")\n",
    "    valid_accident: bool = Field(description=\"\"\"Whether or not the record refers to a vehicle accident\"\"\")\n",
    "\n",
    "    def model_dump(\n",
    "        self,\n",
    "        *,\n",
    "        mode: Literal['json', 'python'] | str = 'python',\n",
    "        include: IncEx | None = None,\n",
    "        exclude: IncEx | None = None,\n",
    "        context: Any | None = None,\n",
    "        by_alias: bool | None = None,\n",
    "        exclude_unset: bool = False,\n",
    "        exclude_defaults: bool = False,\n",
    "        exclude_none: bool = False,\n",
    "        exclude_computed_fields: bool = False,\n",
    "        round_trip: bool = False,\n",
    "        warnings: bool | Literal['none', 'warn', 'error'] = True,\n",
    "        fallback: Callable[[Any], Any] | None = None,\n",
    "        serialize_as_any: bool = False,\n",
    "    ) -> dict[str, Any]:\n",
    "        data = super().model_dump(mode=mode, include=include, exclude=exclude, context=context,\n",
    "                                  by_alias=by_alias, exclude_unset=exclude_unset, exclude_defaults=exclude_defaults,\n",
    "                                  exclude_none=exclude_none, exclude_computed_fields=exclude_computed_fields,\n",
    "                                  round_trip=round_trip, warnings=warnings, fallback=fallback,\n",
    "                                  serialize_as_any=serialize_as_any)\n",
    "        data.pop(\"valid_accident\")\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def empty(id: int):\n",
    "        return AccidentRecord(\n",
    "            id=id,\n",
    "            date_of_accident=None,\n",
    "            num_people_involved=0,\n",
    "            num_drivers=0,\n",
    "            driver_ages=[],\n",
    "            driver_genders=[],\n",
    "            injury_severities=[],\n",
    "            num_vehicles_involved=0,\n",
    "            vehicle_types=[],\n",
    "            road_conditions=[],\n",
    "            human_factors=[],\n",
    "            infrastructure=[],\n",
    "            road_name=None,\n",
    "            locality=None,\n",
    "            valid_accident=False\n",
    "        )\n",
    "\n",
    "\n",
    "class Accidents(BaseModel, extra=\"forbid\"):\n",
    "    accidents: List[AccidentRecord]\n",
    "    \"\"\"A list of all accidents\"\"\"\n",
    "\n",
    "    def to_dict_list(self):\n",
    "        return [a.model_dump() for a in self.accidents if a.valid_accident]"
   ],
   "id": "237e1af0f34acc1c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:46.130153Z",
     "start_time": "2025-12-03T07:41:46.125410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [(\"gpt-5-mini\", 50), (\"grok-4-fast-non-reasoning\", 50), (\"DeepSeek-R1-0528\", 20)]\n",
    "\n",
    "system_msg = lambda dataset_type: f\"\"\"\n",
    "You are given a number of rows of a semi-structured CSV dataset of {dataset_type} about car accidents.\n",
    "Your task is to extract information from the content of each record and return a list of accidents with their corresponding data.\n",
    "Make sure that the number of elements in the list in the returned JSON match the number of rows in the given CSV.\n",
    "\n",
    "IMPORTANT: You MUST return the data by calling the `set_accidents` function.\n",
    "VERY IMPORTANT: YOU MUST RETURN THE SAME NUMBER OF ELEMENTS AS ROWS GIVEN IN THE CSV.\n",
    "If you cannot find a value for a record or field, return a default value:\n",
    "- int: 0\n",
    "- str: \"\"\n",
    "- Optional[...]: None\n",
    "- List: []\n",
    "\n",
    "The ID MUST be copied from the given CSV.\n",
    "If a record is not of a vehicle accident (i.e. of other news), set `valid_accident` to False. Otherwise, set it to True.\n",
    "If you cannot fill in any values, set `valid_accident` to False. DO NOT RETURN A FULLY EMPTY ROW WITH `valid_accident` as True.\n",
    "\n",
    "DO NOT CREATE ANY INFORMATION NOT PRESENT IN THE CSV.\n",
    "DO NOT include any introduction such as \"Here is the answer...\" or conclusion, only the JSON.\n",
    "\"\"\"\n",
    "\n",
    "csv_prompt = lambda dataset_csv, start_rows, end_rows, total_rows, extra=None: f\"\"\"\n",
    "{f\"IMPORTANT: {extra}\" if extra else \"\"}\n",
    "\n",
    "Make sure to return EXACTLY {end_rows - start_rows} rows.\n",
    "\n",
    "Accident CSV Data ({start_rows}-{end_rows}/{total_rows} rows):\n",
    "\n",
    "{dataset_csv}\n",
    "\"\"\"\n",
    "\n",
    "datasets = [\n",
    "    (\"local news articles\", news_articles_df),\n",
    "    (\"police press releases\", press_releases_df),\n",
    "]\n"
   ],
   "id": "6819c4bbbe2fec02",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:41:46.142210Z",
     "start_time": "2025-12-03T07:41:46.134851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = Path(\"data/llm_processed\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "result_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"set_accidents\",\n",
    "        \"description\": \"Set the accident data obtained from the given CSV\",\n",
    "        \"parameters\": Accidents.model_json_schema()\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [result_function]\n",
    "\n",
    "def to_snake_case(text: str):\n",
    "    return text.lower().replace(\" \", \"_\").replace(\"-\", \"_\")"
   ],
   "id": "c900127611ec6f27",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:40:58.884475Z",
     "start_time": "2025-12-03T07:41:46.149693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tqdm.set_lock(threading.RLock())\n",
    "\n",
    "def process_model(model, num_rows_per_request, dataset_df: pd.DataFrame, system_prompt):\n",
    "    save_path = data_dir / f\"{to_snake_case(dataset_type)}_{to_snake_case(model)}.csv\"\n",
    "    if not os.path.exists(save_path):\n",
    "        open(save_path, \"w\").close()\n",
    "\n",
    "    with tqdm(range(0, len(dataset_df.index), num_rows_per_request), desc=f\"Querying {model}\", total=len(dataset_df.index), unit=\" rows\") as pbar:\n",
    "        i = 0\n",
    "        extra = None\n",
    "        while i < len(dataset_df.index):\n",
    "            try:\n",
    "                start = i\n",
    "                end = min(i + num_rows_per_request, len(dataset_df.index))\n",
    "\n",
    "                df_section: pd.DataFrame = dataset_df.iloc[start:end]\n",
    "                df_section_csv = df_section.to_csv(index=False)\n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": csv_prompt(df_section_csv, start + 1, end + 1, len(dataset_df.index), extra),\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    tools=tools,\n",
    "                    messages=messages,\n",
    "                )\n",
    "\n",
    "                result: Accidents = Accidents.model_validate_json(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "                for accident in result.accidents:\n",
    "                    if accident.id not in dataset_df[\"id\"].values:\n",
    "                        raise ValueError(f\"ID {accident.id} not in dataset {dataset_type}\")\n",
    "\n",
    "                if len(result.accidents) != end - start:\n",
    "                    cur_ids = set(accident.id for accident in result.accidents)\n",
    "                    df_ids = set(df_section[\"id\"])\n",
    "                    missing_ids = [str(id) for id in df_ids - cur_ids]\n",
    "                    extra = f\"Make sure to include ids: {\", \".join(missing_ids)}\"\n",
    "                    raise ValueError(f\"Received a different number of rows (expected: {end - start}, received: {len(result.accidents)})\")\n",
    "\n",
    "                result_list = result.to_dict_list()\n",
    "                with open(save_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.DictWriter(f, result_list[0].keys())\n",
    "                    if os.path.getsize(save_path) == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    writer.writerows(result_list)\n",
    "\n",
    "                pbar.update(end - start)\n",
    "                i += num_rows_per_request\n",
    "                extra = None\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to query {model}: {e}\")\n",
    "\n",
    "for dataset_type, dataset_df in datasets:\n",
    "    system_prompt = system_msg(dataset_type)\n",
    "    with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_model,\n",
    "                model,\n",
    "                num_rows_per_request,\n",
    "                dataset_df,\n",
    "                system_prompt\n",
    "            )\n",
    "            for model, num_rows_per_request in models\n",
    "        ]\n",
    "\n",
    "        # Optional: wait for all to complete\n",
    "        for f in futures:\n",
    "            f.result()\n"
   ],
   "id": "fc962ef71b2d6b3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Querying grok-4-fast-non-reasoning:   0%|          | 0/252 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04be476396ba4f5a8fd94f3d36967016"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Querying gpt-5-mini:   0%|          | 0/252 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90999553c8bc490096056c7d83ab8b04"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Querying DeepSeek-R1-0528:   0%|          | 0/252 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28d76ad4833c49699a8475e43afc2484"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to query grok-4-fast-non-reasoning: Request timed out.\n",
      "WARNING:root:Failed to query gpt-5-mini: 2 validation errors for Accidents\n",
      "accidents.0.date_of_accident\n",
      "  Input should be a valid datetime or date, unexpected extra characters at the end of the input [type=datetime_from_date_parsing, input_value='2025-10-09 15:07:23.444052+00', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/datetime_from_date_parsing\n",
      "accidents.1.date_of_accident\n",
      "  Input should be a valid datetime or date, unexpected extra characters at the end of the input [type=datetime_from_date_parsing, input_value='2025-10-10 12:22:30.72069+00', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/datetime_from_date_parsing\n",
      "WARNING:root:Failed to query DeepSeek-R1-0528: 'NoneType' object is not subscriptable\n",
      "WARNING:root:Failed to query DeepSeek-R1-0528: 'NoneType' object is not subscriptable\n",
      "WARNING:root:Failed to query grok-4-fast-non-reasoning: Model service is unavailable.\n",
      "WARNING:root:Failed to query grok-4-fast-non-reasoning: Model service is unavailable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Querying gpt-5-mini:   0%|          | 0/111 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a26ba5983564658b1d13a745b57ce99"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Querying grok-4-fast-non-reasoning:   0%|          | 0/111 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "046088ff91db4d0bb1057944dd35428e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Querying DeepSeek-R1-0528:   0%|          | 0/111 [00:00<?, ? rows/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "575ba53a1dfe4c8db518f8a93d0d5a08"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to query DeepSeek-R1-0528: 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:40:59.093734Z",
     "start_time": "2025-12-03T08:40:59.011320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "# Validate/Clean CSVs\n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    replace_vals = {\n",
    "        \"unknown\": \"na\"\n",
    "    }\n",
    "\n",
    "    def replace_values(iter):\n",
    "        return [replace_vals[v] if v in replace_vals else v for v in iter]\n",
    "\n",
    "    df[\"date_of_accident\"] = pd.to_datetime(df[\"date_of_accident\"], format=\"mixed\", dayfirst=True, utc=True)\n",
    "    df[\"driver_ages\"] = df[\"driver_ages\"].apply(lambda x: replace_values(ast.literal_eval(x)))\n",
    "    df[\"driver_genders\"] = df[\"driver_genders\"].apply(lambda x: replace_values(ast.literal_eval(x)))\n",
    "    df[\"injury_severities\"] = df[\"injury_severities\"].apply(lambda x: replace_values(ast.literal_eval(x)))\n",
    "    df[\"vehicle_types\"] = df[\"vehicle_types\"].apply(lambda x: replace_values(ast.literal_eval(x)))\n",
    "    df[\"road_conditions\"] = df[\"road_conditions\"].apply(lambda x: replace_values(sorted(ast.literal_eval(x))))\n",
    "    df[\"human_factors\"] = df[\"human_factors\"].apply(lambda x: replace_values(sorted(ast.literal_eval(x))))\n",
    "    df[\"infrastructure\"] = df[\"infrastructure\"].apply(lambda x: replace_values(sorted(ast.literal_eval(x))))\n",
    "\n",
    "    return df\n",
    "\n",
    "csv_paths = [\n",
    "    data_dir / f\"{to_snake_case(dataset_type)}_{to_snake_case(model)}.csv\" for (dataset_type, _), (model, _) in itertools.product(datasets, models)\n",
    "]\n",
    "\n",
    "dfs = {}\n",
    "for dataset_type, _ in datasets:\n",
    "    dfs[dataset_type] = {}\n",
    "    for model, _ in models:\n",
    "        csv_path = data_dir / f\"{to_snake_case(dataset_type)}_{to_snake_case(model)}.csv\"\n",
    "        dfs[dataset_type][model] = clean_df(pd.read_csv(csv_path))"
   ],
   "id": "57b3268a9e770dac",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T08:41:00.276603Z",
     "start_time": "2025-12-03T08:40:59.099010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_final_and_confidence(values: np.ndarray, validate_func: Optional[Callable[[Any], bool]]):\n",
    "    total = len(values)\n",
    "    values = [v for v in values if pd.notna(v) and (validate_func is None or validate_func(v))]\n",
    "\n",
    "    if len(values) == 0:\n",
    "        return np.nan, 0\n",
    "\n",
    "    counts = Counter(values)\n",
    "    final_value, freq = counts.most_common(1)[0]\n",
    "\n",
    "    confidence = freq / total\n",
    "\n",
    "    return final_value, confidence\n",
    "\n",
    "def compute_list_final_and_confidence(values: np.ndarray[list], preserve_order: bool, validate_func: Optional[Callable[[Any], bool]]):\n",
    "    total = len(values)\n",
    "    values = [v for v in values if isinstance(v, list)]\n",
    "    if len(values) == 0:\n",
    "        return np.nan, 0\n",
    "\n",
    "    if not preserve_order:\n",
    "        values = [v for vals in values for v in set(vals)]\n",
    "        counts = Counter(values)\n",
    "        final_values, freqs = list(counts.keys()), list(counts.values())\n",
    "        confidences = [freq / total for freq in freqs]\n",
    "        return final_values, confidences\n",
    "    else:\n",
    "        max_index = max(len(v) for v in values)\n",
    "        counters = [Counter([vals[i] for vals in values if len(vals) > i]) for i in range(max_index)]\n",
    "        final_values = [count.most_common(1)[0][0] for count in counters]\n",
    "        confidences = [count.most_common(1)[0][1] / total for count in counters]\n",
    "        return final_values, confidences\n",
    "\n",
    "def combine_field(field_name: str, datasets: dict[str, pd.DataFrame], confidence_func: Callable[[list, Optional[Callable[[Any], bool]]], tuple[Any, Any]], validate_func: Optional[Callable[[Any], bool]] = None) -> pd.DataFrame:\n",
    "    renamed_dfs = []\n",
    "    data_cols = []\n",
    "    for df_name, df in datasets.items():\n",
    "        sub: pd.DataFrame = df[[\"id\", field_name]].copy()\n",
    "        sub = sub.rename(columns={field_name: f\"{field_name}_{df_name}\"})\n",
    "        data_cols.append(f\"{field_name}_{df_name}\")\n",
    "        renamed_dfs.append(sub)\n",
    "\n",
    "    merged = reduce(lambda left, right: pd.merge(left, right, on=\"id\", how=\"outer\"), renamed_dfs)\n",
    "\n",
    "    extra_rows = []\n",
    "    for _, row in merged.iterrows():\n",
    "        values = row[data_cols].values\n",
    "\n",
    "        final_value, confidence = confidence_func(values, validate_func)\n",
    "        if isinstance(confidence, list):\n",
    "            confidence = [f\"{c:.2f}\" for c in confidence]\n",
    "        else:\n",
    "            confidence = f\"{confidence:.2f}\"\n",
    "\n",
    "        extra_rows.append({\n",
    "            f\"{field_name}_final_value\": final_value,\n",
    "            f\"{field_name}_confidence\": confidence,\n",
    "        })\n",
    "\n",
    "    return pd.concat([merged, pd.DataFrame(extra_rows)], axis=1)\n",
    "\n",
    "gt_0 = lambda v: v > 0\n",
    "\n",
    "# TODO: Add field specific comparisons\n",
    "data_cols = [\n",
    "    (\"date_of_accident\", compute_final_and_confidence, None),\n",
    "    (\"num_people_involved\", compute_final_and_confidence, gt_0),\n",
    "    (\"num_drivers\", compute_final_and_confidence, gt_0),\n",
    "    (\"driver_ages\", lambda x, val_func: compute_list_final_and_confidence(x, True, val_func), gt_0),\n",
    "    (\"driver_genders\", lambda x, val_func: compute_list_final_and_confidence(x, True, val_func), None),\n",
    "    (\"injury_severities\", lambda x, val_func: compute_list_final_and_confidence(x, True, val_func), None),\n",
    "    (\"num_vehicles_involved\", compute_final_and_confidence, gt_0),\n",
    "    (\"vehicle_types\", lambda x, val_func: compute_list_final_and_confidence(x, True, val_func), None),\n",
    "    (\"road_conditions\", lambda x, val_func: compute_list_final_and_confidence(x, False, val_func), None),\n",
    "    (\"human_factors\", lambda x, val_func: compute_list_final_and_confidence(x, False, val_func), None),\n",
    "    (\"infrastructure\", lambda x, val_func: compute_list_final_and_confidence(x, False, val_func), None),\n",
    "    (\"road_name\", compute_final_and_confidence, None),\n",
    "    (\"locality\", compute_final_and_confidence, None),\n",
    "]\n",
    "\n",
    "os.makedirs(\"data/merged\", exist_ok=True)\n",
    "\n",
    "for dataset_type, dataset_df in datasets:\n",
    "    dataset_dfs = dfs[dataset_type]\n",
    "    combined_fields = [combine_field(field_name, dataset_dfs, compute_func, validate_func).set_index(\"id\") for field_name, compute_func, validate_func in data_cols]\n",
    "    merged = pd.concat(combined_fields, axis=1).reset_index()\n",
    "    dataset_ids = set(dataset_df[\"id\"])\n",
    "    merged_ids = set(merged[\"id\"])\n",
    "    extra_ids = merged_ids - dataset_ids\n",
    "\n",
    "    if len(extra_ids) > 0:\n",
    "        merged = merged[~merged[\"id\"].isin(extra_ids)]\n",
    "\n",
    "    merged.to_csv(Path(\"data/merged\") / f\"{to_snake_case(dataset_type)}_combined.csv\", index=False)\n"
   ],
   "id": "1acf9c96bda114c8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Review merged\n",
    "- Find empty rows and check if they are relevant\n",
    "- Check non-confident values and manually set the final value\n",
    "\n",
    "# Ambiguities\n",
    "- 751: Date - Only \"night\" was mentioned, no specific time. Set time to 8pm\n",
    "- 1274: Date - Only morning was given and that traffic subsided by 11:15am. Set time to 8am.\n",
    "- 2101: Date - No time mentioned. Leaving time at midnight\n",
    "- 2201: Date - No time mentioned. Leaving time at midnight"
   ],
   "id": "5c0c295ac5f949bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
